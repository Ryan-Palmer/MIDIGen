{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4090.\n"
     ]
    }
   ],
   "source": [
    "if device == \"cuda\":\n",
    "    print(f\"Device: {torch.cuda.get_device_name()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with lets bring in our import / encode / decode functions from last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import chain\n",
    "from itertools import groupby\n",
    "from functools import reduce\n",
    "from typing import Collection, List\n",
    "from pathlib import Path\n",
    "import music21 as m21\n",
    "\n",
    "musescore_path = '/usr/bin/mscore'\n",
    "m21.environment.set('musicxmlPath', musescore_path)\n",
    "m21.environment.set('musescoreDirectPNGPath', musescore_path)\n",
    "\n",
    "BEATS_PER_BAR = 4 # beats per bar\n",
    "DIVISIONS_PER_QUARTER = 4 # i.e. 4 beats per bar and 4 divisions per beat gives 16 divisions per bar\n",
    "MIDI_NOTE_COUNT = 128\n",
    "MAX_NOTE_DUR = (8*BEATS_PER_BAR*DIVISIONS_PER_QUARTER)\n",
    "SEPARATOR_IDX = -1 # separator value for numpy encoding\n",
    "PIANO_RANGE = (21, 108)\n",
    "SOS = '<|sos|>' # Start of sequence\n",
    "EOS = '<|eos|>' # End of sequence\n",
    "SEP = '<|sep|>' # End of timestep (required for polyphony). Note index -1\n",
    " # SEP token must be last, i.e. one place before note tokens, so that adding the note offset still works when encoding\n",
    "SPECIAL_TOKENS = [SOS, EOS, SEP]\n",
    "MIDI_NOTE_COUNT = 128\n",
    "NOTE_TOKENS = [f'n{i}' for i in range(MIDI_NOTE_COUNT)]\n",
    "DURATION_SIZE = 8 * BEATS_PER_BAR * DIVISIONS_PER_QUARTER + 1 # 8 bars of sixteenth (semiquaver) notes + 1 for 0 length\n",
    "DURATION_TOKENS = [f'd{i}' for i in range(DURATION_SIZE)]\n",
    "NOTE_START, NOTE_END = NOTE_TOKENS[0], NOTE_TOKENS[-1]\n",
    "DURATION_START, DURATION_END = DURATION_TOKENS[0], DURATION_TOKENS[-1]\n",
    "ALL_TOKENS = SPECIAL_TOKENS + NOTE_TOKENS + DURATION_TOKENS\n",
    "ALL_TOKENS[0:8]\n",
    "TIMESIG = f'{BEATS_PER_BAR}/4'\n",
    "\n",
    "class MusicVocab():\n",
    "    def __init__(self):\n",
    "        itos = SPECIAL_TOKENS + NOTE_TOKENS + DURATION_TOKENS\n",
    "        # Ensure that the vocab is a multiple of 8 for fp16 training\n",
    "        if len(itos)%8 != 0:\n",
    "            itos = itos + [f'dummy{i}' for i in range(len(itos)%8)]\n",
    "        self.itos = itos\n",
    "        self.stoi = {v:k for k,v in enumerate(self.itos)}\n",
    "\n",
    "    def to_indices(self, t:Collection[str]) -> List[int]:\n",
    "        \"Convert a list of tokens `t` to their indices.\"\n",
    "        return [self.stoi[w] for w in t]\n",
    "\n",
    "    def to_tokens(self, nums:Collection[int], sep=' ') -> List[str]:\n",
    "        \"Convert a list of indices to their tokens.\"\n",
    "        items = [self.itos[i] for i in nums]\n",
    "        return sep.join(items) if sep is not None else items\n",
    "\n",
    "    @property\n",
    "    def bos_idx(self): return self.stoi[SOS]\n",
    "    @property\n",
    "    def eos_idx(self): return self.stoi[EOS]\n",
    "    @property\n",
    "    def sep_idx(self): return self.stoi[SEP]\n",
    "    @property\n",
    "    def note_position_enc_range(self): return (self.stoi[SEP], self.stoi[DURATION_END]+1)\n",
    "    @property\n",
    "    def note_range(self): return self.stoi[NOTE_START], self.stoi[NOTE_END]+1\n",
    "    @property\n",
    "    def duration_range(self): return self.stoi[DURATION_START], self.stoi[DURATION_END]+1\n",
    "\n",
    "def stream_to_sparse_enc(stream_score, note_size=MIDI_NOTE_COUNT, sample_freq=DIVISIONS_PER_QUARTER, max_note_dur=MAX_NOTE_DUR):    \n",
    "    # Time is measured in quarter notes since the start of the piece\n",
    "    # Original states that we are assuming 4/4 time but I don't see why that would be the case. BPB isn't used here.\n",
    "\n",
    "    # (MusicAutobot author:) TODO: need to order by instruments most played and filter out percussion or include the channel\n",
    "    highest_time = max(\n",
    "        stream_score.flatten().getElementsByClass('Note').stream().highestTime,\n",
    "        stream_score.flatten().getElementsByClass('Chord').stream().highestTime)\n",
    "    \n",
    "    # Calculate the maximum number of time steps\n",
    "    max_timestep = round(highest_time * sample_freq) + 1\n",
    "    sparse_score = np.zeros((max_timestep, len(stream_score.parts), note_size), dtype=np.int32)\n",
    "\n",
    "    # Convert a note to a tuple of (pitch,offset,duration)\n",
    "    def note_data(pitch, note):\n",
    "        return (pitch.midi, int(round(note.offset*sample_freq)), int(round(note.duration.quarterLength*sample_freq)))\n",
    "\n",
    "    for idx, part in enumerate(stream_score.parts):\n",
    "        \n",
    "        notes = chain.from_iterable(\n",
    "            [note_data(elem.pitch, elem)] if isinstance(elem, m21.note.Note)\n",
    "            else [note_data(p, elem) for p in elem.pitches] if isinstance(elem, m21.chord.Chord) \n",
    "            else []\n",
    "            for elem in part.flatten()\n",
    "        )\n",
    "\n",
    "        # sort flattened note list by timestep (1), duration (2) so that hits are not overwritten and longer notes have priority\n",
    "        notes_sorted = sorted(notes, key=lambda x: (x[1], x[2])) \n",
    "\n",
    "        for note in notes_sorted:\n",
    "            if note is not None:\n",
    "                pitch, timestep, duration = note\n",
    "                clamped_duration = max_note_dur if max_note_dur is not None and duration > max_note_dur else duration\n",
    "                sparse_score[timestep, idx, pitch] = clamped_duration\n",
    "    \n",
    "    return sparse_score\n",
    "\n",
    "# Pass in the 'one-hot' encoded numpy score\n",
    "def sparse_to_position_enc(sparse_score, skip_last_rest=True):\n",
    "\n",
    "    def encode_timestep(acc, timestep):\n",
    "        encoded_timesteps, wait_count = acc\n",
    "        encoded_timestep = timestep_to_position_enc(timestep) # pass in all notes for both instruments, merged list returned\n",
    "        if len(encoded_timestep) == 0: # i.e. all zeroes at time step\n",
    "            wait_count += 1\n",
    "        else:\n",
    "            if wait_count > 0:\n",
    "                encoded_timesteps.append([SEPARATOR_IDX, wait_count])\n",
    "            encoded_timesteps.extend(encoded_timestep)\n",
    "            wait_count = 1\n",
    "        return encoded_timesteps, wait_count\n",
    "    \n",
    "    encoded_timesteps, final_wait_count = reduce(encode_timestep, sparse_score, ([], 0))\n",
    "\n",
    "    if final_wait_count > 0 and not skip_last_rest:\n",
    "        encoded_timesteps.append([SEPARATOR_IDX, final_wait_count]) # add trailing rests\n",
    "\n",
    "    return np.array(encoded_timesteps).reshape(-1, 2) # reshaping. Just in case result is empty\n",
    "    \n",
    "def timestep_to_position_enc(timestep, note_range=PIANO_RANGE):\n",
    "\n",
    "    note_min, note_max = note_range\n",
    "\n",
    "    def encode_note_data(note_data, active_note_idx):\n",
    "        instrument, pitch = active_note_idx\n",
    "        duration = timestep[instrument, pitch]\n",
    "        if pitch >= note_min and pitch < note_max:\n",
    "            note_data.append([pitch, duration, instrument])\n",
    "        return note_data\n",
    "    \n",
    "    active_note_idxs = zip(*timestep.nonzero())\n",
    "    encoded_notes = reduce(encode_note_data, active_note_idxs, [])\n",
    "    sorted_notes = sorted(encoded_notes, key=lambda x: x[0], reverse=True) # sort by note (highest to lowest)\n",
    "\n",
    "    # Dropping instrument information for simplicity.\n",
    "    # MusicAutobot allows different encoding schemes which include instrument number and split pitch into class / octave.\n",
    "    return [n[:2] for n in sorted_notes]\n",
    "\n",
    "def position_to_idx_enc(note_position_score, vocab):\n",
    "    note_idx_score = note_position_score.copy()\n",
    "    note_min_idx, _ = vocab.note_range\n",
    "    dur_min_idx, _ = vocab.duration_range\n",
    "    \n",
    "    # Replace note and duration tokens with their index in vocab. \n",
    "    # Tokens are the same order as notes and note_min_idx offset is constant so we can apply in one go.\n",
    "    # Using broadcasting to add the 1D [note_min_idx, dur_min_idx] to the 2D note_idx_score.\n",
    "    note_idx_score += np.array([note_min_idx, dur_min_idx])\n",
    "    \n",
    "    prefix =  np.array([vocab.bos_idx])\n",
    "    suffix = np.array([vocab.eos_idx])\n",
    "\n",
    "    return np.concatenate([prefix, note_idx_score.reshape(-1), suffix])\n",
    "\n",
    "def import_midi_file(file_path):\n",
    "    midifile = m21.midi.MidiFile()\n",
    "    if isinstance(file_path, bytes):\n",
    "        midifile.readstr(file_path)\n",
    "    else:\n",
    "        midifile.open(file_path)\n",
    "        midifile.read()\n",
    "        midifile.close()\n",
    "    return midifile\n",
    "\n",
    "def midifile_to_stream(midifile): \n",
    "    return m21.midi.translate.midiFileToStream(midifile)\n",
    "\n",
    "def midifile_to_idx_score(file_path, vocab):\n",
    "    midifile = import_midi_file(file_path)\n",
    "    stream = midifile_to_stream(midifile)\n",
    "    sparse_score = stream_to_sparse_enc(stream)\n",
    "    note_pos_score = sparse_to_position_enc(sparse_score)\n",
    "    return position_to_idx_enc(note_pos_score, vocab)\n",
    "\n",
    "# Combining notes with different durations into a single chord may overwrite conflicting durations.\n",
    "def group_notes_by_duration(notes):\n",
    "    get_note_quarter_length = lambda note: note.duration.quarterLength\n",
    "    sorted_notes = sorted(notes, key=get_note_quarter_length)\n",
    "    return [list(g) for k,g in groupby(sorted_notes, get_note_quarter_length)]\n",
    "\n",
    "def sparse_instrument_to_stream_part(sparse_instrument_score, step_duration):\n",
    "    part = m21.stream.Part()\n",
    "    part.append(m21.instrument.Piano())\n",
    "    \n",
    "    for t_idx, pitch_values in enumerate(sparse_instrument_score):\n",
    "\n",
    "        def decode_sparse_note(notes, pitch_index):\n",
    "            note = m21.note.Note(pitch_index)\n",
    "            quarters = sparse_instrument_score[t_idx, pitch_index]\n",
    "            note.duration = m21.duration.Duration(float(quarters) * step_duration.quarterLength)\n",
    "            notes.append(note)\n",
    "            return notes\n",
    "        \n",
    "        pitch_idxs = np.nonzero(pitch_values)[0]\n",
    "\n",
    "        if len(pitch_idxs) != 0: \n",
    "            notes = reduce(decode_sparse_note, pitch_idxs, [])\n",
    "            for note_group in group_notes_by_duration(notes):\n",
    "                note_position = t_idx*step_duration.quarterLength\n",
    "                if len(note_group) == 1:\n",
    "                    part.insert(note_position, note_group[0])\n",
    "                else:\n",
    "                    chord = m21.chord.Chord(note_group)\n",
    "                    part.insert(note_position, chord)\n",
    "\n",
    "    return part\n",
    "\n",
    "def sparse_to_stream_enc(sparse_score, bpm=120):\n",
    "    step_duration = m21.duration.Duration(1. / DIVISIONS_PER_QUARTER)\n",
    "    stream = m21.stream.Score()\n",
    "    stream.append(m21.meter.TimeSignature(TIMESIG))\n",
    "    stream.append(m21.tempo.MetronomeMark(number=bpm))\n",
    "\n",
    "    # Not required here but left as example of options available\n",
    "    stream.append(m21.key.KeySignature(0))\n",
    "    \n",
    "    for inst in range(sparse_score.shape[1]):\n",
    "        part = sparse_instrument_to_stream_part(sparse_score[:,inst,:], step_duration)\n",
    "        stream.append(part)\n",
    "    \n",
    "    # Again, not required yet but left as example\n",
    "    stream = stream.transpose(0)\n",
    "    \n",
    "    return stream\n",
    "\n",
    "def position_to_sparse_enc(note_position_score):\n",
    "\n",
    "    # Add all the separator durations as they denote the elapsed time\n",
    "    score_length = sum(timestep[1] for timestep in note_position_score if timestep[0] == SEPARATOR_IDX) + 1\n",
    "    \n",
    "    # Single instrument as we discarded the instrument information when encoding\n",
    "    # We will adapt to handle multiple instruments later\n",
    "    instrument = 0\n",
    "\n",
    "    def decode_note_position_step(acc, note_pos_step):\n",
    "        timestep, sparse_score = acc\n",
    "        note, duration = note_pos_step.tolist()\n",
    "        if note < SEPARATOR_IDX:  # Skip special token\n",
    "            return acc\n",
    "        elif note == SEPARATOR_IDX:  # Time elapsed\n",
    "            return (timestep + duration, sparse_score)\n",
    "        else:\n",
    "            sparse_score[timestep, instrument, note] = duration\n",
    "            return (timestep, sparse_score)\n",
    "\n",
    "     # (timesteps, instruments, pitches)\n",
    "    initial_sparse_score = np.zeros((score_length, 1, MIDI_NOTE_COUNT))\n",
    "    _, final_sparse_score = reduce(decode_note_position_step, note_position_score, (0, initial_sparse_score))\n",
    "\n",
    "    return final_sparse_score\n",
    "\n",
    "# No validation of note position encoding included to keep it simple for now\n",
    "def idx_to_position_enc(idx_score, vocab):\n",
    "    \n",
    "    # Filter out special tokens\n",
    "    notes_durs_start, notes_durs_end = vocab.note_position_enc_range # range of non-special token values\n",
    "    notes_durations_idx_score = idx_score[np.where((idx_score >= notes_durs_start) & (idx_score < notes_durs_end))]\n",
    "\n",
    "    # Reshape into pairs of (note, duration). If odd number of tokens, discard the last token.\n",
    "    if notes_durations_idx_score.shape[0] % 2 != 0:\n",
    "        notes_durations_idx_score = notes_durations_idx_score[:-1]\n",
    "\n",
    "    position_score = notes_durations_idx_score.copy().reshape(-1, 2)\n",
    "    \n",
    "    # Shift token index values to note and duration values\n",
    "    if position_score.shape[0] == 0: \n",
    "        return position_score\n",
    "    else:\n",
    "        note_min_idx, _ = vocab.note_range\n",
    "        dur_min_idx, _ = vocab.duration_range\n",
    "        position_score -= np.array([note_min_idx, dur_min_idx])\n",
    "        return position_score\n",
    "\n",
    "def idx_to_stream_enc(idx_score, vocab):\n",
    "    position_score = idx_to_position_enc(idx_score, vocab)\n",
    "    sparse_score = position_to_sparse_enc(position_score)\n",
    "    return sparse_to_stream_enc(sparse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a quick test..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab = MusicVocab()\n",
    "single_bar_path = Path('data/midi/single_bar_example.mid')\n",
    "single_bar_idx_score = midifile_to_idx_score(single_bar_path, vocab)\n",
    "single_bar_stream = idx_to_stream_enc(single_bar_idx_score, vocab)\n",
    "single_bar_stream.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "We have two options when loading data from multiple sources into training batches.\n",
    "\n",
    "1. Concatenate all the sources into one long sequence, with a separator token between (`<|eos|>` in our case). We can then just window through the sequence to pick batches.\n",
    "\n",
    "2. Treat each source as a single batch, and pad shorter sequences to the length of the longest so they are all consistent.\n",
    "\n",
    "`1.` is simpler to implement, although it does make it harder for the model to distinguish between sequence boundaries as it has to learn that nothing before an `<|eos|>` token is relevant to anything after it.\n",
    "\n",
    "`2.` on the other hand has explicit sequence boundaries, but wastes training resources processing `<|pad|>` tokens.\n",
    "\n",
    "For this reason, lets go with `1.`\n",
    "\n",
    "The process will be\n",
    "\n",
    "- Load each file\n",
    "- Tokenise it\n",
    "- Split them into x samples and y labels\n",
    "- Concatenate all the xs into one long collection of samples and the ys into one collection of labels\n",
    "\n",
    "> If we assign labels *before* we concatenate then we avoid having anything follow `<|eos|>` in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ace Attorney_3DS_Phoenix Wright Ace Attorney Spirit of Justice_Cheerful People.mid',\n",
       " 'Animal Crossing_3DS_Animal Crossing New Leaf_100 AM.mid',\n",
       " \"Banjo-Kazooie_GBA_Banjo-Kazooie Grunty's Revenge_Breegull Beach.mid\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "vg_path = Path('data/midi/vg')\n",
    "file_names = [f for f in os.listdir(vg_path) if os.path.isfile(os.path.join(vg_path, f))]\n",
    "file_names[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram revisited\n",
    "\n",
    "To start with, let's do a simple bigram encoding as we did in the previous workbook, but this time on the bigger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((137086,), (137086,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bigram_encode(file_names, vocab):\n",
    "    xs, ys = [], []\n",
    "    for file_name in file_names:\n",
    "        file_path = Path(vg_path, file_name)\n",
    "        idx_score = midifile_to_idx_score(file_path, vocab)\n",
    "        xs.append(idx_score[:-1])\n",
    "        ys.append(idx_score[1:])\n",
    "    return np.concatenate(xs), np.concatenate(ys)\n",
    "\n",
    "xs, ys = bigram_encode(file_names, vocab)\n",
    "xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: <|sos|>, y:n31\n",
      "x: n31, y:d3\n",
      "x: d3, y:<|sep|>\n",
      "x: <|sep|>, y:d4\n",
      "x: d4, y:n31\n"
     ]
    }
   ],
   "source": [
    "for _, (x, y) in enumerate(zip(xs[:5], ys[:5])):\n",
    "    print(f'x: {vocab.to_tokens([x])}, y:{vocab.to_tokens([y])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP\n",
    "\n",
    "We are going to try to roughly follow part 2 of Karpathy's Makemore and use a simple MLP to improve our bigram results (alogn with a much bigger data set!)\n",
    "\n",
    "Spoiler alert, it still won't be very good, but we might see a glimmer of improvement....\n",
    "\n",
    "### Expanding the context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our bigram encoding, we asked 'if the input sequence is `[a]`, what is the output token?'.\n",
    "\n",
    "Now we want to expand the input context - i.e. we are asking 'if the input is `[a, b, c]`, what is the output token?'\n",
    "\n",
    "We still want to only move the window forward by one token for each example to get the most we can out of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((137006, 3), 137006)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 3\n",
    "\n",
    "def block_encode(file_names, vocab, block_size=block_size):\n",
    "    xs, ys = [], []\n",
    "    for file_name in file_names:\n",
    "        file_path = Path(vg_path, file_name)\n",
    "        idx_score = midifile_to_idx_score(file_path, vocab)\n",
    "        for i in range(0, len(idx_score) - block_size, 1):\n",
    "            xs.append(idx_score[i:i+block_size])\n",
    "            ys.append(idx_score[i+block_size])\n",
    "    return np.stack(xs), ys # stack xs to create 2D tensor\n",
    "\n",
    "xs, ys = block_encode(file_names, vocab)\n",
    "xs.shape, len(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: <|sos|> n31 d3, y:<|sep|>\n",
      "x: n31 d3 <|sep|>, y:d4\n",
      "x: d3 <|sep|> d4, y:n31\n",
      "x: <|sep|> d4 n31, y:d3\n",
      "x: d4 n31 d3, y:<|sep|>\n"
     ]
    }
   ],
   "source": [
    "for _, (x, y) in enumerate(zip(xs[:5], ys[:5])):\n",
    "    print(f'x: {vocab.to_tokens(x)}, y:{vocab.to_tokens([y])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: d1 <|sep|> d1, y:n76\n",
      "x: <|sep|> d1 n76, y:d1\n",
      "x: d1 n76 d1, y:n71\n",
      "x: n76 d1 n71, y:d1\n",
      "x: d1 n71 d1, y:<|eos|>\n"
     ]
    }
   ],
   "source": [
    "for _, (x, y) in enumerate(zip(xs[-5:], ys[-5:])):\n",
    "    print(f'x: {vocab.to_tokens(x)}, y:{vocab.to_tokens([y])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(xs, device=device)\n",
    "Y = torch.tensor(ys, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([137006, 3]), torch.int64, torch.Size([137006]), torch.int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings\n",
    "\n",
    "Embeddings are just a lookup from vocab index to tensor. The length of the tensor compared to the number of distinct tokens determines how much compression we are asking the model to perform.\n",
    "\n",
    "We can start by initialising the lookup as random numbers. It will ultimately be trained as the first layer of our MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab.itos)\n",
    "embed_size = 2\n",
    "C = torch.randn((vocab_size, embed_size), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To pick out a value we could index into this tensor directly, or multiply by a one-hot encoded index as we did in the last notebook.\n",
    "\n",
    "We can index into `C` using `X` directly to essentially convert X into embedding values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([137006, 3, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layer\n",
    "\n",
    "We have a context window of 3 and an embedding size of 2, so 6 inputs per example.\n",
    "\n",
    "These need to be connected to each neuron in our hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 100\n",
    "\n",
    "W1 = torch.randn((embed_size*block_size, neurons), device=device)\n",
    "b1 = torch.randn((neurons), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to transform the `[137006, 3, 2]` tensor to `137006, 6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([137006, 6])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_flattened = emb.view(-1, (block_size*embed_size))\n",
    "emb_flattened.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can multiply the embeddings by our hidden layer weights and pass them through the `tanh` non-linearity to get our activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([137006, 100])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tanh(emb_flattened @ W1 + b1)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are our activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.9989, -0.9550,  ...,  0.9323,  0.9991, -0.9905],\n",
       "        [ 0.9114, -0.3525, -0.9556,  ..., -0.9815,  0.9968,  0.8886],\n",
       "        [ 0.8629,  0.4826,  0.9614,  ...,  0.9995,  0.2493, -0.6779],\n",
       "        ...,\n",
       "        [ 0.3838, -0.7313, -0.9820,  ..., -0.2550,  0.4922,  0.9995],\n",
       "        [ 0.9997,  0.7281, -0.9544,  ...,  0.9944,  0.9679, -0.8925],\n",
       "        [-0.8263,  0.0251, -0.4203,  ..., -0.9872,  0.9885,  0.8777]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need our output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([137006, 264])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2 = torch.randn((neurons, vocab_size), device=device)\n",
    "b2 = torch.randn((vocab_size), device=device)\n",
    "\n",
    "logits = h @ W2 + b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([137006, 264])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdim=True)\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to pluck out the 'correct' result from the 264 probabilities, given by `Y`. Then we can see how confident we were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21.7972, device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -prob[torch.arange(len(Y)), Y].log().mean() # Get the Yth element of each row to calculate the negative log likelihood\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
