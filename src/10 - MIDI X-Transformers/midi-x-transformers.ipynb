{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'10 - MIDI X-Transformers')\n",
    "import torch\n",
    "from miditok import REMI, TokenizerConfig  # here we choose to use REMI\n",
    "from pathlib import Path\n",
    "import random\n",
    "from x_transformers import TransformerWrapper, Decoder\n",
    "from miditok.utils import split_files_for_training\n",
    "from miditok.data_augmentation import augment_dataset\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from torch.utils.data import DataLoader\n",
    "from miditok import TokSequence\n",
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import IPython.display\n",
    "import music21 as m21\n",
    "musescore_path = '/usr/bin/mscore'\n",
    "m21.environment.set('musicxmlPath', musescore_path)\n",
    "m21.environment.set('musescoreDirectPNGPath', musescore_path)\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4090.\n"
     ]
    }
   ],
   "source": [
    "if device == \"cuda\":\n",
    "    print(f\"Device: {torch.cuda.get_device_name()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/miditok/tokenizations/remi.py:88: UserWarning: Attribute controls are not compatible with 'config.one_token_stream_for_programs' and multi-vocabulary tokenizers. Disabling them from the config.\n",
      "  super().__init__(tokenizer_config, params)\n"
     ]
    }
   ],
   "source": [
    "CHUNK_LENGTH = 8193 # Offset by 1 for the labels\n",
    "TIMESTEPS = 2048\n",
    "SEGMENTS = (CHUNK_LENGTH - 1) // TIMESTEPS\n",
    "BATCH_SIZE = 16\n",
    "VOCAB_SIZE = 716 # REMI basic untrained token count = 411, +chords = 425, +tempos = 457, +time sig = 530, +rests = 562, +chord root note = 716\n",
    "N_EMBED = 512\n",
    "N_LAYER = 8\n",
    "N_HEAD = 8\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-3\n",
    "NUM_BATCHES = int(1e5)\n",
    "MAX_GRAD_CLIP_NORM = 0.5\n",
    "VALIDATE_EVERY  = 64\n",
    "DIM_HEAD = N_EMBED // N_HEAD\n",
    "VERSION_LABEL = \"X-Transformers-flash-rotary\"\n",
    "TOKENIZER_CONFIG = 'All_Options'\n",
    "\n",
    "midi_path = Path(f'../data/midi')\n",
    "dataset_name = 'lakh_clean'\n",
    "midi_dataset_path = Path(f'{midi_path}/{dataset_name}')\n",
    "midi_file_paths = [p.resolve() for p in midi_dataset_path.glob(\"**/*.mid\")]\n",
    "\n",
    "tokenizer_save_path = Path(f'../data/vocab/MidiTok/{dataset_name}_{VOCAB_SIZE}_{TOKENIZER_CONFIG}.json')\n",
    "\n",
    "if not tokenizer_save_path.exists():\n",
    "    TOKENIZER_PARAMS = {\n",
    "        \"pitch_range\": (21, 109),\n",
    "        \"beat_res\": {(0, 4): 8, (4, 12): 4},\n",
    "        \"num_velocities\": 32,\n",
    "        \"use_programs\": True,\n",
    "        \"program_changes \": True, # Only insert program changes when the instrument changes rather than before every note\n",
    "        \"use_chords\": True,\n",
    "        \"chord_tokens_with_root_note\": True, # Include the root note in the chord token\n",
    "        # \"use_pitch_bends\": True,\n",
    "        \"use_time_signatures\": True,\n",
    "        \"delete_equal_successive_time_sig_changes\": True, # Only insert time signatures when the time signature changes\n",
    "        \"use_tempos\": True,\n",
    "        \"delete_equal_successive_tempo_changes \": True, # Only insert tempos when the tempo changes after downsampling\n",
    "        \"use_rests\": True,\n",
    "    }\n",
    "    tokenizer_confg = TokenizerConfig(**TOKENIZER_PARAMS)\n",
    "    tokenizer = REMI(tokenizer_confg)\n",
    "    print(f\"Untrained token count: {tokenizer.len}\")\n",
    "    tokenizer.train(vocab_size=VOCAB_SIZE, files_paths=midi_file_paths)\n",
    "    tokenizer.save(tokenizer_save_path)\n",
    "else:\n",
    "    tokenizer = REMI(params=tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We split files into train / test / validation folder when we chunk them, so no requirement to set seed here\n",
    "# random.seed(42)\n",
    "\n",
    "random.shuffle(midi_file_paths)\n",
    "len(midi_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 13804, Valid files: 1726, Test files: 1726\n"
     ]
    }
   ],
   "source": [
    "n1 = int(0.8 * len(midi_file_paths))\n",
    "n2 = int(0.9 * len(midi_file_paths))\n",
    "train_filepaths = midi_file_paths[:n1]\n",
    "valid_filepaths = midi_file_paths[n1:n2]\n",
    "test_filepaths = midi_file_paths[n2:]\n",
    "\n",
    "print(f'Train files: {len(train_filepaths)}, Valid files: {len(valid_filepaths)}, Test files: {len(test_filepaths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_path = Path(f'{midi_path}/mtok_split/{dataset_name}/v-{VOCAB_SIZE}_t-{TOKENIZER_CONFIG}_c-{CHUNK_LENGTH}')\n",
    "train_chunk_path = Path(f'{chunk_path}/train')\n",
    "valid_chunk_path = Path(f'{chunk_path}/valid')\n",
    "test_chunk_path = Path(f'{chunk_path}/test')\n",
    "\n",
    "split_data = [\n",
    "    (train_filepaths, train_chunk_path),\n",
    "    (valid_filepaths, valid_chunk_path),\n",
    "    (test_filepaths, test_chunk_path)\n",
    "]\n",
    "\n",
    "def chunk_files(filepaths, tokenizer, chunks_dir, max_seq_len):\n",
    "    split_files_for_training(\n",
    "        files_paths=filepaths,\n",
    "        tokenizer=tokenizer,\n",
    "        save_dir=chunks_dir,\n",
    "        max_seq_len=max_seq_len,\n",
    "        num_overlap_bars=1\n",
    "    )\n",
    "    augment_dataset(\n",
    "        chunks_dir,\n",
    "        pitch_offsets=[-12, 12],\n",
    "        velocity_offsets=[-4, 4],\n",
    "        duration_offsets=[-0.5, 0.5],\n",
    "    )\n",
    "\n",
    "if not chunk_path.exists():\n",
    "    with Pool(processes=3) as pool:\n",
    "        pool.starmap(chunk_files, [(filepaths, tokenizer, chunks_dir, CHUNK_LENGTH) for filepaths, chunks_dir in split_data])\n",
    "\n",
    "# for filepaths, chunks_dir in split_data:\n",
    "#     chunk_files(filepaths, tokenizer, chunks_dir, CHUNK_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train chunks: 218143, Valid chunks: 27665, Test chunks: 27428\n"
     ]
    }
   ],
   "source": [
    "train_chunk_filepaths = list(train_chunk_path.glob(\"**/*.mid\"))\n",
    "valid_chunk_filepaths = list(valid_chunk_path.glob(\"**/*.mid\"))\n",
    "test_chunk_filepaths = list(test_chunk_path.glob(\"**/*.mid\"))\n",
    "\n",
    "print(f'Train chunks: {len(train_chunk_filepaths)}, Valid chunks: {len(valid_chunk_filepaths)}, Test chunks: {len(test_chunk_filepaths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_paths = [train_chunk_filepaths, valid_chunk_filepaths, test_chunk_filepaths]\n",
    "\n",
    "for chunk_path in chunk_paths:\n",
    "    random.shuffle(chunk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "def create_data_set(chunks_path, tokenizer, max_seq_len):\n",
    "    return DatasetMIDI(\n",
    "        pre_tokenize=False,\n",
    "        files_paths=chunks_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_len=max_seq_len,\n",
    "        bos_token_id=tokenizer[\"BOS_None\"],\n",
    "        eos_token_id=tokenizer[\"EOS_None\"])\n",
    "\n",
    "def create_data_loader (dataset, tokenizer, batch_size):\n",
    "    collator = DataCollator(tokenizer.pad_token_id) # copy_inputs_as_labels and shift_labels not needed as done by the transformer\n",
    "    return cycle(DataLoader(dataset=dataset, collate_fn=collator, batch_size=batch_size, num_workers=8, pin_memory=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = map(\n",
    "    lambda chunk_filepaths: create_data_set(chunk_filepaths, tokenizer, CHUNK_LENGTH),\n",
    "    chunk_paths\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = map(\n",
    "    lambda dataset: create_data_loader(dataset, tokenizer, BATCH_SIZE),\n",
    "    datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'memorizing_miditok_{dataset_name}_t-{TIMESTEPS}_v-{VOCAB_SIZE}_{VERSION_LABEL}'\n",
    "model_load_path = Path(f'../data/checkpoints/{model_name}.dat')\n",
    "model_save_path = Path(f'../data/checkpoints/{model_name}.dat')\n",
    "log_dir = Path(f'../tensorboard/{model_name}')\n",
    "tensorboard_writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25928192\n"
     ]
    }
   ],
   "source": [
    "model = TransformerWrapper(\n",
    "    num_tokens = VOCAB_SIZE,\n",
    "    max_seq_len = TIMESTEPS,\n",
    "    max_mem_len = TIMESTEPS, # XL mem, remember it needs a different training loop (can use xl wrapper). requires rel_pos_bias or rotary_pos_emb\n",
    "    shift_mem_down = 1,  # Shift memory down for XL mem to enhance the range\n",
    "    attn_layers = Decoder(\n",
    "        dim = N_EMBED,\n",
    "        depth = N_LAYER,\n",
    "        heads = N_HEAD,\n",
    "        # rel_pos_bias = True, # relative positional embedding\n",
    "        rotary_pos_emb = True, # a form of relative positional embedding which is compatible with flash attention\n",
    "        attn_flash = True, # just set this to True if you have pytorch 2.0 installed\n",
    "        layer_dropout = 0.1,   # stochastic depth - dropout entire layer\n",
    "        attn_dropout = 0.1,    # dropout post-attention\n",
    "        ff_dropout = 0.1       # feedforward dropout\n",
    "    )\n",
    ").to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "token_emb.emb.weight \t torch.Size([716, 512])\n",
      "attn_layers.layers.0.0.0.gamma \t torch.Size([512])\n",
      "attn_layers.layers.0.1.to_q.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.0.1.to_k.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.0.1.to_v.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.0.1.to_out.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.1.0.0.gamma \t torch.Size([512])\n",
      "attn_layers.layers.1.1.ff.0.0.weight \t torch.Size([2048, 512])\n",
      "attn_layers.layers.1.1.ff.0.0.bias \t torch.Size([2048])\n",
      "attn_layers.layers.1.1.ff.2.weight \t torch.Size([512, 2048])\n",
      "attn_layers.layers.1.1.ff.2.bias \t torch.Size([512])\n",
      "attn_layers.layers.2.0.0.gamma \t torch.Size([512])\n",
      "attn_layers.layers.2.1.to_q.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.2.1.to_k.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.2.1.to_v.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.2.1.to_out.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.3.0.0.gamma \t torch.Size([512])\n",
      "attn_layers.layers.3.1.ff.0.0.weight \t torch.Size([2048, 512])\n",
      "attn_layers.layers.3.1.ff.0.0.bias \t torch.Size([2048])\n",
      "attn_layers.layers.3.1.ff.2.weight \t torch.Size([512, 2048])\n",
      "attn_layers.layers.3.1.ff.2.bias \t torch.Size([512])\n",
      "attn_layers.layers.4.0.0.gamma \t torch.Size([512])\n",
      "attn_layers.layers.4.1.to_q.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.4.1.to_k.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.4.1.to_v.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.4.1.to_out.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.5.0.0.gamma \t torch.Size([512])\n",
      "attn_layers.layers.5.1.ff.0.0.weight \t torch.Size([2048, 512])\n",
      "attn_layers.layers.5.1.ff.0.0.bias \t torch.Size([2048])\n",
      "attn_layers.layers.5.1.ff.2.weight \t torch.Size([512, 2048])\n",
      "attn_layers.layers.5.1.ff.2.bias \t torch.Size([512])\n",
      "attn_layers.layers.6.0.0.gamma \t torch.Size([512])\n",
      "attn_layers.layers.6.1.to_q.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.6.1.to_k.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.6.1.to_v.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.6.1.to_out.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.7.0.0.gamma \t torch.Size([512])\n",
      "attn_layers.layers.7.1.ff.0.0.weight \t torch.Size([2048, 512])\n",
      "attn_layers.layers.7.1.ff.0.0.bias \t torch.Size([2048])\n",
      "attn_layers.layers.7.1.ff.2.weight \t torch.Size([512, 2048])\n",
      "attn_layers.layers.7.1.ff.2.bias \t torch.Size([512])\n",
      "attn_layers.layers.8.0.0.gamma \t torch.Size([512])\n",
      "attn_layers.layers.8.1.to_q.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.8.1.to_k.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.8.1.to_v.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.8.1.to_out.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.9.0.0.gamma \t torch.Size([512])\n",
      "attn_layers.layers.9.1.ff.0.0.weight \t torch.Size([2048, 512])\n",
      "attn_layers.layers.9.1.ff.0.0.bias \t torch.Size([2048])\n",
      "attn_layers.layers.9.1.ff.2.weight \t torch.Size([512, 2048])\n",
      "attn_layers.layers.9.1.ff.2.bias \t torch.Size([512])\n",
      "attn_layers.layers.10.0.0.gamma \t torch.Size([512])\n",
      "attn_layers.layers.10.1.to_q.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.10.1.to_k.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.10.1.to_v.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.10.1.to_out.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.11.0.0.gamma \t torch.Size([512])\n",
      "attn_layers.layers.11.1.ff.0.0.weight \t torch.Size([2048, 512])\n",
      "attn_layers.layers.11.1.ff.0.0.bias \t torch.Size([2048])\n",
      "attn_layers.layers.11.1.ff.2.weight \t torch.Size([512, 2048])\n",
      "attn_layers.layers.11.1.ff.2.bias \t torch.Size([512])\n",
      "attn_layers.layers.12.0.0.gamma \t torch.Size([512])\n",
      "attn_layers.layers.12.1.to_q.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.12.1.to_k.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.12.1.to_v.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.12.1.to_out.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.13.0.0.gamma \t torch.Size([512])\n",
      "attn_layers.layers.13.1.ff.0.0.weight \t torch.Size([2048, 512])\n",
      "attn_layers.layers.13.1.ff.0.0.bias \t torch.Size([2048])\n",
      "attn_layers.layers.13.1.ff.2.weight \t torch.Size([512, 2048])\n",
      "attn_layers.layers.13.1.ff.2.bias \t torch.Size([512])\n",
      "attn_layers.layers.14.0.0.gamma \t torch.Size([512])\n",
      "attn_layers.layers.14.1.to_q.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.14.1.to_k.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.14.1.to_v.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.14.1.to_out.weight \t torch.Size([512, 512])\n",
      "attn_layers.layers.15.0.0.gamma \t torch.Size([512])\n",
      "attn_layers.layers.15.1.ff.0.0.weight \t torch.Size([2048, 512])\n",
      "attn_layers.layers.15.1.ff.0.0.bias \t torch.Size([2048])\n",
      "attn_layers.layers.15.1.ff.2.weight \t torch.Size([512, 2048])\n",
      "attn_layers.layers.15.1.ff.2.bias \t torch.Size([512])\n",
      "attn_layers.rotary_pos_emb.inv_freq \t torch.Size([16])\n",
      "attn_layers.final_norm.gamma \t torch.Size([512])\n",
      "to_logits.weight \t torch.Size([716, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For visualizing embeddings from a trained model\n",
    "def visualize_model_embeddings(model, tokenizer):\n",
    "    if tokenizer.is_trained:\n",
    "        all_ids = torch.tensor(list(tokenizer.vocab_model.values())).to(device)\n",
    "        all_ids_cpu = torch.tensor(list(tokenizer.vocab_model.values()))\n",
    "    else:\n",
    "        all_ids = torch.tensor(list(tokenizer.vocab.values())).to(device)\n",
    "        all_ids_cpu = torch.tensor(list(tokenizer.vocab.values()))\n",
    "    \n",
    "    # Get embeddings from model\n",
    "    with torch.no_grad():\n",
    "        # Assuming model.wte is the token embedding layer (adjust as needed)\n",
    "        embeddings = model.token_emb(all_ids).detach().cpu().numpy()\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, perplexity=min(30, len(all_ids_cpu)-1))\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # Create categorical color mapping based on token types\n",
    "    # You can customize this based on your tokenizer's vocabulary\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=all_ids_cpu, cmap='tab20', alpha=0.7)\n",
    "    plt.colorbar(label='Token ID')\n",
    "    plt.title('Model Embeddings t-SNE Projection')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from iteration 32512\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "def save_checkpoint(optimizer, completed_iterations, train_loss, val_loss):\n",
    "    tensorboard_writer.add_scalar('Loss/train', train_loss, completed_iterations)\n",
    "    tensorboard_writer.add_scalar('Loss/val', val_loss, completed_iterations)\n",
    "    clear_output(wait=True)\n",
    "    print(f'Writing to Tensorboard: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    torch.save({\n",
    "        'iter': completed_iterations,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, model_save_path)\n",
    "    visualize_model_embeddings(model, tokenizer)\n",
    "\n",
    "completed_iterations = 0\n",
    "if model_load_path.exists():\n",
    "    checkpoint = torch.load(model_load_path)\n",
    "    completed_iterations = checkpoint['iter']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print(f\"Loaded model from iteration {completed_iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from x_transformers import XLAutoregressiveWrapper\n",
    "# xl_wrapper = XLAutoregressiveWrapper(model, pad_value=tokenizer.pad_token_id)\n",
    "\n",
    "# scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# for i in tqdm.tqdm(range(NUM_BATCHES - completed_iterations), mininterval = 10., desc = 'training'):\n",
    "#     xl_wrapper.train()\n",
    "\n",
    "#     x = next(train_loader)[\"input_ids\"].to(device)\n",
    "#     if x.shape[0] != BATCH_SIZE:\n",
    "#         print(f'Skipping batch {i} as it is not of size {BATCH_SIZE}, but {x.shape[0]}')\n",
    "#         x = next(train_loader)[\"input_ids\"].to(device)\n",
    "\n",
    "#     with torch.cuda.amp.autocast():\n",
    "#         train_loss = xl_wrapper(x)\n",
    "    \n",
    "#     scaler.scale(train_loss).backward()\n",
    "\n",
    "#     # First unscale the gradients (modifies in-place)\n",
    "#     scaler.unscale_(optimizer)\n",
    "\n",
    "#     # Now clip the unscaled gradients\n",
    "#     torch.nn.utils.clip_grad_norm_(xl_wrapper.parameters(), MAX_GRAD_CLIP_NORM)\n",
    "\n",
    "#     # Won't unscale again, just checks for inf/NaN and steps if clean\n",
    "#     scaler.step(optimizer)\n",
    "#     scaler.update()\n",
    "\n",
    "#     optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "#     if not (i % VALIDATE_EVERY):\n",
    "#         xl_wrapper.eval()\n",
    "\n",
    "#         x_valid = next(valid_loader)[\"input_ids\"].to(device)\n",
    "#         if x_valid.shape[0] != BATCH_SIZE:\n",
    "#             print(f'Skipping validation batch {i} as it is not of size {BATCH_SIZE}, but {x_valid.shape[0]}')\n",
    "#             x_valid = next(valid_loader)[\"input_ids\"].to(device)\n",
    "\n",
    "#         valid_loss = 0.\n",
    "\n",
    "#         with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "#             loss = xl_wrapper(x_valid)\n",
    "\n",
    "#         save_checkpoint(optimizer, i + completed_iterations, train_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to Tensorboard: Train Loss: 1.4661, Val Loss: 1.2023\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "loss_fn = F.cross_entropy if not model.output_is_log_prob else F.nll_loss\n",
    "\n",
    "for i in tqdm.tqdm(range(NUM_BATCHES - completed_iterations), mininterval = 10., desc = 'training'):\n",
    "    model.train()\n",
    "\n",
    "    data = next(train_loader)[\"input_ids\"].to(device)\n",
    "    if data.shape[0] != BATCH_SIZE:\n",
    "        print(f'Skipping batch {i} as it is not of size {BATCH_SIZE}, but {data.shape[0]}')\n",
    "        data = next(train_loader)[\"input_ids\"].to(device)\n",
    "\n",
    "    train_loss = 0.\n",
    "    xl_memories = None    \n",
    "    x, y = data[:, :-1], data[:, 1:]\n",
    "\n",
    "    for seq_segment, labels_segment in zip(x.chunk(SEGMENTS, dim = -1), y.chunk(SEGMENTS, dim = -1)):\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits, xl_memories = model(\n",
    "                seq_segment,\n",
    "                mems = xl_memories,\n",
    "                return_mems = True,\n",
    "            )\n",
    "\n",
    "            loss = loss_fn(\n",
    "                rearrange(logits, 'b n c -> b c n'),\n",
    "                labels_segment,\n",
    "                ignore_index = tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            # loss = torch.clamp(loss, max=5.0)\n",
    "\n",
    "        train_loss += loss.item() / SEGMENTS\n",
    "        scaler.scale(loss / SEGMENTS).backward()\n",
    "\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_CLIP_NORM)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    if not (i % VALIDATE_EVERY):\n",
    "        model.eval()\n",
    "\n",
    "        valid_data = next(valid_loader)[\"input_ids\"].to(device)\n",
    "        if valid_data.shape[0] != BATCH_SIZE:\n",
    "            print(f'Skipping validation batch {i} as it is not of size {BATCH_SIZE}, but {valid_data.shape[0]}')\n",
    "            valid_data = next(valid_loader)[\"input_ids\"].to(device)\n",
    "\n",
    "        valid_loss = 0.\n",
    "\n",
    "        xl_memories = None    \n",
    "        x, y = valid_data[:, :-1], valid_data[:, 1:]\n",
    "\n",
    "        for seq_segment, labels_segment in zip(x.chunk(SEGMENTS, dim = -1), y.chunk(SEGMENTS, dim = -1)):\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits, xl_memories = model(\n",
    "                    seq_segment,\n",
    "                    mems = xl_memories,\n",
    "                    return_mems = True,\n",
    "                )\n",
    "\n",
    "                loss = loss_fn(\n",
    "                    rearrange(logits, 'b n c -> b c n'),\n",
    "                    labels_segment,\n",
    "                    ignore_index = tokenizer.pad_token_id\n",
    "                )\n",
    "\n",
    "                # loss = torch.clamp(loss, max=5.0)\n",
    "\n",
    "            valid_loss += loss.item() / SEGMENTS\n",
    "\n",
    "        save_checkpoint(optimizer, i + completed_iterations, train_loss, valid_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
