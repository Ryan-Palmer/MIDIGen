{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'10 - MIDI X-Transformers')\n",
    "import torch\n",
    "from miditok import REMI, TokenizerConfig  # here we choose to use REMI\n",
    "from pathlib import Path\n",
    "import random\n",
    "from x_transformers import TransformerWrapper, Decoder\n",
    "from miditok.utils import split_files_for_training\n",
    "from miditok.data_augmentation import augment_dataset\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from torch.utils.data import DataLoader\n",
    "from miditok import TokSequence\n",
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import IPython.display\n",
    "import music21 as m21\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "musescore_path = '/usr/bin/mscore'\n",
    "m21.environment.set('musicxmlPath', musescore_path)\n",
    "m21.environment.set('musescoreDirectPNGPath', musescore_path)\n",
    "\n",
    "device = ( \n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cuda\":\n",
    "    print(f\"Device: {torch.cuda.get_device_name()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_LENGTH = 16384 + 1 # Offset by 1 for the labels\n",
    "TIMESTEPS = 2048\n",
    "SEGMENTS = (CHUNK_LENGTH - 1) // TIMESTEPS\n",
    "BATCH_SIZE = 4\n",
    "VOCAB_SIZE = 1024 # REMI basic untrained token count = 411, +chords = 425, +tempos = 457, +time sig = 530, +rests = 562, +chord root note = 716, +pitch = 748\n",
    "N_EMBED = 768\n",
    "N_LAYER = 12\n",
    "N_HEAD = 12\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-3\n",
    "NUM_BATCHES = int(1e5)\n",
    "MAX_GRAD_CLIP_NORM = 0.5\n",
    "VALIDATE_EVERY  = 64\n",
    "DIM_HEAD = N_EMBED // N_HEAD\n",
    "VERSION_LABEL = \"X_T-12H-12L-emb-768\"\n",
    "TOKENIZER_CONFIG = 'All_Options_Pitch'\n",
    "\n",
    "midi_path = Path(f'../data/midi')\n",
    "dataset_name = 'lakh_clean_vg_large'\n",
    "midi_dataset_path = Path(f'{midi_path}/{dataset_name}')\n",
    "\n",
    "unfiltered_file_paths = [p.resolve() for p in midi_dataset_path.glob(\"**/*.mid\")]\n",
    "print(f\"Original number of files: {len(unfiltered_file_paths)}\")\n",
    "\n",
    "# There are some files with the same name but different versions (e.g. example.mid, example.1.mid, example.2.mid, etc.)\n",
    "# We need to filter out duplicate songs keeping only the first version otherwise the model can end up with the same song in the training and validation sets.\n",
    "filtered_paths = {}\n",
    "for path in unfiltered_file_paths:\n",
    "    # Extract base filename without the numeric suffix\n",
    "    filename = path.name\n",
    "    base_name = filename\n",
    "    parts = filename.rsplit('.', 2)\n",
    "    if len(parts) > 2 and parts[-2].isdigit():  # If the second-to-last part is a number\n",
    "        base_name = parts[0] + '.mid'  # This is the base filename without the numeric suffix\n",
    "    # Keep only the first occurrence of each song\n",
    "    if base_name not in filtered_paths:\n",
    "        filtered_paths[base_name] = path\n",
    "\n",
    "midi_file_paths = list(filtered_paths.values())\n",
    "print(f\"After filtering duplicates: {len(midi_file_paths)}\")\n",
    "\n",
    "tokenizer_save_path = Path(f'../data/vocab/MidiTok/{dataset_name}_{VOCAB_SIZE}_{TOKENIZER_CONFIG}.json')\n",
    "\n",
    "if not tokenizer_save_path.exists():\n",
    "    TOKENIZER_PARAMS = {\n",
    "        \"pitch_range\": (21, 109),\n",
    "        \"beat_res\": {(0, 4): 8, (4, 12): 4},\n",
    "        \"num_velocities\": 32,\n",
    "        \"use_programs\": True,\n",
    "        \"program_changes \": True, # Only insert program changes when the instrument changes rather than before every note\n",
    "        \"use_chords\": True,\n",
    "        \"chord_tokens_with_root_note\": True, # Include the root note in the chord token\n",
    "        \"use_pitch_bends\": True,\n",
    "        \"use_time_signatures\": True,\n",
    "        \"delete_equal_successive_time_sig_changes\": True, # Only insert time signatures when the time signature changes\n",
    "        \"use_tempos\": True,\n",
    "        \"delete_equal_successive_tempo_changes \": True, # Only insert tempos when the tempo changes after downsampling\n",
    "        \"use_rests\": True,\n",
    "    }\n",
    "    tokenizer_confg = TokenizerConfig(**TOKENIZER_PARAMS)\n",
    "    tokenizer = REMI(tokenizer_confg)\n",
    "    print(f\"Untrained token count: {tokenizer.len}\")\n",
    "    tokenizer.train(vocab_size=VOCAB_SIZE, files_paths=midi_file_paths)\n",
    "    tokenizer.save(tokenizer_save_path)\n",
    "else:\n",
    "    tokenizer = REMI(params=tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split files into train / test / validation folder when we chunk them, so no requirement to set seed here\n",
    "# random.seed(42)\n",
    "\n",
    "random.shuffle(midi_file_paths)\n",
    "len(midi_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = int(0.8 * len(midi_file_paths))\n",
    "n2 = int(0.9 * len(midi_file_paths))\n",
    "train_filepaths = midi_file_paths[:n1]\n",
    "valid_filepaths = midi_file_paths[n1:n2]\n",
    "test_filepaths = midi_file_paths[n2:]\n",
    "\n",
    "print(f'Train files: {len(train_filepaths)}, Valid files: {len(valid_filepaths)}, Test files: {len(test_filepaths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_path = Path(f'{midi_path}/mtok_split/{dataset_name}/v-{VOCAB_SIZE}_t-{TOKENIZER_CONFIG}_c-{CHUNK_LENGTH}')\n",
    "train_chunk_path = Path(f'{chunk_path}/train')\n",
    "valid_chunk_path = Path(f'{chunk_path}/valid')\n",
    "test_chunk_path = Path(f'{chunk_path}/test')\n",
    "\n",
    "split_data = [\n",
    "    (train_filepaths, train_chunk_path),\n",
    "    (valid_filepaths, valid_chunk_path),\n",
    "    (test_filepaths, test_chunk_path)\n",
    "]\n",
    "\n",
    "def chunk_files(filepaths, tokenizer, chunks_dir, max_seq_len):\n",
    "    split_files_for_training(\n",
    "        files_paths=filepaths,\n",
    "        tokenizer=tokenizer,\n",
    "        save_dir=chunks_dir,\n",
    "        max_seq_len=max_seq_len,\n",
    "        num_overlap_bars=1\n",
    "    )\n",
    "    augment_dataset(\n",
    "        chunks_dir,\n",
    "        pitch_offsets=[-12, 12],\n",
    "        velocity_offsets=[-4, 4],\n",
    "        duration_offsets=[-0.5, 0.5],\n",
    "    )\n",
    "\n",
    "if not chunk_path.exists():\n",
    "    with Pool(processes=3) as pool:\n",
    "        pool.starmap(chunk_files, [(filepaths, tokenizer, chunks_dir, CHUNK_LENGTH) for filepaths, chunks_dir in split_data])\n",
    "\n",
    "# for filepaths, chunks_dir in split_data:\n",
    "#     chunk_files(filepaths, tokenizer, chunks_dir, CHUNK_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chunk_filepaths = list(train_chunk_path.glob(\"**/*.mid\"))\n",
    "valid_chunk_filepaths = list(valid_chunk_path.glob(\"**/*.mid\"))\n",
    "test_chunk_filepaths = list(test_chunk_path.glob(\"**/*.mid\"))\n",
    "\n",
    "print(f'Train chunks: {len(train_chunk_filepaths)}, Valid chunks: {len(valid_chunk_filepaths)}, Test chunks: {len(test_chunk_filepaths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_paths = [train_chunk_filepaths, valid_chunk_filepaths, test_chunk_filepaths]\n",
    "\n",
    "for chunk_path in chunk_paths:\n",
    "    random.shuffle(chunk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "def create_data_set(chunks_path, tokenizer, max_seq_len):\n",
    "    return DatasetMIDI(\n",
    "        pre_tokenize=False,\n",
    "        files_paths=chunks_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_len=max_seq_len,\n",
    "        bos_token_id=tokenizer[\"BOS_None\"],\n",
    "        eos_token_id=tokenizer[\"EOS_None\"])\n",
    "\n",
    "def create_data_loader (dataset, tokenizer, batch_size):\n",
    "    collator = DataCollator(tokenizer.pad_token_id) # copy_inputs_as_labels and shift_labels not needed as done by the transformer\n",
    "    return cycle(DataLoader(dataset=dataset, collate_fn=collator, batch_size=batch_size, num_workers=8, pin_memory=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = map(\n",
    "    lambda chunk_filepaths: create_data_set(chunk_filepaths, tokenizer, CHUNK_LENGTH),\n",
    "    chunk_paths\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = map(\n",
    "    lambda dataset: create_data_loader(dataset, tokenizer, BATCH_SIZE),\n",
    "    datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'memorizing_miditok_{dataset_name}_t-{TIMESTEPS}_v-{VOCAB_SIZE}_{VERSION_LABEL}'\n",
    "model_load_path = Path(f'../data/checkpoints/{model_name}.dat')\n",
    "model_save_path = Path(f'../data/checkpoints/{model_name}.dat')\n",
    "log_dir = Path(f'../tensorboard/{model_name}')\n",
    "tensorboard_writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerWrapper(\n",
    "    num_tokens = VOCAB_SIZE,\n",
    "    max_seq_len = TIMESTEPS,\n",
    "    max_mem_len = TIMESTEPS, # XL mem, remember it needs a different training loop (can use xl wrapper). requires rel_pos_bias or rotary_pos_emb\n",
    "    shift_mem_down = 1,  # Shift memory down for XL mem to enhance the range\n",
    "    num_memory_tokens = 20, # https://github.com/lucidrains/x-transformers/tree/main?tab=readme-ov-file#memory-transformers\n",
    "    l2norm_embed = True, # https://github.com/lucidrains/x-transformers/tree/main?tab=readme-ov-file#transformers-without-tears\n",
    "    use_abs_pos_emb = False,\n",
    "    # tie_embedding= True,\n",
    "    attn_layers = Decoder(\n",
    "        dim = N_EMBED,\n",
    "        depth = N_LAYER,\n",
    "        heads = N_HEAD,\n",
    "        rotary_pos_emb = True, # A form of relative positional embedding which is compatible with flash attention\n",
    "        attn_flash = True, # Massive speed up / mem reduction, incompatible with T5\n",
    "        layer_dropout = 0.1,   # Stochastic depth - dropout entire layer\n",
    "        attn_dropout = 0.1,    # Dropout post-attention\n",
    "        ff_dropout = 0.1,       # Feedforward dropout\n",
    "        ff_glu = True, # Leave on  - https://github.com/lucidrains/x-transformers/tree/main?tab=readme-ov-file#glu-variants-improve-transformer\n",
    "        ff_swish = True, # Use this with glu if you want to\n",
    "        use_scalenorm = True, # https://github.com/lucidrains/x-transformers/tree/main?tab=readme-ov-file#transformers-without-tears\n",
    "        ff_no_bias = True, # https://github.com/lucidrains/x-transformers/tree/main?tab=readme-ov-file#no-bias-in-feedforward\n",
    "        attn_qk_norm = True, #  l2 normalize the queries and keys along the head dimension before the dot product (cosine similarity), with the additional change of the scale being learned rather than static. The normalization prevents the attention operation from overflowing, and removes any need for numerical stability measures prior to softmax. Both are perennial problems when training transformers.\n",
    "        attn_qk_norm_dim_scale = True, # set this to True, in addition to `attn_qk_norm = True`\n",
    "    )\n",
    ").to(device)\n",
    "\n",
    "model = torch.compile(model) # Big speedup\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, fused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For visualizing embeddings from a trained model\n",
    "def visualize_model_embeddings(model, tokenizer):\n",
    "    if tokenizer.is_trained:\n",
    "        all_ids = torch.tensor(list(tokenizer.vocab_model.values())).to(device)\n",
    "        all_ids_cpu = torch.tensor(list(tokenizer.vocab_model.values()))\n",
    "    else:\n",
    "        all_ids = torch.tensor(list(tokenizer.vocab.values())).to(device)\n",
    "        all_ids_cpu = torch.tensor(list(tokenizer.vocab.values()))\n",
    "    \n",
    "    # Get embeddings from model\n",
    "    with torch.no_grad():\n",
    "        # Assuming model.wte is the token embedding layer (adjust as needed)\n",
    "        embeddings = model.token_emb(all_ids).detach().cpu().numpy()\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, perplexity=min(30, len(all_ids_cpu)-1))\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # Create categorical color mapping based on token types\n",
    "    # You can customize this based on your tokenizer's vocabulary\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=all_ids_cpu, cmap='tab20', alpha=0.7)\n",
    "    plt.colorbar(label='Token ID')\n",
    "    plt.title('Model Embeddings t-SNE Projection')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "def save_checkpoint(optimizer, completed_iterations, train_loss, val_loss):\n",
    "    tensorboard_writer.add_scalar('Loss/train', train_loss, completed_iterations)\n",
    "    tensorboard_writer.add_scalar('Loss/val', val_loss, completed_iterations)\n",
    "    clear_output(wait=True)\n",
    "    print(f'Writing to Tensorboard: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    torch.save({\n",
    "        'iter': completed_iterations,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, model_save_path)\n",
    "    visualize_model_embeddings(model, tokenizer)\n",
    "\n",
    "completed_iterations = 0\n",
    "if model_load_path.exists():\n",
    "    checkpoint = torch.load(model_load_path)\n",
    "    completed_iterations = checkpoint['iter']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print(f\"Loaded model from iteration {completed_iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy if not model.output_is_log_prob else F.nll_loss\n",
    "\n",
    "for i in tqdm.tqdm(range(NUM_BATCHES - completed_iterations), mininterval = 10., desc = 'training'):\n",
    "    model.train()\n",
    "\n",
    "    data = next(train_loader)[\"input_ids\"].to(device)\n",
    "    if data.shape[0] != BATCH_SIZE:\n",
    "        print(f'Skipping batch {i} as it is not of size {BATCH_SIZE}, but {data.shape[0]}')\n",
    "        data = next(train_loader)[\"input_ids\"].to(device)\n",
    "\n",
    "    train_loss = 0.\n",
    "    xl_memories = None    \n",
    "    x, y = data[:, :-1], data[:, 1:]\n",
    "\n",
    "    for seq_segment, labels_segment in zip(x.chunk(SEGMENTS, dim = -1), y.chunk(SEGMENTS, dim = -1)):\n",
    "        \n",
    "        with torch.cuda.amp.autocast(dtype=torch.bfloat16): # If we specify bfloat16, we don't need grad scaler\n",
    "            logits, xl_memories = model(\n",
    "                seq_segment,\n",
    "                mems = xl_memories,\n",
    "                return_mems = True,\n",
    "            )\n",
    "\n",
    "            loss = loss_fn(\n",
    "                rearrange(logits, 'b n c -> b c n'),\n",
    "                labels_segment,\n",
    "                ignore_index = tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        train_loss += loss.item() / SEGMENTS\n",
    "        (loss / SEGMENTS).backward()\n",
    "\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_CLIP_NORM)\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    if not (i % VALIDATE_EVERY):\n",
    "        model.eval()\n",
    "\n",
    "        valid_data = next(valid_loader)[\"input_ids\"].to(device)\n",
    "        if valid_data.shape[0] != BATCH_SIZE:\n",
    "            print(f'Skipping validation batch {i} as it is not of size {BATCH_SIZE}, but {valid_data.shape[0]}')\n",
    "            valid_data = next(valid_loader)[\"input_ids\"].to(device)\n",
    "\n",
    "        valid_loss = 0.\n",
    "\n",
    "        xl_memories = None    \n",
    "        x, y = valid_data[:, :-1], valid_data[:, 1:]\n",
    "\n",
    "        for seq_segment, labels_segment in zip(x.chunk(SEGMENTS, dim = -1), y.chunk(SEGMENTS, dim = -1)):\n",
    "            \n",
    "            with torch.cuda.amp.autocast(dtype=torch.bfloat16): # If we specify bfloat16, we don't need grad scaler\n",
    "                logits, xl_memories = model(\n",
    "                    seq_segment,\n",
    "                    mems = xl_memories,\n",
    "                    return_mems = True,\n",
    "                )\n",
    "\n",
    "                loss = loss_fn(\n",
    "                    rearrange(logits, 'b n c -> b c n'),\n",
    "                    labels_segment,\n",
    "                    ignore_index = tokenizer.pad_token_id\n",
    "                )\n",
    "\n",
    "            valid_loss += loss.item() / SEGMENTS\n",
    "\n",
    "        save_checkpoint(optimizer, i + completed_iterations, train_loss, valid_loss) # TODO: print grad norm too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_path = random.choice(test_chunk_filepaths)\n",
    "random_test_data = tokenizer.encode(random_test_path)\n",
    "print(f'Random test file: {random_test_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symusic import Synthesizer\n",
    "synth = Synthesizer()\n",
    "\n",
    "random_start_tokens = random_test_data.tokens[TIMESTEPS:2*TIMESTEPS] # TODO: Think about checking / enforcing the correct length\n",
    "random_start_ids = random_test_data.ids[TIMESTEPS:2*TIMESTEPS]\n",
    "\n",
    "random_start_decoded = tokenizer.decode(random_start_tokens)\n",
    "audio = synth.render(random_start_decoded)\n",
    "IPython.display.Audio(audio, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_start_decoded.dump_midi(\"jesus.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def generate(\n",
    "        model,\n",
    "        input_ids,\n",
    "        max_length,\n",
    "        eos_token_id = None,\n",
    "        temperature = 1.0,\n",
    "        top_k = 0,\n",
    "        top_p = 0.9\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generates text autoregressively from the model.\n",
    "        \n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Input token ids [batch_size, seq_len]\n",
    "            max_length (int): Maximum length to generate\n",
    "            eos_token_id (int, optional): EOS token id for early stopping. Defaults to None.\n",
    "            temperature (float, optional): Sampling temperature. Defaults to 1.0.\n",
    "            top_k (int, optional): K for top-k sampling. Defaults to 0 (disabled).\n",
    "            top_p (float, optional): P for nucleus sampling. Defaults to 0.9.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Generated token ids [batch_size, max_length]\n",
    "        \"\"\"\n",
    "        xl_memories = None\n",
    "        timesteps = input_ids.shape[1]            \n",
    "            \n",
    "        # For tracking generated sequence\n",
    "        generated = input_ids.clone()\n",
    "    \n",
    "        # Generate tokens\n",
    "        for _ in range(max_length - input_ids.shape[1]):\n",
    "            # Create a copy of generated that is cropped if needed for the forward pass\n",
    "            input_for_model = generated\n",
    "            if generated.shape[1] > timesteps:\n",
    "                input_for_model = generated[:, -timesteps:]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, xl_memories = model(\n",
    "                input_for_model,\n",
    "                mems = xl_memories,\n",
    "                return_mems = True,\n",
    "            )\n",
    "            \n",
    "            # Get logits for next token prediction\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Apply temperature\n",
    "            if temperature != 1.0:\n",
    "                next_token_logits = next_token_logits / temperature\n",
    "            \n",
    "            # Apply top-k filtering\n",
    "            if top_k > 0:\n",
    "                indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
    "                next_token_logits[indices_to_remove] = -float('Inf')\n",
    "                \n",
    "            # Apply top-p (nucleus) filtering\n",
    "            if top_p < 1.0:\n",
    "                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                \n",
    "                # Remove tokens with cumulative probability above the threshold\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                # Shift the indices to the right to keep also the first token above the threshold\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                \n",
    "                # Scatter sorted tensors to original indexing\n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "                    dim=1, index=sorted_indices, src=sorted_indices_to_remove\n",
    "                )\n",
    "                next_token_logits[indices_to_remove] = -float('Inf')\n",
    "            \n",
    "            # Sample next token\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append next token to generated sequence\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            \n",
    "            # Check if all sequences have hit the EOS token\n",
    "            if exists(eos_token_id) and (next_token == eos_token_id).all():\n",
    "                break\n",
    "            \n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([random_start_ids]).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "generated = generate(\n",
    "        model = model,\n",
    "        input_ids = input_ids,\n",
    "        max_length = 2*TIMESTEPS,\n",
    "        eos_token_id = tokenizer[\"EOS_None\"]\n",
    "    ).detach().cpu()[0]\n",
    "    \n",
    "score = tokenizer.decode(generated)\n",
    "generated_decoded = tokenizer.decode(generated)\n",
    "audio = synth.render(generated_decoded)\n",
    "IPython.display.Audio(audio, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symusic import dump_wav\n",
    "# dump_wav(f\"{VERSION_LABEL}-The Beach Boys-Do It Again 21 sec prompt.wav\", audio, sample_rate=44100)\n",
    "generated_decoded.dump_midi(\"jesus-gen.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does torch.cuda.synchronize() fix the XL autoregressive loader oom?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
