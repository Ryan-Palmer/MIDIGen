{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'5 - TransformerXL')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from itertools import chain\n",
    "from itertools import groupby\n",
    "from functools import reduce\n",
    "from typing import Collection, List\n",
    "from pathlib import Path\n",
    "import music21 as m21\n",
    "musescore_path = '/usr/bin/mscore'\n",
    "m21.environment.set('musicxmlPath', musescore_path)\n",
    "m21.environment.set('musescoreDirectPNGPath', musescore_path)\n",
    "from midi_encoding import *\n",
    "from einops import rearrange, repeat, pack, unpack, einsum\n",
    "import faiss\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4090.\n"
     ]
    }
   ],
   "source": [
    "if device == \"cuda\":\n",
    "    print(f\"Device: {torch.cuda.get_device_name()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 19 14:45:34 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.31.01              Driver Version: 560.81         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0  On |                  Off |\n",
      "| 30%   33C    P0             44W /  450W |    2962MiB /  24564MiB |     34%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A        29      G   /Xwayland                                   N/A      |\n",
      "|    0   N/A  N/A        37      G   /Xwayland                                   N/A      |\n",
      "|    0   N/A  N/A        45      G   /Xwayland                                   N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = MusicVocab()\n",
    "vocab.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "\n",
    "Our memory-augmented transformer will be very similar to the vanilla model developed in the previous notebook.\n",
    "\n",
    "There will be three major additions, all of which are fairly simple:\n",
    "\n",
    "- Relative positional embeddings.\n",
    "- KNN lookup for keys (and their associated values).\n",
    "- Recurrent 'TransformerXL' style memory.\n",
    "\n",
    "## Einops\n",
    "\n",
    "First, lets condense our previous implementation in two ways\n",
    "\n",
    "- Add a dimension to our `MultiHeadAttention` module, abandoning the separate `SelfAttentionHead` module.\n",
    "- Switch to using einops for shape manipulation as it is both simpler to write and read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_head = 8, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.n_head = n_head\n",
    "        self.head_size = n_embed // n_head\n",
    "        head_total_size = n_head * self.head_size\n",
    "        self.key = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.query = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.proj = torch.nn.Linear(head_total_size, n_embed)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # Split heads\n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        k = rearrange(k, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        v = rearrange(v, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "\n",
    "        # Without einsum we had to swap dims using k.transpose(-2, -1)\n",
    "        w = einsum(q, k, 'b h i d, b h j d -> b h i j') * (self.head_size ** -0.5)\n",
    "        \n",
    "        # TODO: Relative positional encoding\n",
    "\n",
    "        i, j = w.shape[-2:]\n",
    "        mask = torch.tril(torch.ones((i,j), dtype = torch.bool))\n",
    "        w = w.masked_fill(mask, float('-inf'))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        attention_scores = w@v\n",
    "\n",
    "        # Concat heads\n",
    "        attention_scores = rearrange(attention_scores, 'b h t d -> b t (h d)')\n",
    "\n",
    "        # TODO: KNN memory\n",
    "\n",
    "        out = self.proj(attention_scores)\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Memory\n",
    "\n",
    "If we want to add KNN memory, we need an indexed data store to look up and retrieve keys and values.\n",
    "\n",
    "For the index we can use [Faiss](https://github.com/facebookresearch/faiss) from Meta.\n",
    "\n",
    "For the data store we can simply use a memory-mapped numpy array.\n",
    "\n",
    "The following code is adapted from the [Colab Notebook](https://colab.research.google.com/drive/1XZz1sjNt1MKRG6ul_hOGSJFQLS4lRtmJ?usp=sharing#scrollTo=gs7RpvCdePZr) accompanying the [Coding a Paper](https://www.youtube.com/playlist?list=PLam9sigHPGwOe8VDoS_6VT4jjlgs9Uepb) series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN():\n",
    "    def __init__(self, dim, max_memories, db_filepath):\n",
    "        self.dim = dim\n",
    "        self.max_memories = max_memories\n",
    "        self.shape = (max_memories, 2, dim) # TODO: Memory per batch dim, cleared when file is exhausted\n",
    "        self.db_offset = 0\n",
    "        self.db_filepath = db_filepath\n",
    "        self.db = np.memmap(self.db_filepath, mode = 'w+', dtype = np.float32, shape = self.shape)\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "\n",
    "    def add_to_db(self, new_data):\n",
    "        new_data_len = new_data.shape[0] # (b t)\n",
    "        ids = (np.arange(new_data_len) + self.db_offset)\n",
    "        self.db[ids] = new_data.detach().numpy()\n",
    "        self.db_offset += new_data_len\n",
    "        # Write to file\n",
    "        self.db.flush()\n",
    "\n",
    "\n",
    "    def search_and_retrieve(self, query, k):\n",
    "\n",
    "        # The tooltip says the args are (n, x, k) but that's the CPP api, it's actually (x, k) in Python (n is the first dim of x anyway so can be inferred).\n",
    "        distances, indices = self.index.search(query, k)\n",
    "        \n",
    "        kvs = self.db[indices]\n",
    "        return kvs\n",
    "\n",
    "    def add(self, new_data):\n",
    "        new_data = rearrange(new_data, 'b t two c -> (b t) two c')\n",
    "\n",
    "        # Add to db\n",
    "        self.add_to_db(new_data)\n",
    "\n",
    "        # Only keys are used in knn index\n",
    "        keys, vals = new_data.unbind(dim=-2)\n",
    "\n",
    "        # Add (b t) c tensors to index\n",
    "        keys = keys.detach().numpy()\n",
    "        keys = np.ascontiguousarray(keys)\n",
    "        self.index.add(keys)\n",
    "\n",
    "    def search(self, query, k):\n",
    "        T = query.shape[1]\n",
    "        query = rearrange(query, 'b t c -> (b t) c')\n",
    "\n",
    "        kvs = self.search_and_retrieve(np.ascontiguousarray(query.detach().numpy()), k)\n",
    "        kvs = torch.tensor(kvs)\n",
    "        kvs = kvs = rearrange(kvs, '(b t) k two c -> b t k two c', t = T)\n",
    "        \n",
    "        return kvs\n",
    "\n",
    "\n",
    "    def clear(self):\n",
    "        self.index.reset()\n",
    "        self.db[:] = 0\n",
    "        self.db_offset = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2857, 0.8111, 0.4562, 0.2322],\n",
       "         [0.8220, 0.7890, 0.6071, 0.4583]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 4\n",
    "t = 2\n",
    "\n",
    "knn = KNN(c, 100000, '../data/numpy/knn-test.db')\n",
    "\n",
    "vector_data = torch.tensor(np.random.random((1000, t, 2, c)).astype('float32'))\n",
    "knn.add(vector_data)\n",
    "\n",
    "query_data = torch.tensor(np.random.random((1, t, c)).astype('float32'))\n",
    "query_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search returns a `(b t k 2 c)` tensor which contains the top_k keys and values for each `b t c` query.\n",
    "\n",
    "Here our query is `1 * 2 * 4` so our results will be `(1 * 2 * 2 * 2 * 4)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0.3740, 0.7438, 0.4960, 0.2564],\n",
       "           [0.9457, 0.5970, 0.5391, 0.2538]],\n",
       "\n",
       "          [[0.2522, 0.9185, 0.4160, 0.2056],\n",
       "           [0.6594, 0.8644, 0.6513, 0.3425]]],\n",
       "\n",
       "\n",
       "         [[[0.7901, 0.8044, 0.6787, 0.4075],\n",
       "           [0.6474, 0.6122, 0.0258, 0.6570]],\n",
       "\n",
       "          [[0.7565, 0.8019, 0.6250, 0.3903],\n",
       "           [0.2035, 0.2504, 0.8388, 0.0084]]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k = 2\n",
    "knn.search(query_data, top_k) # (b t k two c) tensor, returns top_k keys and values for each query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can integrate the memory into our multiheaded attention.\n",
    "\n",
    "We will make a new class for this as we only use KNN on the second to last layer.\n",
    "\n",
    "It will have a KNN memory for each batch dimension, and we will clear that memory if the file in that batch dimension changes.\n",
    "\n",
    "We will know this as the `CustomMidiDataset` returns the file indices of each batch along with the data. \n",
    "\n",
    "These can be passed to our model, which in turn can pass them to the KNN attention block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, n_embed, k, n_head = 8, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.k = k\n",
    "        self.n_head = n_head\n",
    "        self.head_size = n_embed // n_head\n",
    "        head_total_size = n_head * self.head_size\n",
    "        self.key = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.query = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.proj = torch.nn.Linear(head_total_size, n_embed)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Memory per batch dim\n",
    "        self.knn = {i: KNN(n_embed, 100000, f'../data/numpy/knn-{i}.db') for i in range(batch_size)}\n",
    "        self.current_files = None\n",
    "\n",
    "        self.gate_bias = torch.nn.Parameter(torch.randn(self.n_head, 1, 1))\n",
    "    \n",
    "\n",
    "    def forward(self, x, batch_files):\n",
    "\n",
    "        # Clear batch dim's knn memory if file changes\n",
    "        if self.current_files != None:\n",
    "            for i in range(len(self.current_files)):\n",
    "                if self.current_files[i] != batch_files[i]:\n",
    "                    self.knn[i].clear()\n",
    "\n",
    "        self.current_files = batch_files\n",
    "\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # Split heads\n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        k = rearrange(k, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        v = rearrange(v, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "\n",
    "        # Without einsum we had to swap dims using k.transpose(-2, -1)\n",
    "        w = einsum(q, k, 'b h i d, b h j d -> b h i j') * (self.head_size ** -0.5)\n",
    "        \n",
    "        # TODO: Relative positional encoding\n",
    "\n",
    "        i, j = w.shape[-2:]\n",
    "        mask = torch.tril(torch.ones((i,j), dtype = torch.bool))\n",
    "        w = w.masked_fill(mask, float('-inf'))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        attention_scores = w@v\n",
    "\n",
    "        # Concat heads\n",
    "        attention_scores = rearrange(attention_scores, 'b h t d -> b t (h d)')\n",
    "\n",
    "\n",
    "\n",
    "        # KNN memory\n",
    "\n",
    "        # Convert queries to search form\n",
    "        q = rearrange(q, 'b h t d -> b t (h d)')\n",
    "        mem_kv = torch.stack([self.knn[i].search(q[i:i+1], k = self.k) for i in range(B)], dim = 0) # returns b t k 2 c\n",
    "        mem_k, mem_v = mem_kv.unbind(dim = -2)\n",
    "        mem_k = rearrange(mem_k, 'b t k (h d) -> b h t k d', h=self.n_head)\n",
    "        mem_v = rearrange(mem_v, 'b t k (h d) -> b h t k d', h=self.n_head)\n",
    "\n",
    "        # Convert queries to attention form\n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        mem_w = einsum('b h t d, b h t k d -> b h t k', q, mem_k)\n",
    "        mem_w = mem_w * (self.head_size ** -0.5)\n",
    "\n",
    "        mem_w = F.softmax(mem_w, dim=-1)\n",
    "        mem_attention_scores = einsum('b h t k, b h t k d -> b h t d', mem_w, mem_v)\n",
    "\n",
    "        # Combined attentions\n",
    "        combined_attention_scores = mem_attention_scores * self.gate_bias + attention_scores * (1 - self.gate_bias)\n",
    "        combined_attention_scores = rearrange(combined_attention_scores, 'b h t d -> b t (h d)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        out = self.proj(combined_attention_scores)\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- Add KNN lookup to vanilla transformer\n",
    "- XL recurrence\n",
    "- Relative Positional embeddings\n",
    "\n",
    "Once all that is working, consider\n",
    "\n",
    "- Ragged memmap for data loading (allows moving to bigger dataset)\n",
    "- Byte pair encoding (bigger vocab with common token pairs gives us a bigger effective context) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
