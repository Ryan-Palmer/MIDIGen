{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'5 - TransformerXL')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from itertools import chain\n",
    "from itertools import groupby\n",
    "from functools import reduce\n",
    "from typing import Collection, List\n",
    "from pathlib import Path\n",
    "import music21 as m21\n",
    "musescore_path = '/usr/bin/mscore'\n",
    "m21.environment.set('musicxmlPath', musescore_path)\n",
    "m21.environment.set('musescoreDirectPNGPath', musescore_path)\n",
    "from midi_encoding import *\n",
    "from einops import rearrange, repeat, pack, unpack, einsum\n",
    "import faiss\n",
    "import time\n",
    "import math\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4090.\n"
     ]
    }
   ],
   "source": [
    "if device == \"cuda\":\n",
    "    print(f\"Device: {torch.cuda.get_device_name()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 23 14:50:52 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0  On |                  Off |\n",
      "| 30%   32C    P0             49W /  450W |    1946MiB /  24564MiB |     40%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A        29      G   /Xwayland                                   N/A      |\n",
      "|    0   N/A  N/A        48      G   /Xwayland                                   N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = MusicVocab()\n",
    "vocab.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "\n",
    "Our memory-augmented transformer will be very similar to the vanilla model developed in the previous notebook.\n",
    "\n",
    "There will be three major additions, all of which are fairly simple:\n",
    "\n",
    "- Relative positional embeddings.\n",
    "- KNN lookup for keys (and their associated values).\n",
    "- Recurrent 'TransformerXL' style memory.\n",
    "\n",
    "## Einops\n",
    "\n",
    "First, lets condense our previous implementation in two ways\n",
    "\n",
    "- Add a dimension to our `MultiHeadAttention` module, abandoning the separate `SelfAttentionHead` module.\n",
    "- Switch to using einops for shape manipulation as it is both simpler to write and read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_head = 8, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.n_head = n_head\n",
    "        self.head_size = n_embed // n_head\n",
    "        head_total_size = n_head * self.head_size\n",
    "        self.key = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.query = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.proj = torch.nn.Linear(head_total_size, n_embed)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # Split heads\n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        k = rearrange(k, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        v = rearrange(v, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "\n",
    "        # Without einsum we had to swap dims using k.transpose(-2, -1)\n",
    "        w = einsum(q, k, 'b h i d, b h j d -> b h i j') * (self.head_size ** -0.5)\n",
    "        \n",
    "        # TODO: Relative positional encoding\n",
    "\n",
    "        i, j = w.shape[-2:]\n",
    "        mask = torch.tril(torch.ones((i,j), dtype = torch.bool))\n",
    "        w = w.masked_fill(mask, float('-inf'))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        weighted_values = w@v\n",
    "\n",
    "        # Concat heads\n",
    "        weighted_values = rearrange(weighted_values, 'b h t d -> b t (h d)')\n",
    "\n",
    "        # TODO: KNN memory\n",
    "\n",
    "        out = self.proj(weighted_values)\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Memory\n",
    "\n",
    "If we want to add KNN memory, we need an indexed data store to look up and retrieve keys and values.\n",
    "\n",
    "For the index we can use [Faiss](https://github.com/facebookresearch/faiss) from Meta.\n",
    "\n",
    "For the data store we can simply use a memory-mapped numpy array.\n",
    "\n",
    "The following code is adapted from the [Colab Notebook](https://colab.research.google.com/drive/1XZz1sjNt1MKRG6ul_hOGSJFQLS4lRtmJ?usp=sharing#scrollTo=gs7RpvCdePZr) accompanying the [Coding a Paper](https://www.youtube.com/playlist?list=PLam9sigHPGwOe8VDoS_6VT4jjlgs9Uepb) series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN():\n",
    "    def __init__(self, dim, max_memories, db_filepath):\n",
    "        self.dim = dim\n",
    "        self.max_memories = max_memories\n",
    "        self.shape = (max_memories, 2, dim)\n",
    "        self.db_offset = 0\n",
    "        if db_filepath.exists():\n",
    "            dbMode = 'r+'\n",
    "        else:\n",
    "            dbMode = 'w+' # Create file if it doesn't exist\n",
    "        self.db = np.memmap(db_filepath, mode = dbMode, dtype = np.float32, shape = self.shape)\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "    def add_to_db(self, new_data):\n",
    "        new_data_len = new_data.shape[0] # (t)\n",
    "        ids = (np.arange(new_data_len) + self.db_offset)\n",
    "        self.db[ids] = new_data.detach().numpy()\n",
    "        self.db_offset += new_data_len\n",
    "        # Write to file\n",
    "        self.db.flush()\n",
    "\n",
    "    def search_and_retrieve(self, query, k):\n",
    "\n",
    "        # The tooltip says the args are (n, x, k) but that's the CPP api, it's actually (x, k) in Python (n is the first dim of x anyway so can be inferred).\n",
    "        distances, indices = self.index.search(query, k)\n",
    "        \n",
    "        kvs = self.db[indices]\n",
    "        return kvs\n",
    "\n",
    "    def add(self, new_data):\n",
    "        # new_data = (t, 2, c)\n",
    "\n",
    "        # Add to db\n",
    "        self.add_to_db(new_data)\n",
    "\n",
    "        # Only keys are used in knn index\n",
    "        keys, vals = new_data.unbind(dim=-2)\n",
    "\n",
    "        # Add (t, c) tensors to index\n",
    "        keys = keys.detach().numpy()\n",
    "        keys = np.ascontiguousarray(keys)\n",
    "        self.index.add(keys)\n",
    "\n",
    "    def search(self, query, k):\n",
    "\n",
    "        T, C = query.shape\n",
    "        \n",
    "        # If we have enough memories, search and retrieve, otherwise return zeros\n",
    "        if self.index.ntotal >= k:\n",
    "            kvs = self.search_and_retrieve(np.ascontiguousarray(query.detach().numpy()), k)\n",
    "            kvs = torch.tensor(kvs)\n",
    "        else:\n",
    "            kvs = torch.zeros((T, k, 2, C), device=device)\n",
    "\n",
    "        return kvs\n",
    "\n",
    "    def clear(self):\n",
    "        self.index.reset()\n",
    "        self.db[:] = 0\n",
    "        self.db_offset = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2238, 0.3509, 0.1005, 0.2910],\n",
       "        [0.9363, 0.2582, 0.4334, 0.8027]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 4\n",
    "t = 2\n",
    "\n",
    "knn = KNN(c, 100000, Path('../data/numpy/knn-test.db'))\n",
    "\n",
    "for i in range(1000):\n",
    "    vector_data = torch.tensor(np.random.random((t, 2, c)).astype('float32'))\n",
    "    knn.add(vector_data)\n",
    "\n",
    "query_data = torch.tensor(np.random.random((t, c)).astype('float32'))\n",
    "query_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search returns a `(t k 2 c)` tensor which contains the top_k keys and values for each `(t c)` query.\n",
    "\n",
    "Here our query is `(2 * 4)` so our results will be `(2 * 2 * 2 * 4)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1990, 0.3517, 0.1679, 0.2943],\n",
       "          [0.9202, 0.7429, 0.7107, 0.8525]],\n",
       "\n",
       "         [[0.1331, 0.3914, 0.1099, 0.2552],\n",
       "          [0.3810, 0.6787, 0.6610, 0.5767]]],\n",
       "\n",
       "\n",
       "        [[[0.9419, 0.2734, 0.3867, 0.8327],\n",
       "          [0.4675, 0.3117, 0.3460, 0.1590]],\n",
       "\n",
       "         [[0.9230, 0.3176, 0.5073, 0.7970],\n",
       "          [0.5202, 0.4738, 0.0330, 0.5286]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k = 2\n",
    "knn.search(query_data, top_k) # (t k two c) tensor, returns top_k keys and values for each query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can integrate the memory into our multiheaded attention.\n",
    "\n",
    "We will make a new class for this as we only use KNN on the second to last layer.\n",
    "\n",
    "It will have a KNN memory for each batch dimension, and we will clear that memory if the file in that batch dimension changes.\n",
    "\n",
    "We will know this as the `CustomMidiDataset` returns the file indices of each batch along with the data. \n",
    "\n",
    "These can be passed to our model, which in turn can pass them to the KNN attention block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask:\n",
      " tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "Original w:\n",
      " tensor([[0.5835, 0.5431, 0.3137, 0.5969],\n",
      "        [0.8058, 0.2264, 0.0946, 0.2151],\n",
      "        [0.8888, 0.7451, 0.1404, 0.6317],\n",
      "        [0.9363, 0.1994, 0.5483, 0.1877]])\n",
      "Masked w:\n",
      " tensor([[0.7811,   -inf,   -inf,   -inf],\n",
      "        [0.6212, 0.6408,   -inf,   -inf],\n",
      "        [0.0985, 0.5483, 0.7571,   -inf],\n",
      "        [0.6869, 0.0328, 0.5287, 0.2488]])\n"
     ]
    }
   ],
   "source": [
    "# Define i and j\n",
    "i, j = 4, 4\n",
    "\n",
    "# Create the mask\n",
    "mask = torch.logical_not(torch.tril(torch.ones((i, j), dtype=torch.bool)))\n",
    "\n",
    "# Example tensor w\n",
    "w = torch.rand((i, j))\n",
    "\n",
    "# Apply the mask\n",
    "w = w.masked_fill(mask, float('-inf'))\n",
    "\n",
    "print(\"Mask:\\n\", mask)\n",
    "print(\"Original w:\\n\", torch.rand((i, j)))  # Example tensor before masking\n",
    "print(\"Masked w:\\n\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dbFilePath, batch_size,  k, n_embed, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.k = k\n",
    "        self.n_head = n_head\n",
    "        head_size = n_embed // n_head\n",
    "        self.scale_factor = head_size ** -0.5\n",
    "        self.key = torch.nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.query = torch.nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.project = torch.nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Memory per batch dim\n",
    "        self.knn = {i: KNN(n_embed, 100000, f'{dbFilePath}-batch_dim-{i}.db') for i in range(batch_size)} # KNN memory will get or create the files, so we just need to be consistent with the file names.\n",
    "        self.current_file_idxs = None\n",
    "\n",
    "        self.gate_bias = torch.nn.Parameter(torch.randn(self.n_head, 1, 1))\n",
    "    \n",
    "\n",
    "    def forward(self, relative_positions, batch_file_idxs, x):\n",
    "\n",
    "        # Clear batch dim's knn memory if file changes\n",
    "        if self.current_file_idxs != None:\n",
    "            for i in range(len(self.current_file_idxs)):\n",
    "                if self.current_file_idxs[i] != batch_file_idxs[i]:\n",
    "                    self.knn[i].clear()\n",
    "\n",
    "        self.current_file_idxs = batch_file_idxs\n",
    "\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # This helps to mitigate drift in the embeddings which can cause the historical keys to become less aligned to the current queries.\n",
    "        q = F.normalize(q, dim=-1)\n",
    "        k = F.normalize(k, dim=-1)\n",
    "\n",
    "        ### LOCAL ATTENTION\n",
    "\n",
    "        # Split heads\n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        k = rearrange(k, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        v = rearrange(v, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "\n",
    "        # Without einsum we had to swap dims using k.transpose(-2, -1)\n",
    "        w = einsum(q, k, 'b h i d, b h j d -> b h i j')\n",
    "        i, j = w.shape[-2:]\n",
    "        \n",
    "        # Add relative positional encoding and scale\n",
    "        w = w + relative_positions[..., -i:, -j:]\n",
    "        w = w * self.scale_factor\n",
    "\n",
    "        mask = torch.logical_not(torch.tril(torch.ones((i,j), dtype = torch.bool))) # Can't cache this as its shape depends on whether we have XL memory or not.\n",
    "        w = w.masked_fill(mask, float('-inf'))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        weighted_values = w@v # b h t d\n",
    "\n",
    "        ### KNN ATTENTION\n",
    "        knn_mask = torch.tensor([self.knn[i].index.ntotal > 0 for i in range(B)], dtype=torch.bool, device=device)\n",
    "\n",
    "        # Only do knn if there are at least some memories\n",
    "        if knn_mask.any():\n",
    "\n",
    "            t1 = time.time()\n",
    "            print (\"Begin KNN operations\")\n",
    "\n",
    "            # Convert queries to search form\n",
    "            q = rearrange(q, 'b h t d -> b t (h d)')\n",
    "\n",
    "            # KNN returns zeroes if it doesn't have data.\n",
    "            mem_kv = torch.stack([self.knn[i].search(q[i], k = self.k) for i in range(B)], dim = 0) # b, t, k, 2, c\n",
    "            \n",
    "            mem_k, mem_v = mem_kv.unbind(dim = -2)\n",
    "            mem_k = rearrange(mem_k, 'b t k (h d) -> b h t k d', h=self.n_head)\n",
    "            mem_v = rearrange(mem_v, 'b t k (h d) -> b h t k d', h=self.n_head)\n",
    "\n",
    "            # Convert queries to attention form\n",
    "            q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "\n",
    "            # Sum over d for each combination of batch, head, time and top k to get qk affinities, and hence weights for each k. resulting in a tensor of shape (b, h, t, k).\n",
    "            mem_w = einsum('b h t d, b h t k d -> b h t k', q, mem_k)\n",
    "            mem_w = mem_w * self.scale_factor\n",
    "            mem_w = F.softmax(mem_w, dim=-1)\n",
    "\n",
    "            # Weighted sum over the top k dimension for each combination of b, h, and t, resulting in a tensor of shape (b, h, t, d). Equivalent to doing w@v for each k and summing.\n",
    "            mem_weighted_values = einsum('b h t k, b h t k d -> b h t d', mem_w, mem_v)\n",
    "\n",
    "            ## Combined attention\n",
    "            \n",
    "            # Assume every memory has content. Empty memories will be masked out below.\n",
    "            combined_weighted_values = mem_weighted_values * self.gate_bias + weighted_values * (1 - self.gate_bias)\n",
    "\n",
    "            # Mask out combined weighted values where knn memory *is* empty and non-combined values where it *is not* empty, then merge them.\n",
    "            combined_weighted_values = combined_weighted_values * knn_mask.view(B, 1, 1, 1) + weighted_values * (~knn_mask).view(B, 1, 1, 1)\n",
    "\n",
    "            # Concat heads\n",
    "            combined_weighted_values = rearrange(combined_weighted_values, 'b h t d -> b t (h d)')\n",
    "            out = self.project(combined_weighted_values)\n",
    "\n",
    "            t2 = time.time()\n",
    "            print (\"End KNN operations, time taken:\", t2-t1)\n",
    "\n",
    "        else:\n",
    "            # Concat heads\n",
    "            weighted_values = rearrange(weighted_values, 'b h t d -> b t (h d)')\n",
    "            out = self.project(weighted_values)\n",
    "\n",
    "        current_kv = torch.stack((k, v), dim=-2) # (b, t, 2, c)\n",
    "        for i in range(B):\n",
    "            self.knn[i].add(current_kv[i])\n",
    "\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XL Memory\n",
    "\n",
    "We simply need to append the previous iteration's key and value tensors to the current, allowing the queries to search over / swap information from the previous timesteps (which did the same to the timestep before that etc etc), creating a kind of delay-line memory that fades out over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.n_head = n_head\n",
    "        self.head_size = n_embed // n_head\n",
    "        head_total_size = n_head * self.head_size\n",
    "        self.key = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.query = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.project = torch.nn.Linear(head_total_size, n_embed)\n",
    "        self.dropout = torch.nn.Dropout(dropout)    \n",
    "\n",
    "    def forward(self, relative_positions, x, xl_memory = None):\n",
    "\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # Chris's implementation does `queries = queries * (self.head_size ** -0.5)` here but I don't think it is correct.\n",
    "\n",
    "        if xl_memory is not None:\n",
    "            k_xl, v_xl = xl_memory.unbind(dim = -2) # assume stacked\n",
    "            keys = torch.cat((k_xl, keys), dim = -2) # prepend XL memory\n",
    "            values = torch.cat((v_xl, values), dim = -2) # prepend XL memory\n",
    "\n",
    "        ### LOCAL ATTENTION\n",
    "\n",
    "        # Split heads\n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        k = rearrange(k, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        v = rearrange(v, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "\n",
    "        w = einsum(q, k, 'b h i d, b h j d -> b h i j')\n",
    "        i, j = w.shape[-2:]\n",
    "\n",
    "        # Add relative positional encoding and scale\n",
    "        w = w + relative_positions[..., -i:, -j:]\n",
    "        w = w * (self.head_size ** -0.5)\n",
    "        \n",
    "        mask = torch.logical_not(torch.tril(torch.ones((i,j), dtype = torch.bool))) # Can't cache this as its shape depends on whether we have XL memory or not.\n",
    "        w = w.masked_fill(mask, float('-inf'))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        weighted_values = w@v # b h t d\n",
    "        # Concat heads\n",
    "        weighted_values = rearrange(weighted_values, 'b h t d -> b t (h d)')\n",
    "        \n",
    "        out = self.project(weighted_values)\n",
    "\n",
    "        # new XL memories\n",
    "\n",
    "        # Concatenate key and value heads\n",
    "        k = rearrange(k, 'b h t d -> b t (h d)', h = self.n_head)\n",
    "        v = rearrange(v, 'b h t d -> b t (h d)', h = self.n_head)\n",
    "        current_kv = torch.stack((k, v), dim=-2) # b t 2 (h d)\n",
    "\n",
    "        if xl_memory is None:\n",
    "            new_xl_memory = current_kv\n",
    "        else:\n",
    "            new_xl_memory = current_kv[:, -T:]\n",
    "\n",
    "        return self.dropout(out), new_xl_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN-XL\n",
    "\n",
    "Now we can add XL memory to out KNN Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_XLAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dbFilePath, batch_size, k, n_embed, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.k = k\n",
    "        self.n_head = n_head\n",
    "        head_size = n_embed // n_head\n",
    "        self.scale_factor = head_size ** -0.5\n",
    "        self.key = torch.nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.query = torch.nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.project = torch.nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Memory per batch dim\n",
    "        self.knn = {i: KNN(n_embed, 100000, Path(f'{dbFilePath}/batch_dim-{i}.db')) for i in range(batch_size)} # KNN memory will get or create the files, so we just need to be consistent with the file names.\n",
    "        self.current_file_idxs = None\n",
    "\n",
    "        self.gate_bias = torch.nn.Parameter(torch.randn(self.n_head, 1, 1))\n",
    "    \n",
    "\n",
    "    def forward(self, batch_file_idxs, relative_positions, x, xl_memory = None):\n",
    "\n",
    "        # Clear batch dim's knn memory if file changes\n",
    "        if self.current_file_idxs != None:\n",
    "            for i in range(len(self.current_file_idxs)):\n",
    "                if self.current_file_idxs[i] != batch_file_idxs[i]:\n",
    "                    self.knn[i].clear()\n",
    "\n",
    "        self.current_file_idxs = batch_file_idxs\n",
    "\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # This helps to mitigate drift in the embeddings which can cause the historical keys to become less aligned to the current queries.\n",
    "        q = F.normalize(q, dim=-1)\n",
    "        k = F.normalize(k, dim=-1)\n",
    "\n",
    "        if xl_memory is not None:\n",
    "            k_xl, v_xl = xl_memory.unbind(dim = -2) # assume stacked\n",
    "            keys = torch.cat((k_xl, keys), dim = -2) # prepend XL memory\n",
    "            values = torch.cat((v_xl, values), dim = -2) # prepend XL memory\n",
    "\n",
    "        ### LOCAL ATTENTION\n",
    "\n",
    "        # Split heads\n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        k = rearrange(k, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        v = rearrange(v, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "\n",
    "        w = einsum(q, k, 'b h i d, b h j d -> b h i j')\n",
    "        i, j = w.shape[-2:]\n",
    "        \n",
    "        # Add relative positional encoding and scale\n",
    "        w = w + relative_positions[..., -i:, -j:]\n",
    "        w = w * self.scale_factor\n",
    "\n",
    "        mask = torch.logical_not(torch.tril(torch.ones((i,j), dtype = torch.bool))) # Can't cache this as its shape depends on whether we have XL memory or not.\n",
    "        w = w.masked_fill(mask, float('-inf'))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        weighted_values = w@v # b h t d\n",
    "\n",
    "        ### KNN ATTENTION\n",
    "        knn_mask = torch.tensor([self.knn[i].index.ntotal > 0 for i in range(B)], dtype=torch.bool, device=device)\n",
    "\n",
    "        # Only do knn if there are at least some memories\n",
    "        if knn_mask.any():\n",
    "\n",
    "            t1 = time.time()\n",
    "            print (\"Begin KNN operations\")\n",
    "\n",
    "            # Convert queries to search form\n",
    "            q = rearrange(q, 'b h t d -> b t (h d)')\n",
    "\n",
    "            # KNN returns zeroes if it doesn't have data.\n",
    "            mem_kv = torch.stack([self.knn[i].search(q[i], k = self.k) for i in range(B)], dim = 0) # b, t, k, 2, c\n",
    "            \n",
    "            mem_k, mem_v = mem_kv.unbind(dim = -2)\n",
    "            mem_k = rearrange(mem_k, 'b t k (h d) -> b h t k d', h=self.n_head)\n",
    "            mem_v = rearrange(mem_v, 'b t k (h d) -> b h t k d', h=self.n_head)\n",
    "\n",
    "            # Convert queries to attention form\n",
    "            q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "\n",
    "            # Sum over d for each combination of batch, head, time and top k to get qk affinities, and hence weights for each k. resulting in a tensor of shape (b, h, t, k).\n",
    "            mem_w = einsum('b h t d, b h t k d -> b h t k', q, mem_k)\n",
    "            mem_w = mem_w * self.scale_factor\n",
    "            mem_w = F.softmax(mem_w, dim=-1)\n",
    "\n",
    "            # Weighted sum over the top k dimension for each combination of b, h, and t, resulting in a tensor of shape (b, h, t, d). Equivalent to doing w@v for each k and summing.\n",
    "            mem_weighted_values = einsum('b h t k, b h t k d -> b h t d', mem_w, mem_v)\n",
    "\n",
    "            ## Combined attention\n",
    "            \n",
    "            # Assume every memory has content. Empty memories will be masked out below.\n",
    "            combined_weighted_values = mem_weighted_values * self.gate_bias + weighted_values * (1 - self.gate_bias)\n",
    "\n",
    "            # Mask out combined weighted values where knn memory *is* empty and non-combined values where it *is not* empty, then merge them.\n",
    "            combined_weighted_values = combined_weighted_values * knn_mask.view(B, 1, 1, 1) + weighted_values * (~knn_mask).view(B, 1, 1, 1)\n",
    "\n",
    "            # Concat heads\n",
    "            combined_weighted_values = rearrange(combined_weighted_values, 'b h t d -> b t (h d)')\n",
    "            out = self.project(combined_weighted_values)\n",
    "\n",
    "            t2 = time.time()\n",
    "            print (\"End KNN operations, time taken:\", t2-t1)\n",
    "\n",
    "        else:\n",
    "            # Concat heads\n",
    "            weighted_values = rearrange(weighted_values, 'b h t d -> b t (h d)')\n",
    "            out = self.project(weighted_values)\n",
    "\n",
    "\n",
    "        # New XL memories\n",
    "\n",
    "        # Concatenate key and value heads\n",
    "        k = rearrange(k, 'b h t d -> b t (h d)', h = self.n_head)\n",
    "        v = rearrange(v, 'b h t d -> b t (h d)', h = self.n_head)\n",
    "        current_kv = torch.stack((k, v), dim=-2) # b t 2 c\n",
    "\n",
    "        if xl_memory is None:\n",
    "            new_xl_memory = current_kv\n",
    "        else:\n",
    "            new_xl_memory = current_kv[:, -T:]\n",
    "\n",
    "        for i in range(B):\n",
    "            self.knn[i].add(new_xl_memory[i])\n",
    "\n",
    "        return self.dropout(out), new_xl_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative Positional Embeddings\n",
    "\n",
    "In the above Attention classes we have passed relative embeddings to the `forward` methods, which are then added to the weights.\n",
    "\n",
    "This is in contrast to the fixed positional embeddings used by a vanilla transformer, which are applied at the input of the model.\n",
    "\n",
    "We haven't yet defined the relative embedding structure. It is very similar to fixed embeddings, with the 0 point offset by the current position as defined by the triangular mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, -1, -2, -3, -4, -5, -6],\n",
       "        [ 1,  0, -1, -2, -3, -4, -5],\n",
       "        [ 2,  1,  0, -1, -2, -3, -4],\n",
       "        [ 3,  2,  1,  0, -1, -2, -3],\n",
       "        [ 4,  3,  2,  1,  0, -1, -2],\n",
       "        [ 5,  4,  3,  2,  1,  0, -1],\n",
       "        [ 6,  5,  4,  3,  2,  1,  0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 7\n",
    "q_pos = torch.arange(block_size, dtype=torch.long)\n",
    "q_pos = q_pos.reshape(q_pos.shape[0], 1)\n",
    "k_pos = torch.arange(block_size, dtype=torch.long)\n",
    "rel_pos = k_pos - q_pos\n",
    "inv_rel_pos = -rel_pos\n",
    "inv_rel_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [2, 1, 0, 0, 0, 0, 0],\n",
       "        [3, 2, 1, 0, 0, 0, 0],\n",
       "        [4, 3, 2, 1, 0, 0, 0],\n",
       "        [5, 4, 3, 2, 1, 0, 0],\n",
       "        [6, 5, 4, 3, 2, 1, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_rel_pos = torch.max(inv_rel_pos, torch.zeros_like(inv_rel_pos))\n",
    "masked_rel_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than give every position its own index, beyond a certain distance we will group them into logarithmically bigger buckets as they get further from the current position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [2, 1, 0, 0, 0, 0, 0],\n",
       "        [3, 2, 1, 0, 0, 0, 0],\n",
       "        [3, 3, 2, 1, 0, 0, 0],\n",
       "        [3, 3, 3, 2, 1, 0, 0],\n",
       "        [4, 3, 3, 3, 2, 1, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_buckets = 6\n",
    "max_distance = 20\n",
    "max_exact = n_buckets // 2\n",
    "is_small = masked_rel_pos < max_exact\n",
    "val_if_large = max_exact + (torch.log(masked_rel_pos.float() / max_exact) / math.log(max_distance / max_exact) * (n_buckets - max_exact)).long()\n",
    "position_bucket_indices = torch.where(is_small, masked_rel_pos, val_if_large) # below a certain distance, use the raw value, otherwise use the log-scaled value\n",
    "position_bucket_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6],\n",
       "        [ -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5],\n",
       "        [ -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4],\n",
       "        [-10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3],\n",
       "        [-11, -10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2],\n",
       "        [-12, -11, -10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1],\n",
       "        [-13, -12, -11, -10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_pos = torch.arange(block_size, dtype=torch.long)\n",
    "# context_pos = torch.arange(2*block_size, dtype=torch.long)\n",
    "context_pos = torch.arange(-block_size, block_size, dtype=torch.long) # XL memory, context is twice block size, and current position starts in the middle.\n",
    "block_rel_pos = rearrange(block_pos, 'i -> i 1')\n",
    "context_rel_pos = rearrange(context_pos, 'j -> 1 j')\n",
    "rel_pos = context_rel_pos - block_rel_pos\n",
    "rel_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLRelativePosition(torch.nn.Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      n_buckets,\n",
    "      max_distance,\n",
    "      n_head,\n",
    "      scaling_factor):\n",
    "    \n",
    "    super().__init__()\n",
    "    self.scale = scaling_factor\n",
    "    self.num_buckets = n_buckets\n",
    "    self.max_distance = max_distance\n",
    "    self.relative_attention_embedding = torch.nn.Embedding(n_buckets, n_head)\n",
    "\n",
    "  def relative_position_bucket(self, relative_position_matrix):\n",
    "    inv_rel_pos = -relative_position_matrix\n",
    "    masked_rel_pos = torch.max(inv_rel_pos, torch.zeros_like(inv_rel_pos))\n",
    "\n",
    "    max_exact = self.num_buckets // 2\n",
    "\n",
    "    is_small = masked_rel_pos < max_exact\n",
    "    val_if_large = max_exact + (torch.log(masked_rel_pos.float() / max_exact) / math.log(self.max_distance / max_exact) * (self.num_buckets - max_exact)).long()\n",
    "\n",
    "    # Clip the values to the number of buckets - 1\n",
    "    val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, self.num_buckets - 1))\n",
    "\n",
    "    return torch.where(is_small, masked_rel_pos, val_if_large)\n",
    "\n",
    "  def forward(self, block_size):\n",
    "    block_pos = torch.arange(block_size, dtype=torch.long)\n",
    "    context_pos = torch.arange(-block_size, block_size, dtype=torch.long) # XL memory, context is twice block size, and current position starts in the middle.\n",
    "    block_rel_pos = rearrange(block_pos, 'i -> i 1')\n",
    "    context_rel_pos = rearrange(context_pos, 'j -> 1 j')\n",
    "    rel_pos = context_rel_pos - block_rel_pos\n",
    "\n",
    "    position_bucket_indices = self.relative_position_bucket(rel_pos)\n",
    "\n",
    "    rp_values = self.relative_attention_embedding(position_bucket_indices)\n",
    "\n",
    "    rp_values = rearrange(rp_values, 'i j h -> () h i j')\n",
    "    return rp_values * self.scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.6735,  0.8919,  0.8919,  1.2615,  1.2615, -0.3989, -0.7912,\n",
       "           -0.7912, -0.7912, -0.7912, -0.7912, -0.7912],\n",
       "          [-0.6735, -0.6735,  0.8919,  0.8919,  1.2615,  1.2615, -0.3989,\n",
       "           -0.7912, -0.7912, -0.7912, -0.7912, -0.7912],\n",
       "          [-0.6735, -0.6735, -0.6735,  0.8919,  0.8919,  1.2615,  1.2615,\n",
       "           -0.3989, -0.7912, -0.7912, -0.7912, -0.7912],\n",
       "          [-0.6735, -0.6735, -0.6735, -0.6735,  0.8919,  0.8919,  1.2615,\n",
       "            1.2615, -0.3989, -0.7912, -0.7912, -0.7912],\n",
       "          [-0.6735, -0.6735, -0.6735, -0.6735, -0.6735,  0.8919,  0.8919,\n",
       "            1.2615,  1.2615, -0.3989, -0.7912, -0.7912],\n",
       "          [-0.6735, -0.6735, -0.6735, -0.6735, -0.6735, -0.6735,  0.8919,\n",
       "            0.8919,  1.2615,  1.2615, -0.3989, -0.7912]],\n",
       "\n",
       "         [[-1.0920,  0.4090,  0.4090, -1.0790, -1.0790,  0.7588, -0.9241,\n",
       "           -0.9241, -0.9241, -0.9241, -0.9241, -0.9241],\n",
       "          [-1.0920, -1.0920,  0.4090,  0.4090, -1.0790, -1.0790,  0.7588,\n",
       "           -0.9241, -0.9241, -0.9241, -0.9241, -0.9241],\n",
       "          [-1.0920, -1.0920, -1.0920,  0.4090,  0.4090, -1.0790, -1.0790,\n",
       "            0.7588, -0.9241, -0.9241, -0.9241, -0.9241],\n",
       "          [-1.0920, -1.0920, -1.0920, -1.0920,  0.4090,  0.4090, -1.0790,\n",
       "           -1.0790,  0.7588, -0.9241, -0.9241, -0.9241],\n",
       "          [-1.0920, -1.0920, -1.0920, -1.0920, -1.0920,  0.4090,  0.4090,\n",
       "           -1.0790, -1.0790,  0.7588, -0.9241, -0.9241],\n",
       "          [-1.0920, -1.0920, -1.0920, -1.0920, -1.0920, -1.0920,  0.4090,\n",
       "            0.4090, -1.0790, -1.0790,  0.7588, -0.9241]]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos = XLRelativePosition(n_buckets = 5, max_distance = 8, n_head = 2)\n",
    "rp_values = rel_pos(6)\n",
    "rp_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_embed, 4 * n_embed), # 4x is a common expansion factor\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(4 * n_embed, n_embed), # Project back to the residual stream\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, attention, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = attention\n",
    "        self.ff = FeedForward(n_embed, dropout)\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(n_embed)\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual connections\n",
    "        attn_out, new_xl_memories = self.attention(self.layer_norm1(x))\n",
    "        x = x + attn_out\n",
    "        x = x + self.ff(self.layer_norm2(x))\n",
    "        return x, new_xl_memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderTransformer_KNN_XL(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_size, \n",
    "            batch_size, # required so we can init a memory per batch dim\n",
    "            n_embed = 384, # /6 heads = 64 per head\n",
    "            n_head = 6, \n",
    "            n_layer = 6, \n",
    "            max_bar_position = 1024,\n",
    "            top_k = 5,\n",
    "            dropout = 0.2,\n",
    "            n_rel_pos_buckets = 32,\n",
    "            rel_pos_max_distance = 128):\n",
    "        \n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        scaling_factor = head_size ** 0.5\n",
    "        self.n_layer = n_layer\n",
    "        self.max_bar_position = max_bar_position\n",
    "        self.token_embedding = torch.nn.Embedding(vocab_size, n_embed)\n",
    "        self.rel_pos = XLRelativePosition(n_buckets = n_rel_pos_buckets, max_distance = rel_pos_max_distance, n_head = n_head, scaling_factor = scaling_factor)\n",
    "        self.rel_pos_knn = XLRelativePosition(n_buckets = n_rel_pos_buckets, max_distance = rel_pos_max_distance, n_head = n_head, scaling_factor = scaling_factor)\n",
    "        self.beat_embedding = torch.nn.Embedding(SAMPLES_PER_BAR, n_embed)\n",
    "        self.bar_embedding = torch.nn.Embedding(max_bar_position, n_embed)\n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList([])\n",
    "        for i in range(n_layer):\n",
    "\n",
    "            if self.isKNNLayer(i):\n",
    "                attention_type = KNN_XLAttention(\n",
    "                            Path('../data/numpy/knn-demo'),\n",
    "                            batch_size,\n",
    "                            top_k,\n",
    "                            n_embed,\n",
    "                            n_head,\n",
    "                            dropout)\n",
    "            else:\n",
    "                attention_type = XLAttention(\n",
    "                            n_embed,\n",
    "                            n_head,\n",
    "                            dropout)\n",
    "\n",
    "            self.blocks.append(Block(n_embed, attention_type, dropout))\n",
    "            \n",
    "        self.layer_norm = torch.nn.LayerNorm(n_embed)\n",
    "        self.lm_head = torch.nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def isKNNLayer(self, i):\n",
    "        return i == self.n_layer - 2\n",
    "\n",
    "    def forward(self, batch_file_idxs, x, xl_memories=None, targets=None):\n",
    "\n",
    "        B, T, C = x.shape()\n",
    "\n",
    "        # Could split these out in one go using the unbind function\n",
    "        token_idx = x[:, :, 0] # (B,T)\n",
    "        time_idx = x[:, :, 1] # (B,T)\n",
    "\n",
    "        sample_idx = time_idx % SAMPLES_PER_BAR # (B,T)\n",
    "        bar_idx = (time_idx // SAMPLES_PER_BAR) % self.max_bar_position # (B,T)\n",
    "\n",
    "        if xl_memories is None:\n",
    "            xl_memories = (None,) * self.n_layer\n",
    "        else:\n",
    "            xl_memories = xl_memories\n",
    "\n",
    "        rel_pos = self.rel_pos(T)\n",
    "        rel_pos_knn = self.rel_pos_knn(T)\n",
    "\n",
    "        token_embed = self.token_embedding(token_idx) # (B,T,Embed)\n",
    "        bar_embed = self.bar_embedding(bar_idx) # (B,T,Embed)\n",
    "        sample_embed = self.beat_embedding(sample_idx) # (B,T,Embed)\n",
    "\n",
    "        x = token_embed + bar_embed + sample_embed\n",
    "\n",
    "        # Store the XL memories for each pass\n",
    "        new_xl_memories = []\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "\n",
    "            if self.isKNNLayer(i):\n",
    "                x, xl_mem = block(batch_file_idxs, rel_pos_knn, x, xl_memories[i])\n",
    "            else:\n",
    "                x, xl_mem = block(rel_pos, x, xl_memories[i])\n",
    "\n",
    "            new_xl_memories.append(xl_mem.detach())\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        # TODO: Convert this section to use einops rearrange\n",
    "        if targets is None:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "        else:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # Flatten all the batches\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        if len(new_xl_memories) > 0:\n",
    "            return logits, loss, new_xl_memories\n",
    "        else:\n",
    "            return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11347982\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "model = DecoderTransformer_KNN_XL(vocab_size=vocab.size, batch_size=batch_size)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is slightly less than our vanilla model, which was surprising. Looking at the breakdown below though, it seems to be that our relative position params are quite small (`n_bucket * n_head` rather than `T * n_embed`).\n",
    "\n",
    "Also we aren't registering a `tril` buffer which was included in the vanilla param count (256 * 256 * n_head * n_layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "token_embedding.weight \t torch.Size([392, 384])\n",
      "rel_pos.relative_attention_embedding.weight \t torch.Size([32, 6])\n",
      "rel_pos_knn.relative_attention_embedding.weight \t torch.Size([32, 6])\n",
      "beat_embedding.weight \t torch.Size([32, 384])\n",
      "bar_embedding.weight \t torch.Size([1024, 384])\n",
      "blocks.0.attention.key.weight \t torch.Size([384, 384])\n",
      "blocks.0.attention.query.weight \t torch.Size([384, 384])\n",
      "blocks.0.attention.value.weight \t torch.Size([384, 384])\n",
      "blocks.0.attention.project.weight \t torch.Size([384, 384])\n",
      "blocks.0.attention.project.bias \t torch.Size([384])\n",
      "blocks.0.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.0.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.0.ff.net.2.weight \t torch.Size([384, 1536])\n",
      "blocks.0.ff.net.2.bias \t torch.Size([384])\n",
      "blocks.0.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.0.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.0.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.0.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.1.attention.key.weight \t torch.Size([384, 384])\n",
      "blocks.1.attention.query.weight \t torch.Size([384, 384])\n",
      "blocks.1.attention.value.weight \t torch.Size([384, 384])\n",
      "blocks.1.attention.project.weight \t torch.Size([384, 384])\n",
      "blocks.1.attention.project.bias \t torch.Size([384])\n",
      "blocks.1.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.1.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.1.ff.net.2.weight \t torch.Size([384, 1536])\n",
      "blocks.1.ff.net.2.bias \t torch.Size([384])\n",
      "blocks.1.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.1.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.1.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.1.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.2.attention.key.weight \t torch.Size([384, 384])\n",
      "blocks.2.attention.query.weight \t torch.Size([384, 384])\n",
      "blocks.2.attention.value.weight \t torch.Size([384, 384])\n",
      "blocks.2.attention.project.weight \t torch.Size([384, 384])\n",
      "blocks.2.attention.project.bias \t torch.Size([384])\n",
      "blocks.2.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.2.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.2.ff.net.2.weight \t torch.Size([384, 1536])\n",
      "blocks.2.ff.net.2.bias \t torch.Size([384])\n",
      "blocks.2.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.2.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.2.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.2.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.3.attention.key.weight \t torch.Size([384, 384])\n",
      "blocks.3.attention.query.weight \t torch.Size([384, 384])\n",
      "blocks.3.attention.value.weight \t torch.Size([384, 384])\n",
      "blocks.3.attention.project.weight \t torch.Size([384, 384])\n",
      "blocks.3.attention.project.bias \t torch.Size([384])\n",
      "blocks.3.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.3.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.3.ff.net.2.weight \t torch.Size([384, 1536])\n",
      "blocks.3.ff.net.2.bias \t torch.Size([384])\n",
      "blocks.3.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.3.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.3.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.3.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.4.attention.gate_bias \t torch.Size([6, 1, 1])\n",
      "blocks.4.attention.key.weight \t torch.Size([384, 384])\n",
      "blocks.4.attention.query.weight \t torch.Size([384, 384])\n",
      "blocks.4.attention.value.weight \t torch.Size([384, 384])\n",
      "blocks.4.attention.project.weight \t torch.Size([384, 384])\n",
      "blocks.4.attention.project.bias \t torch.Size([384])\n",
      "blocks.4.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.4.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.4.ff.net.2.weight \t torch.Size([384, 1536])\n",
      "blocks.4.ff.net.2.bias \t torch.Size([384])\n",
      "blocks.4.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.4.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.4.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.4.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.5.attention.key.weight \t torch.Size([384, 384])\n",
      "blocks.5.attention.query.weight \t torch.Size([384, 384])\n",
      "blocks.5.attention.value.weight \t torch.Size([384, 384])\n",
      "blocks.5.attention.project.weight \t torch.Size([384, 384])\n",
      "blocks.5.attention.project.bias \t torch.Size([384])\n",
      "blocks.5.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.5.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.5.ff.net.2.weight \t torch.Size([384, 1536])\n",
      "blocks.5.ff.net.2.bias \t torch.Size([384])\n",
      "blocks.5.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.5.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.5.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.5.layer_norm2.bias \t torch.Size([384])\n",
      "layer_norm.weight \t torch.Size([384])\n",
      "layer_norm.bias \t torch.Size([384])\n",
      "lm_head.weight \t torch.Size([392, 384])\n",
      "lm_head.bias \t torch.Size([392])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- Training loop (inc. pause / save and load / continue)\n",
    "\n",
    "Once that is working, consider\n",
    "\n",
    "- Ragged memmap for data loading (will make loading much faster)\n",
    "- Byte pair encoding (bigger vocab with common token pairs gives us a bigger effective context) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
