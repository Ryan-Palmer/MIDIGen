{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'5 - TransformerXL')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from itertools import chain\n",
    "from itertools import groupby\n",
    "from functools import reduce\n",
    "from typing import Collection, List\n",
    "from pathlib import Path\n",
    "import music21 as m21\n",
    "musescore_path = '/usr/bin/mscore'\n",
    "m21.environment.set('musicxmlPath', musescore_path)\n",
    "m21.environment.set('musescoreDirectPNGPath', musescore_path)\n",
    "from midi_encoding import *\n",
    "from einops import rearrange, repeat, pack, unpack, einsum\n",
    "import faiss\n",
    "import time\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4090.\n"
     ]
    }
   ],
   "source": [
    "if device == \"cuda\":\n",
    "    print(f\"Device: {torch.cuda.get_device_name()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 22 17:52:01 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0  On |                  Off |\n",
      "| 30%   31C    P0             50W /  450W |    3042MiB /  24564MiB |     36%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A        29      G   /Xwayland                                   N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = MusicVocab()\n",
    "vocab.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "\n",
    "Our memory-augmented transformer will be very similar to the vanilla model developed in the previous notebook.\n",
    "\n",
    "There will be three major additions, all of which are fairly simple:\n",
    "\n",
    "- Relative positional embeddings.\n",
    "- KNN lookup for keys (and their associated values).\n",
    "- Recurrent 'TransformerXL' style memory.\n",
    "\n",
    "## Einops\n",
    "\n",
    "First, lets condense our previous implementation in two ways\n",
    "\n",
    "- Add a dimension to our `MultiHeadAttention` module, abandoning the separate `SelfAttentionHead` module.\n",
    "- Switch to using einops for shape manipulation as it is both simpler to write and read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_head = 8, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.n_head = n_head\n",
    "        self.head_size = n_embed // n_head\n",
    "        head_total_size = n_head * self.head_size\n",
    "        self.key = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.query = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.proj = torch.nn.Linear(head_total_size, n_embed)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # Split heads\n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        k = rearrange(k, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        v = rearrange(v, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "\n",
    "        # Without einsum we had to swap dims using k.transpose(-2, -1)\n",
    "        w = einsum(q, k, 'b h i d, b h j d -> b h i j') * (self.head_size ** -0.5)\n",
    "        \n",
    "        # TODO: Relative positional encoding\n",
    "\n",
    "        i, j = w.shape[-2:]\n",
    "        mask = torch.tril(torch.ones((i,j), dtype = torch.bool))\n",
    "        w = w.masked_fill(mask, float('-inf'))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        weighted_values = w@v\n",
    "\n",
    "        # Concat heads\n",
    "        weighted_values = rearrange(weighted_values, 'b h t d -> b t (h d)')\n",
    "\n",
    "        # TODO: KNN memory\n",
    "\n",
    "        out = self.proj(weighted_values)\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Memory\n",
    "\n",
    "If we want to add KNN memory, we need an indexed data store to look up and retrieve keys and values.\n",
    "\n",
    "For the index we can use [Faiss](https://github.com/facebookresearch/faiss) from Meta.\n",
    "\n",
    "For the data store we can simply use a memory-mapped numpy array.\n",
    "\n",
    "The following code is adapted from the [Colab Notebook](https://colab.research.google.com/drive/1XZz1sjNt1MKRG6ul_hOGSJFQLS4lRtmJ?usp=sharing#scrollTo=gs7RpvCdePZr) accompanying the [Coding a Paper](https://www.youtube.com/playlist?list=PLam9sigHPGwOe8VDoS_6VT4jjlgs9Uepb) series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN():\n",
    "    def __init__(self, dim, max_memories, db_filepath):\n",
    "        self.dim = dim\n",
    "        self.max_memories = max_memories\n",
    "        self.shape = (max_memories, 2, dim)\n",
    "        self.db_offset = 0\n",
    "        if db_filepath.exists():\n",
    "            dbMode = 'r+'\n",
    "        else:\n",
    "            dbMode = 'w+' # Create file if it doesn't exist\n",
    "        self.db = np.memmap(db_filepath, mode = dbMode, dtype = np.float32, shape = self.shape)\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "    def add_to_db(self, new_data):\n",
    "        new_data_len = new_data.shape[0] # (t)\n",
    "        ids = (np.arange(new_data_len) + self.db_offset)\n",
    "        self.db[ids] = new_data.detach().numpy()\n",
    "        self.db_offset += new_data_len\n",
    "        # Write to file\n",
    "        self.db.flush()\n",
    "\n",
    "    def search_and_retrieve(self, query, k):\n",
    "\n",
    "        # The tooltip says the args are (n, x, k) but that's the CPP api, it's actually (x, k) in Python (n is the first dim of x anyway so can be inferred).\n",
    "        distances, indices = self.index.search(query, k)\n",
    "        \n",
    "        kvs = self.db[indices]\n",
    "        return kvs\n",
    "\n",
    "    def add(self, new_data):\n",
    "        # new_data = (t, 2, c)\n",
    "\n",
    "        # Add to db\n",
    "        self.add_to_db(new_data)\n",
    "\n",
    "        # Only keys are used in knn index\n",
    "        keys, vals = new_data.unbind(dim=-2)\n",
    "\n",
    "        # Add (t, c) tensors to index\n",
    "        keys = keys.detach().numpy()\n",
    "        keys = np.ascontiguousarray(keys)\n",
    "        self.index.add(keys)\n",
    "\n",
    "    def search(self, query, k):\n",
    "\n",
    "        T, C = query.shape\n",
    "        \n",
    "        # If we have enough memories, search and retrieve, otherwise return zeros\n",
    "        if self.index.ntotal >= k:\n",
    "            kvs = self.search_and_retrieve(np.ascontiguousarray(query.detach().numpy()), k)\n",
    "            kvs = torch.tensor(kvs)\n",
    "        else:\n",
    "            kvs = torch.zeros((T, k, 2, C), device=device)\n",
    "\n",
    "        return kvs\n",
    "\n",
    "    def clear(self):\n",
    "        self.index.reset()\n",
    "        self.db[:] = 0\n",
    "        self.db_offset = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2335, 0.3282, 0.2448, 0.4773],\n",
       "        [0.9102, 0.1002, 0.2155, 0.3850]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 4\n",
    "t = 2\n",
    "\n",
    "knn = KNN(c, 100000, Path('../data/numpy/knn-test.db'))\n",
    "\n",
    "for i in range(1000):\n",
    "    vector_data = torch.tensor(np.random.random((t, 2, c)).astype('float32'))\n",
    "    knn.add(vector_data)\n",
    "\n",
    "query_data = torch.tensor(np.random.random((t, c)).astype('float32'))\n",
    "query_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search returns a `(t k 2 c)` tensor which contains the top_k keys and values for each `(t c)` query.\n",
    "\n",
    "Here our query is `(2 * 4)` so our results will be `(2 * 2 * 2 * 4)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2213, 0.3221, 0.2657, 0.5227],\n",
       "          [0.3680, 0.0744, 0.3142, 0.5691]],\n",
       "\n",
       "         [[0.3283, 0.3834, 0.2424, 0.5119],\n",
       "          [0.7836, 0.4819, 0.4896, 0.9313]]],\n",
       "\n",
       "\n",
       "        [[[0.8644, 0.1047, 0.1088, 0.4424],\n",
       "          [0.4714, 0.6815, 0.9910, 0.2402]],\n",
       "\n",
       "         [[0.8612, 0.0825, 0.1059, 0.3038],\n",
       "          [0.6274, 0.8587, 0.2832, 0.2548]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k = 2\n",
    "knn.search(query_data, top_k) # (t k two c) tensor, returns top_k keys and values for each query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can integrate the memory into our multiheaded attention.\n",
    "\n",
    "We will make a new class for this as we only use KNN on the second to last layer.\n",
    "\n",
    "It will have a KNN memory for each batch dimension, and we will clear that memory if the file in that batch dimension changes.\n",
    "\n",
    "We will know this as the `CustomMidiDataset` returns the file indices of each batch along with the data. \n",
    "\n",
    "These can be passed to our model, which in turn can pass them to the KNN attention block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask:\n",
      " tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "Original w:\n",
      " tensor([[0.9309, 0.0458, 0.1893, 0.1659],\n",
      "        [0.0803, 0.6873, 0.5376, 0.6530],\n",
      "        [0.9119, 0.4572, 0.0314, 0.4545],\n",
      "        [0.2545, 0.7634, 0.3870, 0.7951]])\n",
      "Masked w:\n",
      " tensor([[0.1389,   -inf,   -inf,   -inf],\n",
      "        [0.1542, 0.8455,   -inf,   -inf],\n",
      "        [0.9759, 0.5102, 0.5729,   -inf],\n",
      "        [0.1436, 0.0020, 0.8549, 0.0032]])\n"
     ]
    }
   ],
   "source": [
    "# Define i and j\n",
    "i, j = 4, 4\n",
    "\n",
    "# Create the mask\n",
    "mask = torch.logical_not(torch.tril(torch.ones((i, j), dtype=torch.bool)))\n",
    "\n",
    "# Example tensor w\n",
    "w = torch.rand((i, j))\n",
    "\n",
    "# Apply the mask\n",
    "w = w.masked_fill(mask, float('-inf'))\n",
    "\n",
    "print(\"Mask:\\n\", mask)\n",
    "print(\"Original w:\\n\", torch.rand((i, j)))  # Example tensor before masking\n",
    "print(\"Masked w:\\n\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dbFilePath, batch_size, n_embed, k, n_head = 8, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.k = k\n",
    "        self.n_head = n_head\n",
    "        self.head_size = n_embed // n_head\n",
    "        head_total_size = n_head * self.head_size\n",
    "        self.key = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.query = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.project = torch.nn.Linear(head_total_size, n_embed)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Memory per batch dim\n",
    "        self.knn = {i: KNN(n_embed, 100000, f'{dbFilePath}-batch_dim-{i}.db') for i in range(batch_size)} # KNN memory will get or create the files, so we just need to be consistent with the file names.\n",
    "        self.current_file_indexes = None\n",
    "\n",
    "        self.gate_bias = torch.nn.Parameter(torch.randn(self.n_head, 1, 1))\n",
    "    \n",
    "\n",
    "    def forward(self, x, batch_file_indexes):\n",
    "\n",
    "        # Clear batch dim's knn memory if file changes\n",
    "        if self.current_file_indexes != None:\n",
    "            for i in range(len(self.current_file_indexes)):\n",
    "                if self.current_file_indexes[i] != batch_file_indexes[i]:\n",
    "                    self.knn[i].clear()\n",
    "\n",
    "        self.current_file_indexes = batch_file_indexes\n",
    "\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # This helps to mitigate drift in the embeddings which can cause the historical keys to become less aligned to the current queries.\n",
    "        q = F.normalize(q, dim=-1)\n",
    "        k = F.normalize(k, dim=-1)\n",
    "\n",
    "        ### LOCAL ATTENTION\n",
    "\n",
    "        # Split heads\n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        k = rearrange(k, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        v = rearrange(v, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "\n",
    "        # Without einsum we had to swap dims using k.transpose(-2, -1)\n",
    "        w = einsum(q, k, 'b h i d, b h j d -> b h i j') * (self.head_size ** -0.5)\n",
    "        \n",
    "        # TODO: Relative positional encoding\n",
    "\n",
    "        i, j = w.shape[-2:]\n",
    "        mask = torch.logical_not(torch.tril(torch.ones((i,j), dtype = torch.bool))) # Can't cache this as its shape depends on whether we have XL memory or not.\n",
    "        w = w.masked_fill(mask, float('-inf'))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        weighted_values = w@v # b h t d\n",
    "\n",
    "        ### KNN ATTENTION\n",
    "        knn_mask = torch.tensor([self.knn[i].index.ntotal > 0 for i in range(B)], dtype=torch.bool, device=device)\n",
    "\n",
    "        # Only do knn if there are at least some memories\n",
    "        if knn_mask.any():\n",
    "\n",
    "            t1 = time.time()\n",
    "            print (\"Begin KNN operations\")\n",
    "\n",
    "            # Convert queries to search form\n",
    "            q = rearrange(q, 'b h t d -> b t (h d)')\n",
    "\n",
    "            # KNN returns zeroes if it doesn't have data.\n",
    "            mem_kv = torch.stack([self.knn[i].search(q[i], k = self.k) for i in range(B)], dim = 0) # b, t, k, 2, c\n",
    "            \n",
    "            mem_k, mem_v = mem_kv.unbind(dim = -2)\n",
    "            mem_k = rearrange(mem_k, 'b t k (h d) -> b h t k d', h=self.n_head)\n",
    "            mem_v = rearrange(mem_v, 'b t k (h d) -> b h t k d', h=self.n_head)\n",
    "\n",
    "            # Convert queries to attention form\n",
    "            q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "\n",
    "            # Sum over d for each combination of batch, head, time and top k to get qk affinities, and hence weights for each k. resulting in a tensor of shape (b, h, t, k).\n",
    "            mem_w = einsum('b h t d, b h t k d -> b h t k', q, mem_k)\n",
    "            mem_w = mem_w * (self.head_size ** -0.5)\n",
    "            mem_w = F.softmax(mem_w, dim=-1)\n",
    "\n",
    "            # Weighted sum over the top k dimension for each combination of b, h, and t, resulting in a tensor of shape (b, h, t, d). Equivalent to doing w@v for each k and summing.\n",
    "            mem_weighted_values = einsum('b h t k, b h t k d -> b h t d', mem_w, mem_v)\n",
    "\n",
    "            ## Combined attention\n",
    "            \n",
    "            # Assume every memory has content. Empty memories will be masked out below.\n",
    "            combined_weighted_values = mem_weighted_values * self.gate_bias + weighted_values * (1 - self.gate_bias)\n",
    "\n",
    "            # Mask out combined weighted values where knn memory *is* empty and non-combined values where it *is not* empty, then merge them.\n",
    "            combined_weighted_values = combined_weighted_values * knn_mask.view(B, 1, 1, 1) + weighted_values * (~knn_mask).view(B, 1, 1, 1)\n",
    "\n",
    "            # Concat heads\n",
    "            combined_weighted_values = rearrange(combined_weighted_values, 'b h t d -> b t (h d)')\n",
    "            out = self.project(combined_weighted_values)\n",
    "\n",
    "            t2 = time.time()\n",
    "            print (\"End KNN operations, time taken:\", t2-t1)\n",
    "\n",
    "        else:\n",
    "            # Concat heads\n",
    "            weighted_values = rearrange(weighted_values, 'b h t d -> b t (h d)')\n",
    "            out = self.project(weighted_values)\n",
    "\n",
    "        current_kv = torch.stack((k, v), dim=-2) # (b, t, 2, c)\n",
    "        for i in range(B):\n",
    "            self.knn[i].add(current_kv[i])\n",
    "\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XL Memory\n",
    "\n",
    "We simply need to append the previous iteration's key and value tensors to the current, allowing the queries to search over / swap information from the previous timesteps (which did the same to the timestep before that etc etc), creating a kind of delay-line memory that fades out over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_head = 8, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.n_head = n_head\n",
    "        self.head_size = n_embed // n_head\n",
    "        head_total_size = n_head * self.head_size\n",
    "        self.key = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.query = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.project = torch.nn.Linear(head_total_size, n_embed)\n",
    "        self.dropout = torch.nn.Dropout(dropout)    \n",
    "\n",
    "    def forward(self, x, xl_memory = None):\n",
    "\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # Chris's implementation does `queries = queries * (self.head_size ** -0.5)` here but I don't think it is correct.\n",
    "\n",
    "        if xl_memory is not None:\n",
    "            k_xl, v_xl = xl_memory.unbind(dim = -2) # assume stacked\n",
    "            keys = torch.cat((k_xl, keys), dim = -2) # prepend XL memory\n",
    "            values = torch.cat((v_xl, values), dim = -2) # prepend XL memory\n",
    "\n",
    "        ### LOCAL ATTENTION\n",
    "\n",
    "        # Split heads\n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        k = rearrange(k, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        v = rearrange(v, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "\n",
    "        w = einsum(q, k, 'b h i d, b h j d -> b h i j') * (self.head_size ** -0.5)\n",
    "        \n",
    "        # TODO: Relative positional encoding\n",
    "\n",
    "        i, j = w.shape[-2:]\n",
    "        mask = torch.logical_not(torch.tril(torch.ones((i,j), dtype = torch.bool))) # Can't cache this as its shape depends on whether we have XL memory or not.\n",
    "        w = w.masked_fill(mask, float('-inf'))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        weighted_values = w@v # b h t d\n",
    "        # Concat heads\n",
    "        weighted_values = rearrange(weighted_values, 'b h t d -> b t (h d)')\n",
    "        \n",
    "        out = self.project(weighted_values)\n",
    "\n",
    "        # new XL memories\n",
    "\n",
    "        # Concatenate key and value heads\n",
    "        k = rearrange(k, 'b h t d -> b t (h d)', h = self.heads)\n",
    "        v = rearrange(v, 'b h t d -> b t (h d)', h=self.heads)\n",
    "        kv_memories = torch.stack((k, v), dim=-2) # b t 2 (h d)\n",
    "\n",
    "        if xl_memory is None:\n",
    "            new_xl_memory = kv_memories\n",
    "        else:\n",
    "            new_xl_memory = kv_memories[:, -T:]\n",
    "\n",
    "        return self.dropout(out), new_xl_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- Relative Positional embeddings\n",
    "- Wire it all together\n",
    "- Training loop (inc. pause / save and load / continue)\n",
    "\n",
    "Once all that is working, consider\n",
    "\n",
    "- Ragged memmap for data loading (will make loading much faster)\n",
    "- Byte pair encoding (bigger vocab with common token pairs gives us a bigger effective context) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
