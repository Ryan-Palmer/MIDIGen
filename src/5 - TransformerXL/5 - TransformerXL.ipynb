{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerXL\n",
    "\n",
    "Our vanilla transformer showed improvements, but suffered from only having attention within the current block.\n",
    "\n",
    "We also only used absolute positional encodings, so tokens knew where they were in the sequence but not where they were relative to other tokens.\n",
    "\n",
    "[TransformerXL](https://research.google/blog/transformer-xl-unleashing-the-potential-of-attention-models/) tackles both of these problems by \n",
    "1. Using a 'memory' for keys and values from the previous block, allowing information to propagate through time.\n",
    "2. Employing relative positional encoding.\n",
    "\n",
    "Obviously for either of these to work, data needs to be fed in sequentially, so our loading and batching strategy will once again need revisiting.\n",
    "\n",
    "# Coding A Paper\n",
    "\n",
    "Luckily I found [this walkthrough](https://www.youtube.com/playlist?list=PLam9sigHPGwOe8VDoS_6VT4jjlgs9Uepb) in the style of Karpathy's makemore videos.\n",
    "\n",
    "## Notes\n",
    "\n",
    "### Ep.2 Keeping GPUs Busy\n",
    "\n",
    "We need to keep our music blocks contiguous across batches, e.g. for a batch size of four:\n",
    "\n",
    "\n",
    "          |        Chunk 1        |------|        Chunk 2        |\n",
    "|        | Batch 1 | Batch 2 | Batch 3 |        | Batch 4 | Batch 5 | Batch 6 |\n",
    "|--------|---------|---------|---------|--------|---------|---------|---------|\n",
    "| Song 1 | Block 1 | Block 2 | Block 3 | Song 5 | Block 1 | Block 2 | Block 3 |\n",
    "| Song 2 | Block 1 | Block 2 | Block 3 | Song 6 | Block 1 | Block 2 | Block 3 |\n",
    "| Song 3 | Block 1 | Block 2 | Block 3 | Song 7 | Block 1 | Block 2 | Block 3 |\n",
    "| Song 4 | Block 1 | Block 2 | Block 3 | Song 8 | Block 1 | Block 2 | Block 3 |\n",
    "\n",
    "Note that the above shows songs that are all the same length, which of course isn't what we have in reality.\n",
    "\n",
    "This means that we either\n",
    "- Crop long songs\n",
    "- Pad short songs\n",
    "- Connect them in a ragged way\n",
    "\n",
    "The video takes the cropping approach, picking a given 'chunk' (i.e. multiple of block) size and cropping the song to a multiple of this chunk size, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks = 3\n",
    "block_size = 256\n",
    "chunk_size = blocks * block_size\n",
    "chunk_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So mod the song length by chunk size and crop.\n",
    "\n",
    "Use `reshape` (or `view`?) to rearrange the a song into chunks, then `concat` to join the songs into one list of chunks, then `chunk` to split into batches.\n",
    "\n",
    "Following the above, batch 1 block 1 should be the precursor to batch 2 block 1.\n",
    "\n",
    "Data and labels per chunk are the same as in a vanilla transformer - labels are data offset by one.\n",
    "\n",
    "# To consider\n",
    "\n",
    "- Bigger data set\n",
    "    - Cleaning / preparing files\n",
    "    - Wrap in PyTorch data helper classes (help with batching?)\n",
    "    - Streaming?\n",
    "    - If we are moving to pop music with Lakh MIDI dataset, how will we handle instruments and percussion?\n",
    "\n",
    "- Encoding strategy\n",
    "    - Could be on the fly if quicker than GPU takes to process a batch, otherwise pre-encode\n",
    "    - Follow song-per-batch-layer as outlined above\n",
    "\n",
    "- Use einops rather than manually applying transformation functions where practical\n",
    "\n",
    "- Relative positional encodings\n",
    "\n",
    "- KNN memory\n",
    "    - Vector index (fais?)\n",
    "    - Memory mapped file for db\n",
    "    - Second to last block only\n",
    "    - Look up K nearest keys / values\n",
    "\n",
    "- TransformerXL recurrent memory\n",
    "\n",
    "- Vectorise head operations\n",
    "\n",
    "- Monitoring during training\n",
    "    - Tensorboard?\n",
    "\n",
    "- 'Reverse teacher forcing' (offset future mask extra step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lakh MIDI Dataset\n",
    "\n",
    "This is a huge (~6GB) set of MIDI files of pretty much every kind of music scraped from across the internet.\n",
    "\n",
    "See [the website](https://colinraffel.com/projects/lmd/#get) for more details.\n",
    "\n",
    "## Data Quality\n",
    "\n",
    "If we move to processing such a large dataset, we are going to need to pay more attention to quality.\n",
    "\n",
    "That is, there will likely be corrupt files, but also files with long gaps of silence etc. that could throw off the training.\n",
    "\n",
    "## Instrument Info\n",
    "\n",
    "We are going to get a lot of different instruments, and setting them all as piano will lose a huge amount of information.\n",
    "\n",
    "There are 128 instruments, identified by their program number.\n",
    "\n",
    "Percussion in particular really needs to be mapped correctly. \n",
    "\n",
    "Rather than have a GM instrument per drum, percussion is mapped across 47 notes (35 -> 81) on Channel 10.\n",
    "\n",
    "> NOTE - GM level 2 expanded the range of percussion, amongst other things. It goes from 27 -> 87. Perhaps it is better to have a 128 dim embedding and be done with it?\n",
    "\n",
    "It would also be good to have pitch (and mod?) control info incorporated, as this is used a lot, however it is very high resolution both in terms of range and sample rate if you want to get smooth (i.e. not stepped) pitch bends.\n",
    "\n",
    "### Encoding\n",
    "\n",
    "Ignoring the pitch / CC stuff, we have 128 instruments that can each play 128 notes, plus 47 instruments that can play 1 note.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16431"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruments = 128\n",
    "pitches = 128\n",
    "perscussion = 47\n",
    "(instruments * pitches) + perscussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokens\n",
    "\n",
    "We could swap our `n{i}` for `{instrument}{i}` tokens, but that would result in 16431 note tokens (as opposed to the 128 we currently have).\n",
    "\n",
    "Some of these would also be very rarely used.\n",
    "\n",
    "#### Embeddings\n",
    "\n",
    "We could use an embedding lookup to add instrument information to each token, the same as we do for bar / beat, packaging the instrument info alongside the note and timestep.\n",
    "\n",
    "The trouble with this is we will face the same challenge that came with bars and beats, which is reconstructing them at the output.\n",
    "\n",
    "You could have a second output layer with `instruments + percussion` (175) neurons representing the most likely instrument for that note and softmax over it?\n",
    "\n",
    "Unlike bar and beat, we don't rely on it being perfect in order to render the performance.\n",
    "\n",
    "We might expect the residual stream to have the ability to pass the embedding information straight through the network to the output layer.\n",
    "\n",
    "> Copilot suggest we sum the losses from the two output heads\n",
    "\n",
    "I think this is the most reasonable way to proceed.\n",
    "\n",
    "- Encode instrument info alongside note, duration and timestep\n",
    "- Use this to create a second set of labels\n",
    "- Embed the instrument info at the inout layer\n",
    "- Pass output of the transformer through two linear layers for classification, one of vocab dims and one of 175 for the instruments.\n",
    "- Score the outputs against the respective labels\n",
    "- Sum the loss \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
