{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'5 - TransformerXL')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from itertools import chain\n",
    "from itertools import groupby\n",
    "from functools import reduce\n",
    "from typing import Collection, List\n",
    "from pathlib import Path\n",
    "import music21 as m21\n",
    "musescore_path = '/usr/bin/mscore'\n",
    "m21.environment.set('musicxmlPath', musescore_path)\n",
    "m21.environment.set('musescoreDirectPNGPath', musescore_path)\n",
    "# from midi_encoding import *\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerXL\n",
    "\n",
    "Our vanilla transformer showed improvements, but suffered from only having attention within the current block.\n",
    "\n",
    "We also only used absolute positional encodings, so tokens knew where they were in the sequence but not where they were relative to other tokens.\n",
    "\n",
    "[TransformerXL](https://research.google/blog/transformer-xl-unleashing-the-potential-of-attention-models/) tackles both of these problems by \n",
    "1. Using a 'memory' for keys and values from the previous block, allowing information to propagate through time.\n",
    "2. Employing relative positional encoding.\n",
    "\n",
    "Obviously for either of these to work, data needs to be fed in sequentially, so our loading and batching strategy will once again need revisiting.\n",
    "\n",
    "# Coding A Paper\n",
    "\n",
    "Luckily I found [this walkthrough](https://www.youtube.com/playlist?list=PLam9sigHPGwOe8VDoS_6VT4jjlgs9Uepb) in the style of Karpathy's makemore videos.\n",
    "\n",
    "## Notes\n",
    "\n",
    "### Ep.2 Keeping GPUs Busy\n",
    "\n",
    "We need to keep our music blocks contiguous across batches, e.g. for a batch size of four:\n",
    "\n",
    "\n",
    "          |        Chunk 1        |------|        Chunk 2        |\n",
    "|        | Batch 1 | Batch 2 | Batch 3 |        | Batch 4 | Batch 5 | Batch 6 |\n",
    "|--------|---------|---------|---------|--------|---------|---------|---------|\n",
    "| Song 1 | Block 1 | Block 2 | Block 3 | Song 5 | Block 1 | Block 2 | Block 3 |\n",
    "| Song 2 | Block 1 | Block 2 | Block 3 | Song 6 | Block 1 | Block 2 | Block 3 |\n",
    "| Song 3 | Block 1 | Block 2 | Block 3 | Song 7 | Block 1 | Block 2 | Block 3 |\n",
    "| Song 4 | Block 1 | Block 2 | Block 3 | Song 8 | Block 1 | Block 2 | Block 3 |\n",
    "\n",
    "Note that the above shows songs that are all the same length, which of course isn't what we have in reality.\n",
    "\n",
    "This means that we either\n",
    "- Crop long songs\n",
    "- Pad short songs\n",
    "- Connect them in a ragged way\n",
    "\n",
    "The video takes the cropping approach, picking a given 'chunk' (i.e. multiple of block) size and cropping the song to a multiple of this chunk size, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks = 3\n",
    "block_size = 256\n",
    "chunk_size = blocks * block_size\n",
    "chunk_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So mod the song length by chunk size and crop.\n",
    "\n",
    "Use `reshape` (or `view`?) to rearrange the a song into chunks, then `concat` to join the songs into one list of chunks, then `chunk` to split into batches.\n",
    "\n",
    "Following the above, batch 1 block 1 should be the precursor to batch 2 block 1.\n",
    "\n",
    "Data and labels per chunk are the same as in a vanilla transformer - labels are data offset by one.\n",
    "\n",
    "# To consider\n",
    "\n",
    "- Bigger data set\n",
    "    - Cleaning / preparing files\n",
    "    - Wrap in PyTorch data helper classes (help with batching?)\n",
    "    - Streaming?\n",
    "    - Parallel loading / processing on CPU with [pebble](https://pypi.org/project/Pebble/)\n",
    "    - If we are moving to pop music with Lakh MIDI dataset, how will we handle instruments and percussion?\n",
    "\n",
    "- Encoding strategy\n",
    "    - Could be on the fly if quicker than GPU takes to process a batch, otherwise pre-encode\n",
    "    - Follow song-per-batch-layer as outlined above\n",
    "\n",
    "- Use einops rather than manually applying transformation functions where practical\n",
    "\n",
    "- Relative positional encodings\n",
    "\n",
    "- KNN memory\n",
    "    - Vector index (fais?)\n",
    "    - Memory mapped file for db\n",
    "    - Second to last block only\n",
    "    - Look up K nearest keys / values\n",
    "\n",
    "- TransformerXL recurrent memory\n",
    "\n",
    "- Vectorise head operations\n",
    "\n",
    "- Monitoring during training\n",
    "    - Tensorboard?\n",
    "\n",
    "- 'Reverse teacher forcing' (offset future mask extra step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lakh MIDI Dataset\n",
    "\n",
    "This is a huge (~6GB) set of MIDI files of pretty much every kind of music scraped from across the internet.\n",
    "\n",
    "See [the website](https://colinraffel.com/projects/lmd/#get) for more details.\n",
    "\n",
    "## Data Quality\n",
    "\n",
    "If we move to processing such a large dataset, we are going to need to pay more attention to quality.\n",
    "\n",
    "That is, there will likely be corrupt files, but also files with long gaps of silence etc. that could throw off the training.\n",
    "\n",
    "## Instrument Info\n",
    "\n",
    "We are going to get a lot of different instruments, and setting them all as piano will lose a huge amount of information.\n",
    "\n",
    "There are 128 instruments, identified by their program number.\n",
    "\n",
    "Percussion in particular really needs to be mapped correctly. \n",
    "\n",
    "Rather than have a GM instrument per drum, percussion is mapped across 47 notes (35 -> 81) on Channel 10.\n",
    "\n",
    "> NOTE - GM level 2 expanded the range of percussion, amongst other things. It goes from 27 -> 87. Perhaps it is better to have a 128 dim embedding and be done with it?\n",
    "\n",
    "It would also be good to have pitch (and mod?) control info incorporated, as this is used a lot, however it is very high resolution both in terms of range and sample rate if you want to get smooth (i.e. not stepped) pitch bends.\n",
    "\n",
    "### Encoding\n",
    "\n",
    "Ignoring the pitch / CC stuff, we have 128 instruments that can each play 128 notes, plus 47 instruments that can play 1 note.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16431"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruments = 128\n",
    "pitches = 128\n",
    "perscussion = 47\n",
    "(instruments * pitches) + perscussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokens\n",
    "\n",
    "We could swap our `n{i}` for `{instrument}{i}` tokens, but that would result in 16431 note tokens (as opposed to the 128 we currently have).\n",
    "\n",
    "Some of these would also be very rarely used.\n",
    "\n",
    "#### Embeddings\n",
    "\n",
    "We could use an embedding lookup to add instrument information to each token, the same as we do for bar / beat, packaging the instrument info alongside the note and timestep.\n",
    "\n",
    "The trouble with this is we will face the same challenge that came with bars and beats, which is reconstructing them at the output.\n",
    "\n",
    "You could have a second output layer with `instruments + percussion` (175) neurons representing the most likely instrument for that note and softmax over it?\n",
    "\n",
    "Unlike bar and beat, we don't rely on it being perfect in order to render the performance.\n",
    "\n",
    "We might expect the residual stream to have the ability to pass the embedding information straight through the network to the output layer.\n",
    "\n",
    "> Copilot suggest we sum the losses from the two output heads\n",
    "\n",
    "I think this is the most reasonable way to proceed.\n",
    "\n",
    "- Encode instrument info alongside note, duration and timestep\n",
    "- Use this to create a second set of labels\n",
    "- Embed the instrument info at the inout layer\n",
    "- Pass output of the transformer through two linear layers for classification, one of vocab dims and one of 175 for the instruments.\n",
    "- Score the outputs against the respective labels\n",
    "- Sum the loss\n",
    "\n",
    "## Plan\n",
    "\n",
    "All of the above considered, it is probably best to first update the architecture, then update the dataset / encoding afterwards otherwise it will be a lot at once and any bugs will be hard to track.\n",
    "\n",
    "\n",
    "### Loading data\n",
    "\n",
    "PyTorch has a built-in helper class for loading data in a custom way, called [Batch Sampler](https://medium.com/@haleema.ramzan/how-to-build-a-custom-batch-sampler-in-pytorch-ce04161583ee).\n",
    "\n",
    "Its job is to generate the index of the next item from the data set. This index is then fed to another customised PyTorch class, [DataSet](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html), which uses them to return the appropiate sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, file_paths, sample_length):\n",
    "        self.file_paths = file_paths\n",
    "        self.data = []\n",
    "        self.file_lengths = []\n",
    "        self.total_samples = 0\n",
    "        self.sample_length = sample_length\n",
    "\n",
    "    def load_samples(self):\n",
    "        for file_path in self.file_paths:\n",
    "            with open(file_path, 'r') as file:\n",
    "                samples = file.read().split('\\n')  # Assuming each line is a sample\n",
    "                samples = [[int(char) for char in sample] for sample in samples if len(sample) == self.sample_length] # Filter out samples that are not the correct length\n",
    "                if len(samples) == 0: # Skip files with no valid samples\n",
    "                    continue\n",
    "                self.data.append(samples)\n",
    "                self.file_lengths.append(len(samples))\n",
    "        self.total_samples = sum(self.file_lengths)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_idx = idx[0]\n",
    "        sample_idx = idx[1]\n",
    "        sample = self.data[file_idx][sample_idx]\n",
    "        return torch.tensor(sample)\n",
    "\n",
    "class CustomBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.current_files = list(range(batch_size))\n",
    "        self.batch_indices = []\n",
    "    \n",
    "    def precompute_indices(self):\n",
    "        file_indices = [0] * len(self.dataset.file_paths)\n",
    "        while sum(file_indices) < self.dataset.total_samples:\n",
    "            batch = []\n",
    "            for i in range(self.batch_size):\n",
    "                current_file = self.current_files[i]\n",
    "\n",
    "                # Check if the current file is exhausted\n",
    "                if file_indices[current_file] == self.dataset.file_lengths[current_file]:\n",
    "                    # Find the next file that hasn't been started\n",
    "                    found_new_file = False\n",
    "                    for next_file in range(len(self.dataset.file_lengths)):\n",
    "                        if file_indices[next_file] == 0:\n",
    "                            current_file = next_file\n",
    "                            self.current_files[i] = current_file\n",
    "                            found_new_file = True\n",
    "                            break\n",
    "                    \n",
    "                    if not found_new_file:\n",
    "                        # No more unstarted files, break the loop\n",
    "                        return\n",
    "\n",
    "                indices = [current_file, file_indices[current_file]]\n",
    "                batch.append(indices)\n",
    "                \n",
    "                file_indices[current_file] += 1\n",
    "\n",
    "            self.batch_indices.append(torch.tensor(batch))\n",
    "        return\n",
    "\n",
    "    def __iter__(self):\n",
    "        for index in self.batch_indices:\n",
    "            yield index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this with some dummy data.  How about\n",
    "\n",
    "- Batch size 4\n",
    "- 6 batches\n",
    "\n",
    "- Dim 1 - 1 file * 6 samples\n",
    "- Dim 2 - 2 files * 3 samples\n",
    "- Dim 3 - 2 files, 1 * 2 samples, 1 * 4 samples\n",
    "- Dim 4 - 3 files * 2 samples\n",
    "\n",
    "We would expect these file indices in the respective batch dims:\n",
    "\n",
    "- 1\n",
    "- 2, 7\n",
    "- 3, 5\n",
    "- 4, 6, 8\n",
    "\n",
    "If we add up to `batch_size-1` extra files, they shouldn't be used as we should `return` if we haven't got enough to fill a whole batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6, 1, 1],\n",
      "        [3, 1, 1],\n",
      "        [2, 1, 1],\n",
      "        [2, 2, 1]])\n",
      "tensor([[6, 1, 2],\n",
      "        [3, 1, 2],\n",
      "        [2, 1, 2],\n",
      "        [2, 2, 2]])\n",
      "tensor([[6, 1, 3],\n",
      "        [3, 1, 3],\n",
      "        [4, 1, 1],\n",
      "        [2, 3, 1]])\n",
      "tensor([[6, 1, 4],\n",
      "        [3, 2, 1],\n",
      "        [4, 1, 2],\n",
      "        [2, 3, 2]])\n",
      "tensor([[6, 1, 5],\n",
      "        [3, 2, 2],\n",
      "        [4, 1, 3],\n",
      "        [2, 4, 1]])\n",
      "tensor([[6, 1, 6],\n",
      "        [3, 2, 3],\n",
      "        [4, 1, 4],\n",
      "        [2, 4, 2]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "file_names = [      # File index\n",
    "    '6.txt',        # 1\n",
    "    '3.1.txt',      # 2\n",
    "    '2.1.txt',      # 3\n",
    "    '2.2.txt',      # 4\n",
    "    '4.txt',        # 5\n",
    "    '2.3.txt',      # 6\n",
    "    '3.2.txt',      # 7\n",
    "    '2.4.txt',      # 8\n",
    "    'extra.1.txt',  # 9\n",
    "    'extra.2.txt',  # 10\n",
    "    'extra.3.txt',  # 11\n",
    "    'empty.txt',    # 12\n",
    "    'bad.txt',      # 13\n",
    "]\n",
    "\n",
    "file_paths = list(map(lambda filename: Path(f'../data/text/{filename}'), file_names))\n",
    "\n",
    "dataset = CustomTextDataset(file_paths, 3)\n",
    "dataset.load_samples()\n",
    "\n",
    "sampler = CustomBatchSampler(dataset, batch_size)\n",
    "sampler.precompute_indices()\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_sampler=sampler)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler.__len__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
