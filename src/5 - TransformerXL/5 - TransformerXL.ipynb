{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerXL\n",
    "\n",
    "Our vanilla transformer showed improvements, but suffered from only having attention within the current block.\n",
    "\n",
    "We also only used absolute positional encodings, so tokens knew where they were in the sequence but not where they were relative to other tokens.\n",
    "\n",
    "[TransformerXL](https://research.google/blog/transformer-xl-unleashing-the-potential-of-attention-models/) tackles both of these problems by \n",
    "1. Using a 'memory' for keys and values from the previous block, allowing information to propagate through time.\n",
    "2. Employing relative positional encoding.\n",
    "\n",
    "Obviously for either of these to work, data needs to be fed in sequentially, so our loading and batching strategy will once again need revisiting.\n",
    "\n",
    "# Coding A Paper\n",
    "\n",
    "Luckily I found [this walkthrough](https://www.youtube.com/playlist?list=PLam9sigHPGwOe8VDoS_6VT4jjlgs9Uepb) in the style of Karpathy's makemore videos.\n",
    "\n",
    "## Notes\n",
    "\n",
    "### Ep.2 Keeping GPUs Busy\n",
    "\n",
    "We need to keep our music blocks contiguous across batches, e.g. for a batch size of four:\n",
    "\n",
    "|        | Batch 1 | Batch 2 | Batch 3 |        | Batch 4 | Batch 5 | Batch 6 |\n",
    "|--------|---------|---------|---------|--------|---------|---------|---------|\n",
    "| Song 1 | Block 1 | Block 2 | Block 3 | Song 5 | Block 1 | Block 2 | Block 3 |\n",
    "| Song 2 | Block 1 | Block 2 | Block 3 | Song 6 | Block 1 | Block 2 | Block 3 |\n",
    "| Song 3 | Block 1 | Block 2 | Block 3 | Song 7 | Block 1 | Block 2 | Block 3 |\n",
    "| Song 4 | Block 1 | Block 2 | Block 3 | Song 8 | Block 1 | Block 2 | Block 3 |\n",
    "\n",
    "Note that the above shows songs that are all the same length, which of course isn't what we have in reality.\n",
    "\n",
    "This means that we either crop all the songs to the length of the shortest, or we connect them in a ragged way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
