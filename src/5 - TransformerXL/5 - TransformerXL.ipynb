{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerXL\n",
    "\n",
    "Our vanilla transformer showed improvements, but suffered from only having attention within the current block.\n",
    "\n",
    "We also only used absolute positional encodings, so tokens knew where they were in the sequence but not where they were relative to other tokens.\n",
    "\n",
    "[TransformerXL](https://research.google/blog/transformer-xl-unleashing-the-potential-of-attention-models/) tackles both of these problems by \n",
    "1. Using a 'memory' for keys and values from the previous block, allowing information to propagate through time.\n",
    "2. Employing relative positional encoding.\n",
    "\n",
    "Obviously for either of these to work, data needs to be fed in sequentially, so our loading and batching strategy will once again need revisiting.\n",
    "\n",
    "# Coding A Paper\n",
    "\n",
    "Luckily I found [this walkthrough](https://www.youtube.com/playlist?list=PLam9sigHPGwOe8VDoS_6VT4jjlgs9Uepb) in the style of Karpathy's makemore videos.\n",
    "\n",
    "## Notes\n",
    "\n",
    "### Ep.2 Keeping GPUs Busy\n",
    "\n",
    "We need to keep our music blocks contiguous across batches, e.g. for a batch size of four:\n",
    "\n",
    "\n",
    "          |        Chunk 1        |------|        Chunk 2        |\n",
    "|        | Batch 1 | Batch 2 | Batch 3 |        | Batch 4 | Batch 5 | Batch 6 |\n",
    "|--------|---------|---------|---------|--------|---------|---------|---------|\n",
    "| Song 1 | Block 1 | Block 2 | Block 3 | Song 5 | Block 1 | Block 2 | Block 3 |\n",
    "| Song 2 | Block 1 | Block 2 | Block 3 | Song 6 | Block 1 | Block 2 | Block 3 |\n",
    "| Song 3 | Block 1 | Block 2 | Block 3 | Song 7 | Block 1 | Block 2 | Block 3 |\n",
    "| Song 4 | Block 1 | Block 2 | Block 3 | Song 8 | Block 1 | Block 2 | Block 3 |\n",
    "\n",
    "Note that the above shows songs that are all the same length, which of course isn't what we have in reality.\n",
    "\n",
    "This means that we either\n",
    "- Crop long songs\n",
    "- Pad short songs\n",
    "- Connect them in a ragged way\n",
    "\n",
    "The video takes the cropping approach, picking a given 'chunk' (i.e. multiple of block) size and cropping the song to a multiple of this chunk size, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks = 3\n",
    "block_size = 256\n",
    "chunk_size = blocks * block_size\n",
    "chunk_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So mod the song length by chunk size and crop.\n",
    "\n",
    "Use `reshape` (or `view`?) to rearrange the a song into chunks, then `concat` to join the songs into one list of chunks, then `chunk` to split into batches.\n",
    "\n",
    "Following the above, batch 1 block 1 should be the precursor to batch 2 block 1.\n",
    "\n",
    "Data and labels per chunk are the same as in a vanilla transformer - labels are data offset by one.\n",
    "\n",
    "# To consider\n",
    "\n",
    "- Bigger data set\n",
    "    - Cleaning / preparing files\n",
    "    - Wrap in PyTorch data helper classes (help with batching?)\n",
    "    - Streaming?\n",
    "\n",
    "- Encoding strategy\n",
    "    - Could be on the fly if quicker than GPU takes to process a batch, otherwise pre-encode\n",
    "    - Follow song-per-batch-layer as outlined above\n",
    "\n",
    "- Use einops rather than manually applying transformation functions where practical\n",
    "\n",
    "- Relative positional encodings\n",
    "\n",
    "- KNN memory\n",
    "    - Vector index (fais?)\n",
    "    - Memory mapped file for db\n",
    "    - Second to last block only\n",
    "    - Look up K nearest keys / values\n",
    "\n",
    "- TransformerXL recurrent memory\n",
    "\n",
    "- Vectorise head operations\n",
    "\n",
    "- Monitoring during training\n",
    "    - Tensorflow?\n",
    "\n",
    "- 'Reverse teacher forcing' (offset future mask extra step)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
