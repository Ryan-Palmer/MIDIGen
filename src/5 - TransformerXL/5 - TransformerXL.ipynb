{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerXL\n",
    "\n",
    "Our vanilla transformer showed improvements, but suffered from only having attention within the current block.\n",
    "\n",
    "We also only used absolute positional encodings, so tokens knew where they were in the sequence but not where they were relative to other tokens.\n",
    "\n",
    "[TransformerXL](https://research.google/blog/transformer-xl-unleashing-the-potential-of-attention-models/) tackles both of these problems by \n",
    "1. Using a 'memory' for keys and values from the previous block, allowing information to propagate through time.\n",
    "2. Employing relative positional encoding.\n",
    "\n",
    "Obviously for either of these to work, data needs to be fed in sequentially, so our loading and batching strategy will once again need revisiting.\n",
    "\n",
    "# Coding A Paper\n",
    "\n",
    "Luckily I found [this walkthrough](https://www.youtube.com/playlist?list=PLam9sigHPGwOe8VDoS_6VT4jjlgs9Uepb) in the style of Karpathy's makemore videos.\n",
    "\n",
    "## Notes\n",
    "\n",
    "### Ep.2 Keeping GPUs Busy\n",
    "\n",
    "We need to keep our music blocks contiguous across batches, e.g. for a batch size of four:\n",
    "\n",
    "\n",
    "          |        Chunk 1        |------|        Chunk 2        |\n",
    "|        | Batch 1 | Batch 2 | Batch 3 |        | Batch 4 | Batch 5 | Batch 6 |\n",
    "|--------|---------|---------|---------|--------|---------|---------|---------|\n",
    "| Song 1 | Block 1 | Block 2 | Block 3 | Song 5 | Block 1 | Block 2 | Block 3 |\n",
    "| Song 2 | Block 1 | Block 2 | Block 3 | Song 6 | Block 1 | Block 2 | Block 3 |\n",
    "| Song 3 | Block 1 | Block 2 | Block 3 | Song 7 | Block 1 | Block 2 | Block 3 |\n",
    "| Song 4 | Block 1 | Block 2 | Block 3 | Song 8 | Block 1 | Block 2 | Block 3 |\n",
    "\n",
    "Note that the above shows songs that are all the same length, which of course isn't what we have in reality.\n",
    "\n",
    "This means that we either\n",
    "- Crop long songs\n",
    "- Pad short songs\n",
    "- Connect them in a ragged way\n",
    "\n",
    "The video takes the cropping approach, picking a given 'chunk' (i.e. multiple of block) size and cropping the song to a multiple of this chunk size, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks = 3\n",
    "block_size = 256\n",
    "chunk_size = blocks * block_size\n",
    "chunk_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So mod the song length by chunk size and crop.\n",
    "\n",
    "Use `reshape` (or `view`?) to rearrange the a song into chunks, then `concat` to join the songs into one list of chunks, then `chunk` to split into batches.\n",
    "\n",
    "Following the above, batch 1 block 1 should be the precursor to batch 2 block 1.\n",
    "\n",
    "Data and labels per chunk are the same as in a vanilla transformer - labels are data offset by one.\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
