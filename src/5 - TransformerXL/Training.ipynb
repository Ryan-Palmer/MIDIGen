{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'5 - TransformerXL')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from itertools import chain, cycle, groupby\n",
    "from functools import reduce\n",
    "from typing import Collection, List\n",
    "from pathlib import Path\n",
    "import music21 as m21\n",
    "musescore_path = '/usr/bin/mscore'\n",
    "m21.environment.set('musicxmlPath', musescore_path)\n",
    "m21.environment.set('musescoreDirectPNGPath', musescore_path)\n",
    "from midi_encoding import *\n",
    "from einops import rearrange, repeat, pack, unpack, einsum\n",
    "import faiss\n",
    "import time\n",
    "import math\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4090.\n"
     ]
    }
   ],
   "source": [
    "if device == \"cuda\":\n",
    "    print(f\"Device: {torch.cuda.get_device_name()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep  1 12:07:08 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0  On |                  Off |\n",
      "| 30%   34C    P0             48W /  450W |    2536MiB /  24564MiB |     33%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A        36      G   /Xwayland                                   N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = MusicVocab()\n",
    "vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContiguousBatchSampler(Sampler):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.batches = []\n",
    "    \n",
    "    def precompute_indices(self, batch_size):\n",
    "        \n",
    "        file_count = len(self.dataset.file_lengths)\n",
    "        if file_count < batch_size:\n",
    "            raise ValueError('The number of files must be greater than or equal to the batch size, as files must be spread across a single batch dimension.')\n",
    "        \n",
    "        file_idxs = list(range(batch_size))\n",
    "        file_positions = [0] * batch_size\n",
    "\n",
    "        while True:\n",
    "            batch = []\n",
    "            for batch_idx in range(batch_size):\n",
    "                \n",
    "                current_file_idx = file_idxs[batch_idx]\n",
    "                current_file_position = file_positions[batch_idx]\n",
    "                current_file_length = self.dataset.file_lengths[current_file_idx]\n",
    "                \n",
    "                # Check if the current file is exhausted\n",
    "                if current_file_position == current_file_length:\n",
    "                    # Find the next file that hasn't been started\n",
    "                    files_exhausted = True\n",
    "                    min_file_index = max(file_idxs) + 1\n",
    "                    for next_file_idx in range(min_file_index, file_count):\n",
    "                        if self.dataset.file_lengths[next_file_idx] > 0:\n",
    "                            current_file_idx = next_file_idx\n",
    "                            current_file_position = 0\n",
    "                            file_idxs[batch_idx] = current_file_idx\n",
    "                            file_positions[batch_idx] = current_file_position\n",
    "                            files_exhausted = False\n",
    "                            break\n",
    "                    \n",
    "                    if files_exhausted:\n",
    "                        return\n",
    "\n",
    "                batch.append([current_file_idx, current_file_position])                \n",
    "                file_positions[batch_idx] += 1\n",
    "\n",
    "            self.batches.append(batch)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in cycle(self.batches):\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidiDataset(Dataset):\n",
    "    def __init__(self, file_names, midi_path, score_path, sample_length, max_file_length):\n",
    "        self.file_names = file_names\n",
    "        self.data = None\n",
    "        self.file_lengths = None\n",
    "        self.total_samples = 0\n",
    "        self.sample_length = sample_length\n",
    "        self.midi_path = midi_path\n",
    "        self.score_path = score_path\n",
    "        self.max_file_length = max_file_length\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def load_samples(self):\n",
    "        data = []\n",
    "        file_lengths = []\n",
    "        for file_name in self.file_names:\n",
    "\n",
    "            midi_file_path = Path(self.midi_path, file_name)\n",
    "            score_file_path = Path(self.score_path, file_name)\n",
    "            encoded_file_path = Path(self.score_path, f'{file_name}.npy')\n",
    "\n",
    "            if (encoded_file_path.exists()):\n",
    "                # print(f'Loading {score_file_path}')\n",
    "                idx_score = np.load(encoded_file_path, allow_pickle=True)\n",
    "            else:\n",
    "                # print(f'Processing {midi_file_path}')\n",
    "                idx_score = midifile_to_idx_score(midi_file_path, vocab, True)\n",
    "                if (idx_score is None): # Skip files that could not be processed\n",
    "                    # print(f'Could not process {midi_file_path}')\n",
    "                    continue\n",
    "                np.save(score_file_path, idx_score)\n",
    "\n",
    "            samples = []\n",
    "            \n",
    "            # Split idx_score into blocks of size sample_length, padding the last blocks if necessary\n",
    "            for i in range(0, len(idx_score), self.sample_length):\n",
    "                block = idx_score[i:i + self.sample_length]\n",
    "                if len(block) < self.sample_length:\n",
    "                    last_tidx = block[-1, 1]\n",
    "                    pad_tidx = last_tidx + 1\n",
    "                    padding_count = self.sample_length - len(block)\n",
    "                    padding = np.stack([[vocab.pad_idx, pad_tidx]] * padding_count)\n",
    "                    block = np.concatenate([block, padding])\n",
    "\n",
    "                samples.append(block)\n",
    "\n",
    "            # Skip files that are empty or too long\n",
    "            if len(samples) == 0 or len(samples) > self.max_file_length:\n",
    "                continue\n",
    "            \n",
    "            data.append(torch.tensor(np.array(samples), device=device))\n",
    "            file_lengths.append(len(samples))\n",
    "        \n",
    "        self.total_samples = sum(file_lengths)\n",
    "        self.data = torch.nested.nested_tensor(data, device=device)\n",
    "        self.file_lengths = torch.tensor(file_lengths, device=device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_idx = idx[0]\n",
    "        sample_idx = idx[1]\n",
    "        sample = self.data[file_idx, sample_idx] # = self.data[idx]\n",
    "        return file_idx, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3839"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vg_large_path = Path('../data/midi/vg_large')\n",
    "vg_large_file_names = [f for f in os.listdir(vg_large_path) if os.path.isfile(os.path.join(vg_large_path, f))]\n",
    "\n",
    "# Ensure files are shuffled directly after assignment.\n",
    "# If they are shuffled in a different cell, and that cell is run multiple times, the order will change as we are shuffling the already-shuffled list.\n",
    "random.seed(42)\n",
    "random.shuffle(vg_large_file_names)\n",
    "\n",
    "len(vg_large_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we will have to go with splitting on filenames again this time, even though this doesn't equate to the same split in volume of training data (as not all tracks are the same length).\n",
    "\n",
    "We avoided this with the vanilla transformer by appending all files into one mega-performance and splitting that, but here we need to have files assigned and locked to a given batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file names: 3071, Valid file names: 384, Test file names: 384\n",
      "Loading train samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nested/__init__.py:166: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905979055/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  return _nested.nested_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading valid samples\n",
      "Loading test samples\n"
     ]
    }
   ],
   "source": [
    "sample_length = 256\n",
    "max_file_length = 32\n",
    "\n",
    "midi_path = Path('../data/midi/vg_large')\n",
    "score_path = Path(f'../data/numpy/vg_large/all')\n",
    "\n",
    "n1 = int(0.8 * len(vg_large_file_names))\n",
    "n2 = int(0.9 * len(vg_large_file_names))\n",
    "train_filenames = vg_large_file_names[:n1]\n",
    "valid_filenames = vg_large_file_names[n1:n2]\n",
    "test_filenames = vg_large_file_names[n2:]\n",
    "\n",
    "print(f'Train file names: {len(train_filenames)}, Valid file names: {len(valid_filenames)}, Test file names: {len(test_filenames)}')\n",
    "\n",
    "train_dataset = MidiDataset(train_filenames, midi_path, score_path, sample_length, max_file_length)\n",
    "valid_dataset = MidiDataset(valid_filenames, midi_path, score_path, sample_length, max_file_length)\n",
    "test_dataset = MidiDataset(test_filenames, midi_path, score_path, sample_length, max_file_length)\n",
    "\n",
    "print(f'Loading train samples')\n",
    "train_dataset.load_samples()\n",
    "\n",
    "print(f'Loading valid samples')\n",
    "valid_dataset.load_samples()\n",
    "\n",
    "print(f'Loading test samples')\n",
    "test_dataset.load_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 2198, Valid files: 275, Test files: 282\n"
     ]
    }
   ],
   "source": [
    "print(f'Train files: {len(train_dataset.file_lengths)}, Valid files: {len(valid_dataset.file_lengths)}, Test files: {len(test_dataset.file_lengths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing indices\n"
     ]
    }
   ],
   "source": [
    "# Batch size can be changed for a second phase of training quite quickly, it only requires re-computing the indices, not re-loading the data.\n",
    "batch_size = 32 \n",
    "train_sampler = ContiguousBatchSampler(train_dataset)\n",
    "valid_sampler = ContiguousBatchSampler(valid_dataset)\n",
    "test_sampler = ContiguousBatchSampler(test_dataset)\n",
    "\n",
    "print(f'Precomputing indices')\n",
    "train_sampler.precompute_indices(batch_size)\n",
    "valid_sampler.precompute_indices(batch_size)\n",
    "test_sampler.precompute_indices(batch_size)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_sampler=train_sampler)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_sampler=valid_sampler)\n",
    "test_data_loader = DataLoader(test_dataset, batch_sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss.contrib.torch_utils\n",
    "\n",
    "cache_size_gb = 8\n",
    "resources = faiss.StandardGpuResources() \n",
    "resources.setTempMemory(cache_size_gb * 1024 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN():\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __init__(self, dim, max_memories, db_filepath):\n",
    "        self.dim = dim\n",
    "        self.max_memories = max_memories\n",
    "        self.shape = (max_memories, 2, dim)\n",
    "        self.db_offset = 0\n",
    "        self.db = torch.zeros(self.shape, dtype = torch.float32, device=device) # np.memmap(db_filepath, mode = 'w+', dtype = np.float32, shape = self.shape)\n",
    "        self.index = faiss.GpuIndexFlatL2(resources, dim)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def add_to_db(self, new_data):\n",
    "        new_data_len = new_data.shape[0] # (t)\n",
    "\n",
    "        if new_data_len > self.max_memories:\n",
    "            raise ValueError('Batch size exceeds memory limit.')\n",
    "\n",
    "        # Circular buffer\n",
    "        # ids = (np.arange(new_data_len) + self.db_offset) % self.max_memories\n",
    "        ids = torch.arange(new_data_len) + self.db_offset\n",
    "\n",
    "        self.db[ids] = new_data#.detach().cpu().numpy()\n",
    "        self.db_offset += new_data_len\n",
    "        # Write to file\n",
    "        # self.db.flush()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search_and_retrieve(self, query, k):\n",
    "\n",
    "        # The tooltip says the args are (n, x, k) but that's the CPP api, it's actually (x, k) in Python (n is the first dim of x anyway so can be inferred).\n",
    "        distances, indices = self.index.search(query, k)\n",
    "        kvs = self.db[indices]\n",
    "        return kvs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def add(self, new_data):\n",
    "        # new_data = (t, 2, c)\n",
    "\n",
    "        # Add to db\n",
    "        self.add_to_db(new_data)\n",
    "\n",
    "        # Only keys are used in knn index\n",
    "        keys, vals = new_data.unbind(dim=-2)\n",
    "\n",
    "        # Add (t, c) tensors to index\n",
    "        # keys = keys.detach().cpu().numpy()\n",
    "        # keys = np.ascontiguousarray(keys)\n",
    "        self.index.add(keys.contiguous())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, query, k):\n",
    "\n",
    "        T, C = query.shape\n",
    "        \n",
    "        # If we have enough memories, search and retrieve, otherwise return zeros\n",
    "        if self.index.ntotal >= k:\n",
    "            # kvs = self.search_and_retrieve(np.ascontiguousarray(query.detach().cpu().numpy()), k)\n",
    "            kvs = self.search_and_retrieve(query, k)\n",
    "            # kvs = torch.tensor(kvs, device=device)\n",
    "        else:\n",
    "            kvs = torch.zeros((T, k, 2, C), device=device)\n",
    "\n",
    "        return kvs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def clear(self):\n",
    "        self.index.reset()\n",
    "        # self.db[:] = 0\n",
    "        self.db = torch.zeros(self.shape, dtype = torch.float32, device=device)\n",
    "        self.db_offset = 0\n",
    "        # print('Flushing DB')\n",
    "        # self.db.flush() # Is this necessary?\n",
    "        # print('Db Flushed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLRelativePosition(torch.nn.Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      n_buckets,\n",
    "      max_distance,\n",
    "      n_head,\n",
    "      scaling_factor):\n",
    "    \n",
    "    super().__init__()\n",
    "    self.scale = scaling_factor\n",
    "    self.num_buckets = n_buckets\n",
    "    self.max_distance = max_distance\n",
    "    self.relative_attention_embedding = torch.nn.Embedding(n_buckets, n_head)\n",
    "\n",
    "  def relative_position_bucket(self, relative_position_matrix):\n",
    "    inv_rel_pos = -relative_position_matrix\n",
    "    masked_rel_pos = torch.max(inv_rel_pos, torch.zeros_like(inv_rel_pos, device=device))\n",
    "\n",
    "    max_exact = self.num_buckets // 2\n",
    "\n",
    "    is_small = masked_rel_pos < max_exact\n",
    "    val_if_large = max_exact + (torch.log(masked_rel_pos.float() / max_exact) / math.log(self.max_distance / max_exact) * (self.num_buckets - max_exact)).long()\n",
    "\n",
    "    # Clip the values to the number of buckets - 1\n",
    "    val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, self.num_buckets - 1, device=device))\n",
    "\n",
    "    return torch.where(is_small, masked_rel_pos, val_if_large)\n",
    "\n",
    "  def forward(self, block_size):\n",
    "    block_pos = torch.arange(block_size, dtype=torch.long, device=device)\n",
    "    context_pos = torch.arange(-block_size, block_size, dtype=torch.long, device=device) # XL memory, context is twice block size, and current position starts in the middle.\n",
    "    block_rel_pos = rearrange(block_pos, 'i -> i 1')\n",
    "    context_rel_pos = rearrange(context_pos, 'j -> 1 j')\n",
    "    rel_pos = context_rel_pos - block_rel_pos\n",
    "\n",
    "    position_bucket_indices = self.relative_position_bucket(rel_pos)\n",
    "\n",
    "    rp_values = self.relative_attention_embedding(position_bucket_indices)\n",
    "    rp_values = rearrange(rp_values, 'i j h -> () h i j')\n",
    "    \n",
    "    return rp_values * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.n_head = n_head\n",
    "        self.head_size = n_embed // n_head\n",
    "        head_total_size = n_head * self.head_size\n",
    "        self.key = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.query = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.project = torch.nn.Linear(head_total_size, n_embed)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, relative_positions, x, xl_memory):\n",
    "\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # Chris's implementation does `queries = queries * (self.head_size ** -0.5)` here but I don't think it is correct.\n",
    "\n",
    "        k_xl, v_xl = xl_memory.unbind(dim = -2) # assume stacked\n",
    "        k = torch.cat((k_xl, k), dim = -2) # prepend XL memory\n",
    "        v = torch.cat((v_xl, v), dim = -2) # prepend XL memory\n",
    "\n",
    "        ### LOCAL ATTENTION\n",
    "\n",
    "        # Split heads\n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        k = rearrange(k, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        v = rearrange(v, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "\n",
    "        w = einsum(q, k, 'b h i d, b h j d -> b h i j')\n",
    "        i, j = w.shape[-2:]\n",
    "\n",
    "        # Add relative positional encoding and scale\n",
    "        w = w + relative_positions[..., -i:, -j:]\n",
    "        w = w * (self.head_size ** -0.5)\n",
    "        \n",
    "        mask = torch.ones((i,j), dtype = torch.bool, device=device).triu(j-i+1) # Can't cache this as its shape depends on whether we have XL memory or not.\n",
    "        w = w.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        self.dropout(w)\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        weighted_values = w@v # b h t d\n",
    "        # Concat heads\n",
    "        weighted_values = rearrange(weighted_values, 'b h t d -> b t (h d)')\n",
    "        \n",
    "        out = self.project(weighted_values)\n",
    "\n",
    "        # new XL memories\n",
    "\n",
    "        # Concatenate key and value heads\n",
    "        k = rearrange(k, 'b h t d -> b t (h d)', h = self.n_head)\n",
    "        v = rearrange(v, 'b h t d -> b t (h d)', h = self.n_head)\n",
    "        current_kv = torch.stack((k, v), dim=-2) # b t 2 (h d)\n",
    "\n",
    "        new_xl_memory = current_kv[:, -T:]\n",
    "\n",
    "        return self.dropout(out), new_xl_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_XLAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, db_filepath, sample_length, max_file_length, k, n_embed, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.k = k\n",
    "        self.n_head = n_head\n",
    "        head_size = n_embed // n_head\n",
    "        self.scale_factor = head_size ** -0.5\n",
    "        self.key = torch.nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.query = torch.nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.project = torch.nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.db_filepath = db_filepath\n",
    "\n",
    "        # Memory per batch dim, e.g. 32 sequences at 256 per sequence is 8192 memories per batch dim.\n",
    "        self.max_memories = max_file_length * sample_length\n",
    "        self.knn = None\n",
    "        self.current_file_idxs = None\n",
    "\n",
    "        self.gate_bias = torch.nn.Parameter(torch.randn(self.n_head, 1, 1))\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        self.current_file_idxs = None\n",
    "        self.knn = None\n",
    "\n",
    "    def forward(self, batch_file_idxs, relative_positions, x, xl_memory, eval):\n",
    "\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        if self.knn is None:\n",
    "            self.knn = {i: KNN(dim=self.n_embed, max_memories=self.max_memories, db_filepath=Path(f'{self.db_filepath}/batch_dim-{i}.db')) for i in range(B)}\n",
    "\n",
    "        # Clear batch dim's knn memory if file changes\n",
    "        if self.current_file_idxs != None:\n",
    "            for i in range(B):\n",
    "                if self.current_file_idxs[i] != batch_file_idxs[i]:\n",
    "                    # print(f'Clearing knn memory for batch dim {i}')\n",
    "                    self.knn[i].clear()\n",
    "\n",
    "        self.current_file_idxs = batch_file_idxs\n",
    "\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # This helps to mitigate drift in the embeddings which can cause the historical keys to become less aligned to the current queries.\n",
    "        q = F.normalize(q, dim=-1)\n",
    "        k = F.normalize(k, dim=-1)\n",
    "\n",
    "        k_xl, v_xl = xl_memory.unbind(dim = -2) # assume stacked\n",
    "        k = torch.cat((k_xl, k), dim = -2) # prepend XL memory\n",
    "        v = torch.cat((v_xl, v), dim = -2) # prepend XL memory\n",
    "\n",
    "        ### LOCAL ATTENTION\n",
    "\n",
    "        # Split heads\n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        k = rearrange(k, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        v = rearrange(v, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "\n",
    "        w = einsum(q, k, 'b h i d, b h j d -> b h i j')\n",
    "        i, j = w.shape[-2:]\n",
    "        \n",
    "        # Add relative positional encoding and scale\n",
    "\n",
    "        w = w + relative_positions[..., -i:, -j:]\n",
    "        w = w * self.scale_factor\n",
    "\n",
    "        mask = torch.ones((i,j), dtype = torch.bool, device=device).triu(j-i+1) # Can't cache this as its shape depends on whether we have XL memory or not.\n",
    "        w = w.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        self.dropout(w)\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        weighted_values = w@v # b h t d\n",
    "\n",
    "        ### KNN ATTENTION\n",
    "        knn_mask = torch.tensor([self.knn[i].index.ntotal > 0 for i in range(B)], dtype=torch.bool, device=device)\n",
    "\n",
    "        # Only do knn if there are at least some memories\n",
    "        if knn_mask.any():\n",
    "\n",
    "            # t1 = time.time()\n",
    "            # print (\"Begin KNN operations\")\n",
    "\n",
    "            # Convert queries to search form\n",
    "            q = rearrange(q, 'b h t d -> b t (h d)')\n",
    "\n",
    "            # KNN returns zeroes if it doesn't have data.\n",
    "            mem_kv = torch.stack([self.knn[i].search(q[i], k = self.k) for i in range(B)], dim = 0) # b, t, k, 2, c\n",
    "            \n",
    "            mem_k, mem_v = mem_kv.unbind(dim = -2)\n",
    "            mem_k = rearrange(mem_k, 'b t k (h d) -> b h t k d', h=self.n_head)\n",
    "            mem_v = rearrange(mem_v, 'b t k (h d) -> b h t k d', h=self.n_head)\n",
    "\n",
    "            # Convert queries to attention form\n",
    "            q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "\n",
    "            # Sum over d for each combination of batch, head, time and top k to get qk affinities, and hence weights for each k. resulting in a tensor of shape (b, h, t, k).\n",
    "            mem_w = einsum(q, mem_k, 'b h t d, b h t k d -> b h t k')\n",
    "            mem_w = mem_w * self.scale_factor\n",
    "\n",
    "            self.dropout(mem_w)\n",
    "            mem_w = F.softmax(mem_w, dim=-1)\n",
    "\n",
    "            # Weighted sum over the top k dimension for each combination of b, h, and t, resulting in a tensor of shape (b, h, t, d). Equivalent to doing w@v for each k and summing.\n",
    "            mem_weighted_values = einsum(mem_w, mem_v, 'b h t k, b h t k d -> b h t d')\n",
    "\n",
    "            ## Combined attention\n",
    "            \n",
    "            # Assume every memory has content. Empty memories will be masked out below.\n",
    "            combined_weighted_values = mem_weighted_values * self.gate_bias + weighted_values * (1 - self.gate_bias)\n",
    "\n",
    "            # Mask out combined weighted values where knn memory *is* empty and non-combined values where it *is not* empty, then merge them.\n",
    "            combined_weighted_values = combined_weighted_values * knn_mask.view(B, 1, 1, 1) + weighted_values * (~knn_mask).view(B, 1, 1, 1)\n",
    "\n",
    "            # Concat heads\n",
    "            combined_weighted_values = rearrange(combined_weighted_values, 'b h t d -> b t (h d)')\n",
    "            out = self.project(combined_weighted_values)\n",
    "\n",
    "            # t2 = time.time()\n",
    "            # print (\"End KNN operations, time taken:\", t2-t1)\n",
    "\n",
    "        else:\n",
    "            # Concat heads\n",
    "            weighted_values = rearrange(weighted_values, 'b h t d -> b t (h d)')\n",
    "            out = self.project(weighted_values)\n",
    "\n",
    "\n",
    "        # New XL memories\n",
    "\n",
    "        # Concatenate key and value heads\n",
    "        k = rearrange(k, 'b h t d -> b t (h d)', h = self.n_head)\n",
    "        v = rearrange(v, 'b h t d -> b t (h d)', h = self.n_head)\n",
    "        current_kv = torch.stack((k, v), dim=-2) # b t 2 c\n",
    "\n",
    "        new_xl_memory = current_kv[:, -T:]\n",
    "        # print(f'new mem shape:{new_xl_memory.shape}')\n",
    "        for i in range(B):\n",
    "            if eval:\n",
    "                 # Only add the last token to the memory during evaluation as we are passing in the same sequence offset by one token on each iteration.\n",
    "                trimmed_xl_memory = new_xl_memory[i][-1].unsqueeze(0)\n",
    "                # print(f'trimmed mem shape:{trimmed_xl_memory.shape}')\n",
    "                self.knn[i].add(trimmed_xl_memory)\n",
    "            else:\n",
    "                # During training, we advance a whole sequence block at a time.\n",
    "                self.knn[i].add(new_xl_memory[i])\n",
    "\n",
    "        return self.dropout(out), new_xl_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_embed, 4 * n_embed), # 4x is a common expansion factor\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(4 * n_embed, n_embed) # Project back to the residual stream\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = XLAttention(\n",
    "                            n_embed,\n",
    "                            n_head,\n",
    "                            dropout)\n",
    "        self.ff = FeedForward(n_embed, dropout)\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(n_embed)\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, rel_pos, x, xl_memories):\n",
    "        # Residual connections\n",
    "        attn_out, new_xl_memories = self.attention(rel_pos, self.layer_norm1(x), xl_memories)\n",
    "        x = x + attn_out\n",
    "        x = x + self.ff(self.layer_norm2(x))\n",
    "        return x, new_xl_memories\n",
    "    \n",
    "class KNNBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dbFilePath, sample_length, max_file_length, n_embed, n_head, top_k, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = KNN_XLAttention(\n",
    "                            dbFilePath,\n",
    "                            sample_length, \n",
    "                            max_file_length,\n",
    "                            top_k,\n",
    "                            n_embed,\n",
    "                            n_head,\n",
    "                            dropout)\n",
    "        self.ff = FeedForward(n_embed, dropout)\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(n_embed)\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, batch_file_idxs, relative_positions, x, xl_memory = None, eval=False):\n",
    "        # Residual connections\n",
    "        attn_out, new_xl_memories = self.attention(batch_file_idxs, relative_positions, self.layer_norm1(x), xl_memory, eval)\n",
    "        x = x + attn_out\n",
    "        x = x + self.ff(self.layer_norm2(x))\n",
    "        return x, new_xl_memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderTransformer_KNN_XL(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            db_filepath,\n",
    "            vocab,\n",
    "            sample_length,\n",
    "            max_file_length,\n",
    "            use_knn = False,\n",
    "            n_embed = 384, # /6 heads = 64 per head\n",
    "            n_head = 6, \n",
    "            n_layer = 6, \n",
    "            max_bar_position = 1024,\n",
    "            top_k = 5,\n",
    "            dropout = 0.2,\n",
    "            n_rel_pos_buckets = 32,\n",
    "            rel_pos_max_distance = 128):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.n_embed = n_embed\n",
    "        head_size = n_embed // n_head\n",
    "        scaling_factor = head_size ** 0.5\n",
    "        self.sample_length = sample_length\n",
    "        self.use_knn = use_knn\n",
    "        self.n_layer = n_layer\n",
    "        self.max_bar_position = max_bar_position\n",
    "        self.current_file_idxs = None\n",
    "        self.token_embedding = torch.nn.Embedding(self.vocab.size, n_embed)\n",
    "        self.rel_pos = XLRelativePosition(n_buckets = n_rel_pos_buckets, max_distance = rel_pos_max_distance, n_head = n_head, scaling_factor = scaling_factor)\n",
    "        self.rel_pos_knn = XLRelativePosition(n_buckets = n_rel_pos_buckets, max_distance = rel_pos_max_distance, n_head = n_head, scaling_factor = scaling_factor)\n",
    "        self.beat_embedding = torch.nn.Embedding(SAMPLES_PER_BAR, n_embed)\n",
    "        self.bar_embedding = torch.nn.Embedding(max_bar_position, n_embed)\n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList([])\n",
    "        for i in range(n_layer):\n",
    "\n",
    "            if self.isKNNLayer(i):\n",
    "                self.blocks.append(KNNBlock(db_filepath, sample_length, max_file_length, n_embed, n_head, top_k, dropout))\n",
    "            else:\n",
    "                self.blocks.append(Block(n_embed, n_head, dropout))\n",
    "            \n",
    "        self.layer_norm = torch.nn.LayerNorm(n_embed)\n",
    "        self.lm_head = torch.nn.Linear(n_embed, self.vocab.size)\n",
    "\n",
    "    def isKNNLayer(self, i):\n",
    "        if self.use_knn:\n",
    "            return i == self.n_layer - 2\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def forward(self, batch_file_idxs, x, xl_memories=None, targets=None):\n",
    "\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Could split these out in one go using the unbind function\n",
    "        token_idx = x[:, :, 0] # (B,T)\n",
    "        time_idx = x[:, :, 1] # (B,T)\n",
    "\n",
    "        sample_idx = time_idx % SAMPLES_PER_BAR # (B,T)\n",
    "        bar_idx = (time_idx // SAMPLES_PER_BAR) % self.max_bar_position # (B,T)\n",
    "\n",
    "        rel_pos = self.rel_pos(T)\n",
    "        rel_pos_knn = self.rel_pos_knn(T)\n",
    "\n",
    "        token_embed = self.token_embedding(token_idx) # (B,T,Embed)\n",
    "        bar_embed = self.bar_embedding(bar_idx) # (B,T,Embed)\n",
    "        sample_embed = self.beat_embedding(sample_idx) # (B,T,Embed)\n",
    "\n",
    "        x = token_embed + bar_embed + sample_embed\n",
    "\n",
    "        # If no XL memories, initialise them as 0 and reset the KNN memory\n",
    "        if xl_memories is None:\n",
    "            self.current_file_idxs = None\n",
    "            xl_memories = []\n",
    "            empty_batch_mem = torch.zeros((B, T, 2, self.n_embed), dtype=torch.long, device=device)\n",
    "            for layer, block in enumerate(self.blocks):\n",
    "                xl_memories.append(empty_batch_mem)\n",
    "                if self.isKNNLayer(layer):\n",
    "                    block.attention.clear_memory()\n",
    "\n",
    "        # If any file has changed (and it isn't the first run), replace the XL memory for that specific batch dim in every layer with 0\n",
    "        if self.current_file_idxs != None:\n",
    "            empty_mem = torch.zeros((T, 2, self.n_embed), dtype=torch.long, device=device)\n",
    "            for batch_dim, current_file_idx in enumerate(self.current_file_idxs):\n",
    "                if current_file_idx != batch_file_idxs[batch_dim]:\n",
    "                    # print(f\"Clearing XL mem for batch dim {batch_dim}\")\n",
    "                    for layer in range(self.n_layer):\n",
    "                        xl_memories[layer][batch_dim] = empty_mem\n",
    "        \n",
    "        self.current_file_idxs = batch_file_idxs\n",
    "\n",
    "        # Store the XL memories for each pass\n",
    "        new_xl_memories = []\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            if self.isKNNLayer(i):\n",
    "                x, xl_mem = block(batch_file_idxs, rel_pos_knn, x, xl_memories[i], eval=targets is None)\n",
    "            else:\n",
    "                x, xl_mem = block(rel_pos, x, xl_memories[i])\n",
    "\n",
    "            new_xl_memories.append(xl_mem.detach())\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        # TODO: Convert this section to use einops rearrange\n",
    "        if targets is None:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "        else:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.reshape(B*T, C) # Flatten all the batches\n",
    "            targets = targets.reshape(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss, new_xl_memories\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, x, max_new_tokens=1024, temperature=1.0):\n",
    "        self.eval()\n",
    "        \n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # We will just have one 'file' per dimension we are generating so that the knn memory is created and persists for the whole generation.\n",
    "        file_idxs = torch.arange(B, device=device)\n",
    "        xl_memories = None\n",
    "        dur_start, _ = vocab.duration_range\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # Get the second to last note index if it exists, otherwise return pad idx\n",
    "            if x.size(1) > 1:\n",
    "                second_to_last_nidx = x[:, -2, 0].unsqueeze(0) # (B, 1)\n",
    "            else:\n",
    "                second_to_last_nidx = torch.stack([torch.tensor([vocab.pad_idx], device=device) for _ in range(B)], dim=0)\n",
    "\n",
    "            # print(f'second_to_last_nidx: {second_to_last_nidx.size()}')\n",
    "            \n",
    "            # Could probably use unbind here\n",
    "            last_nidx = x[:, -1, 0] # (B, 1)\n",
    "            # print(f'last_nidx: {last_nidx.size()}')\n",
    "            last_tidx = x[:, -1, 1] # (B, 1)\n",
    "            # print(f'last_tidx: {last_tidx.size()}')\n",
    "\n",
    "            # If two tokens ago was a separator, the last token was a time-incrementing duration\n",
    "            duration_mask = second_to_last_nidx == vocab.sep_idx # (B, 1)\n",
    "\n",
    "            # Offset the duration idx to get the actual duration, and zero out if the previous token was not a separator\n",
    "            t_inc = (last_nidx - dur_start) * duration_mask\n",
    "            # print(f't_inc: {t_inc.size()}')\n",
    "\n",
    "            # Increment the time index by the duration\n",
    "            tidx_next = last_tidx + t_inc # (B, 1)\n",
    "            # print(f'tidx: {tidx_next.size()}')\n",
    "\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            x_cropped = x if x.size(1) <= self.sample_length else x[:, -self.sample_length:]\n",
    "\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _, xl_memories = self(file_idxs, x_cropped, xl_memories)\n",
    "\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # sample from the distribution\n",
    "            nidx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # print(f'nidx: {nidx_next.size()}')\n",
    "\n",
    "            # Concat with the time index\n",
    "            idx_next = torch.cat((nidx_next, tidx_next), dim=1).unsqueeze(0) # (B, C)\n",
    "            # print(f'idx_next: {idx_next.size()}')\n",
    "\n",
    "            # append sampled index to the running sequence and continue\n",
    "            x = torch.cat((x, idx_next), dim=1) # (B, T+1, C)\n",
    "\n",
    "        self.train()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11347982\n"
     ]
    }
   ],
   "source": [
    "model = DecoderTransformer_KNN_XL(db_filepath=Path('../data/numpy/knn-demo'), vocab=vocab, sample_length=sample_length, max_file_length=max_file_length, use_knn=True)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "token_embedding.weight \t torch.Size([392, 384])\n",
      "rel_pos.relative_attention_embedding.weight \t torch.Size([32, 6])\n",
      "rel_pos_knn.relative_attention_embedding.weight \t torch.Size([32, 6])\n",
      "beat_embedding.weight \t torch.Size([32, 384])\n",
      "bar_embedding.weight \t torch.Size([1024, 384])\n",
      "blocks.0.attention.key.weight \t torch.Size([384, 384])\n",
      "blocks.0.attention.query.weight \t torch.Size([384, 384])\n",
      "blocks.0.attention.value.weight \t torch.Size([384, 384])\n",
      "blocks.0.attention.project.weight \t torch.Size([384, 384])\n",
      "blocks.0.attention.project.bias \t torch.Size([384])\n",
      "blocks.0.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.0.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.0.ff.net.3.weight \t torch.Size([384, 1536])\n",
      "blocks.0.ff.net.3.bias \t torch.Size([384])\n",
      "blocks.0.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.0.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.0.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.0.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.1.attention.key.weight \t torch.Size([384, 384])\n",
      "blocks.1.attention.query.weight \t torch.Size([384, 384])\n",
      "blocks.1.attention.value.weight \t torch.Size([384, 384])\n",
      "blocks.1.attention.project.weight \t torch.Size([384, 384])\n",
      "blocks.1.attention.project.bias \t torch.Size([384])\n",
      "blocks.1.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.1.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.1.ff.net.3.weight \t torch.Size([384, 1536])\n",
      "blocks.1.ff.net.3.bias \t torch.Size([384])\n",
      "blocks.1.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.1.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.1.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.1.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.2.attention.key.weight \t torch.Size([384, 384])\n",
      "blocks.2.attention.query.weight \t torch.Size([384, 384])\n",
      "blocks.2.attention.value.weight \t torch.Size([384, 384])\n",
      "blocks.2.attention.project.weight \t torch.Size([384, 384])\n",
      "blocks.2.attention.project.bias \t torch.Size([384])\n",
      "blocks.2.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.2.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.2.ff.net.3.weight \t torch.Size([384, 1536])\n",
      "blocks.2.ff.net.3.bias \t torch.Size([384])\n",
      "blocks.2.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.2.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.2.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.2.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.3.attention.key.weight \t torch.Size([384, 384])\n",
      "blocks.3.attention.query.weight \t torch.Size([384, 384])\n",
      "blocks.3.attention.value.weight \t torch.Size([384, 384])\n",
      "blocks.3.attention.project.weight \t torch.Size([384, 384])\n",
      "blocks.3.attention.project.bias \t torch.Size([384])\n",
      "blocks.3.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.3.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.3.ff.net.3.weight \t torch.Size([384, 1536])\n",
      "blocks.3.ff.net.3.bias \t torch.Size([384])\n",
      "blocks.3.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.3.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.3.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.3.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.4.attention.gate_bias \t torch.Size([6, 1, 1])\n",
      "blocks.4.attention.key.weight \t torch.Size([384, 384])\n",
      "blocks.4.attention.query.weight \t torch.Size([384, 384])\n",
      "blocks.4.attention.value.weight \t torch.Size([384, 384])\n",
      "blocks.4.attention.project.weight \t torch.Size([384, 384])\n",
      "blocks.4.attention.project.bias \t torch.Size([384])\n",
      "blocks.4.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.4.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.4.ff.net.3.weight \t torch.Size([384, 1536])\n",
      "blocks.4.ff.net.3.bias \t torch.Size([384])\n",
      "blocks.4.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.4.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.4.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.4.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.5.attention.key.weight \t torch.Size([384, 384])\n",
      "blocks.5.attention.query.weight \t torch.Size([384, 384])\n",
      "blocks.5.attention.value.weight \t torch.Size([384, 384])\n",
      "blocks.5.attention.project.weight \t torch.Size([384, 384])\n",
      "blocks.5.attention.project.bias \t torch.Size([384])\n",
      "blocks.5.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.5.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.5.ff.net.3.weight \t torch.Size([384, 1536])\n",
      "blocks.5.ff.net.3.bias \t torch.Size([384])\n",
      "blocks.5.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.5.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.5.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.5.layer_norm2.bias \t torch.Size([384])\n",
      "layer_norm.weight \t torch.Size([384])\n",
      "layer_norm.bias \t torch.Size([384])\n",
      "lm_head.weight \t torch.Size([392, 384])\n",
      "lm_head.bias \t torch.Size([392])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-3\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 100\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(data_loader):\n",
    "    model.eval()\n",
    "        \n",
    "    xl_memories = None\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    \n",
    "    # Not working\n",
    "    # Start at a random point in the data, making sure we have enough data to evaluate\n",
    "    # offset = random.randint(0, len(data_loader.dataset) - (eval_iters + 1))\n",
    "    # data_iter = itertools.islice(iter(data_loader), offset, None)\n",
    "\n",
    "    data_iter = iter(data_loader)\n",
    "\n",
    "    for k in range(eval_iters):\n",
    "        file_idxs, batch = next(data_iter)\n",
    "        X, Y = batch[:, :-1], batch[:, 1:, 0] # drop absolute position from Y\n",
    "        _, loss, xl_memories = model(file_idxs, X, xl_memories, Y)\n",
    "        losses[k] = loss.item()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'midi_transformer_knn_xl-3'\n",
    "model_load_path = Path(f'../data/checkpoints/{model_name}.dat')\n",
    "model_save_path = Path(f'../data/checkpoints/{model_name}.dat')\n",
    "log_dir = Path(f'../tensorboard/{model_name}')\n",
    "tensorboard_writer = SummaryWriter(log_dir)\n",
    "\n",
    "average_log_losses = {  \n",
    "    \"train\" : [],\n",
    "    \"val\" : []\n",
    "}\n",
    "\n",
    "epochs = 0\n",
    "\n",
    "def save_checkpoint(iterations):\n",
    "    train_loss = estimate_loss(train_data_loader)\n",
    "    val_loss = estimate_loss(valid_data_loader)\n",
    "    tensorboard_writer.add_scalar('Loss/train', train_loss, iterations)\n",
    "    tensorboard_writer.add_scalar('Loss/val', val_loss, iterations)\n",
    "    train_log_loss = train_loss.log10().item()\n",
    "    val_log_loss = val_loss.log10().item()\n",
    "    average_log_losses['train'].append(train_log_loss)\n",
    "    average_log_losses['val'].append(val_log_loss)\n",
    "    print(f'Iteration {iterations}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    torch.save({\n",
    "        'iter': iterations,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'losses': average_log_losses,\n",
    "        'epochs': epochs\n",
    "    }, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 200\n",
    "total_iterations = 40000\n",
    "start_iter = 0\n",
    "\n",
    "if model_load_path.exists():\n",
    "    checkpoint = torch.load(model_load_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    average_log_losses = checkpoint['losses']\n",
    "    iterations = checkpoint['iter']\n",
    "    epochs = checkpoint['epochs']\n",
    "    start_iter = iterations + 1\n",
    "    print(f\"Loaded model from iteration {iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training from epoch 0 for 40000 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  x.storage().data_ptr() + x.storage_offset() * 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Train Loss: 6.0940, Val Loss: 6.0878\n",
      "Iteration 200, Train Loss: 1.8475, Val Loss: 1.8602\n",
      "Iteration 400, Train Loss: 1.6842, Val Loss: 1.6945\n",
      "Iteration 600, Train Loss: 1.6153, Val Loss: 1.6266\n"
     ]
    }
   ],
   "source": [
    "# import torch.profiler\n",
    "\n",
    "model.train()\n",
    "\n",
    "remaining_iters = total_iterations - start_iter\n",
    "if remaining_iters != -1:\n",
    "\n",
    "    # Start with no memory, from the first batch.\n",
    "    # Could load memories and offset batch index if we wanted to resume training with the same memories.\n",
    "\n",
    "    print(f\"Training from epoch {epochs} for {remaining_iters} iterations\")\n",
    "    \n",
    "    xl_memories = None\n",
    "    initial_file_idxs = None\n",
    "    train_data = iter(train_data_loader)\n",
    "    offset_iter = start_iter\n",
    "    # # Initialize the profiler\n",
    "    # with torch.profiler.profile(\n",
    "    #     activities=[\n",
    "    #         torch.profiler.ProfilerActivity.CPU,\n",
    "    #         torch.profiler.ProfilerActivity.CUDA,\n",
    "    #     ],\n",
    "    #     schedule=torch.profiler.schedule(wait=1, warmup=1, active=3),\n",
    "    #     on_trace_ready=torch.profiler.tensorboard_trace_handler(log_dir),\n",
    "    #     record_shapes=True,\n",
    "    #     profile_memory=True,\n",
    "    #     with_stack=True\n",
    "    # ) as prof:\n",
    "    for iteration in range(remaining_iters):\n",
    "        offset_iter = iteration + start_iter\n",
    "\n",
    "        # print(f'Iteration {offset_iter}')\n",
    "\n",
    "        if offset_iter % eval_interval == 0:\n",
    "            save_checkpoint(offset_iter)\n",
    "\n",
    "        # Configure minibatch\n",
    "        file_idxs, batch = next(train_data)\n",
    "\n",
    "        if initial_file_idxs is None:\n",
    "            initial_file_idxs = file_idxs\n",
    "\n",
    "        if torch.equal(initial_file_idxs, file_idxs):\n",
    "            epochs += 1\n",
    "        \n",
    "        X, Y = batch[:, :-1], batch[:, 1:, 0]\n",
    "\n",
    "        # Forward pass\n",
    "        logits, loss, xl_memories = model(file_idxs, X, xl_memories, Y)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # prof.step()\n",
    "\n",
    "    save_checkpoint(offset_iter + 1)\n",
    "\n",
    "    # Print profiler results\n",
    "    # print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final training loss:', 10 ** average_log_losses['train'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final validation loss:', 10 ** average_log_losses['val'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(average_log_losses['train'][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(average_log_losses['val'][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_idx = torch.zeros((1,1,2), dtype=torch.long, device=device)\n",
    "generated_tokens = model.generate(init_idx, max_new_tokens=512).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = generated_tokens[0, :, 0]\n",
    "vocab.to_tokens(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_stream = idx_to_stream_enc(np.array(score), vocab)\n",
    "generated_stream.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_stream.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_midi_file = random.choice(test_filenames)\n",
    "random_test_path = Path(midi_path, random_test_midi_file)\n",
    "random_test_idx_score = midifile_to_idx_score(random_test_path, vocab)\n",
    "random_test_intro = random_test_idx_score[:256]\n",
    "random_test_intro_stream = idx_to_stream_enc(np.array(random_test_intro[:, 0]), vocab)\n",
    "random_test_intro_stream.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_intro_stream.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_init = torch.tensor(random_test_intro, device=device).unsqueeze(0)\n",
    "random_test_continued = model.generate(random_test_init, max_new_tokens=512).cpu()[0, :, 0]\n",
    "random_test_continued_stream = idx_to_stream_enc(np.array(random_test_continued), vocab)\n",
    "random_test_continued_stream.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_continued_stream.show('midi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
