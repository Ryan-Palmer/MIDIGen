{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'5 - TransformerXL')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from itertools import chain, cycle, groupby\n",
    "from functools import reduce\n",
    "from typing import Collection, List\n",
    "from pathlib import Path\n",
    "import music21 as m21\n",
    "musescore_path = '/usr/bin/mscore'\n",
    "m21.environment.set('musicxmlPath', musescore_path)\n",
    "m21.environment.set('musescoreDirectPNGPath', musescore_path)\n",
    "from midi_encoding import *\n",
    "from einops import rearrange, repeat, pack, unpack, einsum\n",
    "import faiss\n",
    "import time\n",
    "import math\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4090.\n"
     ]
    }
   ],
   "source": [
    "if device == \"cuda\":\n",
    "    print(f\"Device: {torch.cuda.get_device_name()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug 25 14:40:05 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0  On |                  Off |\n",
      "| 30%   32C    P0             49W /  450W |    2948MiB /  24564MiB |     39%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = MusicVocab()\n",
    "vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContiguousBatchSampler(Sampler):\n",
    "    def __init__(self, dataset): #, start_offset=0):\n",
    "        self.dataset = dataset\n",
    "        self.batches = []\n",
    "        #self.start_offset = start_offset # Allows resuming training from a specific batch\n",
    "    \n",
    "    def precompute_indices(self, batch_size):\n",
    "        \n",
    "        file_count = len(self.dataset.file_lengths)\n",
    "        if file_count < batch_size:\n",
    "            raise ValueError('The number of files must be greater than or equal to the batch size, as files must be spread across a single batch dimension.')\n",
    "        \n",
    "        file_idxs = list(range(batch_size))\n",
    "        file_positions = [0] * batch_size\n",
    "\n",
    "        while True:\n",
    "            batch = []\n",
    "            for batch_idx in range(batch_size):\n",
    "                \n",
    "                current_file_idx = file_idxs[batch_idx]\n",
    "                current_file_position = file_positions[batch_idx]\n",
    "                current_file_length = self.dataset.file_lengths[current_file_idx]\n",
    "                \n",
    "                # Check if the current file is exhausted\n",
    "                if current_file_position == current_file_length:\n",
    "                    # Find the next file that hasn't been started\n",
    "                    files_exhausted = True\n",
    "                    min_file_index = max(file_idxs) + 1\n",
    "                    for next_file_idx in range(min_file_index, file_count):\n",
    "                        if self.dataset.file_lengths[next_file_idx] > 0:\n",
    "                            current_file_idx = next_file_idx\n",
    "                            current_file_position = 0\n",
    "                            file_idxs[batch_idx] = current_file_idx\n",
    "                            file_positions[batch_idx] = current_file_position\n",
    "                            files_exhausted = False\n",
    "                            break\n",
    "                    \n",
    "                    if files_exhausted:\n",
    "                        return\n",
    "\n",
    "                batch.append([current_file_idx, current_file_position])                \n",
    "                file_positions[batch_idx] += 1\n",
    "\n",
    "            self.batches.append(batch)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in cycle(self.batches): #[self.start_offset:]:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidiDataset(Dataset):\n",
    "    def __init__(self, file_names, midi_path, score_path, sample_length):\n",
    "        self.file_names = file_names\n",
    "        self.data = []\n",
    "        self.file_lengths = []\n",
    "        self.total_samples = 0\n",
    "        self.sample_length = sample_length\n",
    "        self.midi_path = midi_path\n",
    "        self.score_path = score_path\n",
    "\n",
    "    def load_samples(self):\n",
    "        for file_name in self.file_names:\n",
    "\n",
    "            midi_file_path = Path(self.midi_path, file_name)\n",
    "            score_file_path = Path(self.score_path, file_name)\n",
    "            encoded_file_path = Path(self.score_path, f'{file_name}.npy')\n",
    "\n",
    "            if (encoded_file_path.exists()):\n",
    "                # print(f'Loading {score_file_path}')\n",
    "                idx_score = np.load(encoded_file_path, allow_pickle=True)\n",
    "            else:\n",
    "                # print(f'Processing {midi_file_path}')\n",
    "                idx_score = midifile_to_idx_score(midi_file_path, vocab, True)\n",
    "                if (idx_score is None): # Skip files that could not be processed\n",
    "                    # print(f'Could not process {midi_file_path}')\n",
    "                    continue\n",
    "                np.save(score_file_path, idx_score)\n",
    "\n",
    "            samples = []\n",
    "            \n",
    "            # Split idx_score into blocks of size sample_length, padding the last blocks if necessary\n",
    "            for i in range(0, len(idx_score), self.sample_length):\n",
    "                block = idx_score[i:i + self.sample_length]\n",
    "                if len(block) < self.sample_length:\n",
    "                    last_tidx = block[-1, 1]\n",
    "                    pad_tidx = last_tidx + 1\n",
    "                    padding_count = self.sample_length - len(block)\n",
    "                    padding = np.stack([[vocab.pad_idx, pad_tidx]] * padding_count)\n",
    "                    block = np.concatenate([block, padding])\n",
    "\n",
    "                samples.append(block)\n",
    "\n",
    "            if len(samples) == 0: # Skip files with no valid samples\n",
    "                continue\n",
    "            \n",
    "            self.data.append(samples)\n",
    "            self.file_lengths.append(len(samples))\n",
    "        \n",
    "        self.total_samples = sum(self.file_lengths)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_idx = idx[0]\n",
    "        sample_idx = idx[1]\n",
    "        sample = self.data[file_idx][sample_idx]\n",
    "        return file_idx, torch.tensor(sample, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3839"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vg_large_path = Path('../data/midi/vg_large')\n",
    "vg_large_file_names = [f for f in os.listdir(vg_large_path) if os.path.isfile(os.path.join(vg_large_path, f))]\n",
    "\n",
    "# Ensure files are shuffled directly after assignment.\n",
    "# If they are shuffled in a different cell, and that cell is run multiple times, the order will change as we are shuffling the already-shuffled list.\n",
    "random.seed(42)\n",
    "random.shuffle(vg_large_file_names)\n",
    "\n",
    "len(vg_large_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we will have to go with splitting on filenames again this time, even though this doesn't equate to the same split in volume of training data (as not all tracks are the same length).\n",
    "\n",
    "We avoided this with the vanilla transformer by appending all files into one mega-performance and splitting that, but here we need to have files assigned and locked to a given batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File counts - train: 3071, valid: 384, test: 384\n",
      "Loading train samples\n",
      "Loading valid samples\n",
      "Loading test samples\n"
     ]
    }
   ],
   "source": [
    "# I think these would be difficult to change after the model training has begun as the batch idxs and memory dimensions would already be established.\n",
    "# We might be able to start a 'second phase' of training by changing these values and clearing the memories.\n",
    "batch_size = 64\n",
    "sample_length = 256\n",
    "\n",
    "midi_path = Path('../data/midi/vg_large')\n",
    "score_path = Path(f'../data/numpy/vg_large/all')\n",
    "\n",
    "n1 = int(0.8 * len(vg_large_file_names))\n",
    "n2 = int(0.9 * len(vg_large_file_names))\n",
    "train_filenames = vg_large_file_names[:n1]\n",
    "valid_filenames = vg_large_file_names[n1:n2]\n",
    "test_filenames = vg_large_file_names[n2:]\n",
    "\n",
    "print(f'File counts - train: {len(train_filenames)}, valid: {len(valid_filenames)}, test: {len(test_filenames)}')\n",
    "\n",
    "train_dataset = MidiDataset(train_filenames, midi_path, score_path, sample_length)\n",
    "valid_dataset = MidiDataset(valid_filenames, midi_path, score_path, sample_length)\n",
    "test_dataset = MidiDataset(test_filenames, midi_path, score_path, sample_length)\n",
    "\n",
    "print(f'Loading train samples')\n",
    "train_dataset.load_samples()\n",
    "\n",
    "print(f'Loading valid samples')\n",
    "valid_dataset.load_samples()\n",
    "\n",
    "print(f'Loading test samples')\n",
    "test_dataset.load_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing indices\n"
     ]
    }
   ],
   "source": [
    "train_sampler = ContiguousBatchSampler(train_dataset)\n",
    "valid_sampler = ContiguousBatchSampler(valid_dataset)\n",
    "test_sampler = ContiguousBatchSampler(test_dataset)\n",
    "\n",
    "print(f'Precomputing indices')\n",
    "train_sampler.precompute_indices(batch_size)\n",
    "valid_sampler.precompute_indices(batch_size)\n",
    "test_sampler.precompute_indices(batch_size)\n",
    "\n",
    "train_dataloader = iter(DataLoader(train_dataset, batch_sampler=train_sampler))\n",
    "valid_dataloader = iter(DataLoader(valid_dataset, batch_sampler=valid_sampler))\n",
    "test_dataloader = iter(DataLoader(test_dataset, batch_sampler=test_sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN():\n",
    "    def __init__(self, dim, max_memories, db_filepath):\n",
    "        self.dim = dim\n",
    "        self.max_memories = max_memories\n",
    "        self.shape = (max_memories, 2, dim)\n",
    "        self.db_offset = 0\n",
    "        self.db = np.memmap(db_filepath, mode = 'w+', dtype = np.float32, shape = self.shape)\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "    def add_to_db(self, new_data):\n",
    "        new_data_len = new_data.shape[0] # (t)\n",
    "        ids = (np.arange(new_data_len) + self.db_offset)\n",
    "        self.db[ids] = new_data.detach().cpu().numpy()\n",
    "        self.db_offset += new_data_len\n",
    "        # Write to file\n",
    "        self.db.flush()\n",
    "\n",
    "    def search_and_retrieve(self, query, k):\n",
    "\n",
    "        # The tooltip says the args are (n, x, k) but that's the CPP api, it's actually (x, k) in Python (n is the first dim of x anyway so can be inferred).\n",
    "        distances, indices = self.index.search(query, k)\n",
    "        \n",
    "        kvs = self.db[indices]\n",
    "        return kvs\n",
    "\n",
    "    def add(self, new_data):\n",
    "        # new_data = (t, 2, c)\n",
    "\n",
    "        # Add to db\n",
    "        self.add_to_db(new_data)\n",
    "\n",
    "        # Only keys are used in knn index\n",
    "        keys, vals = new_data.unbind(dim=-2)\n",
    "\n",
    "        # Add (t, c) tensors to index\n",
    "        keys = keys.detach().cpu().numpy()\n",
    "        keys = np.ascontiguousarray(keys)\n",
    "        self.index.add(keys)\n",
    "\n",
    "    def search(self, query, k):\n",
    "\n",
    "        T, C = query.shape\n",
    "        \n",
    "        # If we have enough memories, search and retrieve, otherwise return zeros\n",
    "        if self.index.ntotal >= k:\n",
    "            kvs = self.search_and_retrieve(np.ascontiguousarray(query.detach().cpu().numpy()), k)\n",
    "            kvs = torch.tensor(kvs, device=device)\n",
    "        else:\n",
    "            kvs = torch.zeros((T, k, 2, C), device=device)\n",
    "\n",
    "        return kvs\n",
    "\n",
    "    def clear(self):\n",
    "        self.index.reset()\n",
    "        self.db[:] = 0\n",
    "        self.db_offset = 0\n",
    "        self.db.flush() # Is this necessary?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLRelativePosition(torch.nn.Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      n_buckets,\n",
    "      max_distance,\n",
    "      n_head,\n",
    "      scaling_factor):\n",
    "    \n",
    "    super().__init__()\n",
    "    self.scale = scaling_factor\n",
    "    self.num_buckets = n_buckets\n",
    "    self.max_distance = max_distance\n",
    "    self.relative_attention_embedding = torch.nn.Embedding(n_buckets, n_head)\n",
    "\n",
    "  def relative_position_bucket(self, relative_position_matrix):\n",
    "    inv_rel_pos = -relative_position_matrix\n",
    "    masked_rel_pos = torch.max(inv_rel_pos, torch.zeros_like(inv_rel_pos, device=device))\n",
    "\n",
    "    max_exact = self.num_buckets // 2\n",
    "\n",
    "    is_small = masked_rel_pos < max_exact\n",
    "    val_if_large = max_exact + (torch.log(masked_rel_pos.float() / max_exact) / math.log(self.max_distance / max_exact) * (self.num_buckets - max_exact)).long()\n",
    "\n",
    "    # Clip the values to the number of buckets - 1\n",
    "    val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, self.num_buckets - 1, device=device))\n",
    "\n",
    "    return torch.where(is_small, masked_rel_pos, val_if_large)\n",
    "\n",
    "  def forward(self, block_size):\n",
    "    block_pos = torch.arange(block_size, dtype=torch.long, device=device)\n",
    "    context_pos = torch.arange(-block_size, block_size, dtype=torch.long, device=device) # XL memory, context is twice block size, and current position starts in the middle.\n",
    "    block_rel_pos = rearrange(block_pos, 'i -> i 1')\n",
    "    context_rel_pos = rearrange(context_pos, 'j -> 1 j')\n",
    "    rel_pos = context_rel_pos - block_rel_pos\n",
    "\n",
    "    position_bucket_indices = self.relative_position_bucket(rel_pos)\n",
    "\n",
    "    rp_values = self.relative_attention_embedding(position_bucket_indices)\n",
    "\n",
    "    rp_values = rearrange(rp_values, 'i j h -> () h i j')\n",
    "    return rp_values * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.n_head = n_head\n",
    "        self.head_size = n_embed // n_head\n",
    "        head_total_size = n_head * self.head_size\n",
    "        self.key = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.query = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embed, head_total_size, bias=False)\n",
    "        self.project = torch.nn.Linear(head_total_size, n_embed)\n",
    "        self.dropout = torch.nn.Dropout(dropout)    \n",
    "\n",
    "    def forward(self, relative_positions, x, xl_memory = None):\n",
    "\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # Chris's implementation does `queries = queries * (self.head_size ** -0.5)` here but I don't think it is correct.\n",
    "\n",
    "        if xl_memory is not None:\n",
    "            k_xl, v_xl = xl_memory.unbind(dim = -2) # assume stacked\n",
    "            k = torch.cat((k_xl, k), dim = -2) # prepend XL memory\n",
    "            v = torch.cat((v_xl, v), dim = -2) # prepend XL memory\n",
    "\n",
    "        ### LOCAL ATTENTION\n",
    "\n",
    "        # Split heads\n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        k = rearrange(k, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        v = rearrange(v, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "\n",
    "        w = einsum(q, k, 'b h i d, b h j d -> b h i j')\n",
    "        i, j = w.shape[-2:]\n",
    "\n",
    "        # Add relative positional encoding and scale\n",
    "        w = w + relative_positions[..., -i:, -j:]\n",
    "        w = w * (self.head_size ** -0.5)\n",
    "        \n",
    "        mask = torch.logical_not(torch.tril(torch.ones((i,j), dtype = torch.bool, device=device))) # Can't cache this as its shape depends on whether we have XL memory or not.\n",
    "        w = w.masked_fill(mask, float('-inf'))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        weighted_values = w@v # b h t d\n",
    "        # Concat heads\n",
    "        weighted_values = rearrange(weighted_values, 'b h t d -> b t (h d)')\n",
    "        \n",
    "        out = self.project(weighted_values)\n",
    "\n",
    "        # new XL memories\n",
    "\n",
    "        # Concatenate key and value heads\n",
    "        k = rearrange(k, 'b h t d -> b t (h d)', h = self.n_head)\n",
    "        v = rearrange(v, 'b h t d -> b t (h d)', h = self.n_head)\n",
    "        current_kv = torch.stack((k, v), dim=-2) # b t 2 (h d)\n",
    "\n",
    "        if xl_memory is None:\n",
    "            new_xl_memory = current_kv\n",
    "        else:\n",
    "            new_xl_memory = current_kv[:, -T:]\n",
    "\n",
    "        return self.dropout(out), new_xl_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_XLAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dbFilePath, batch_size, k, n_embed, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.k = k\n",
    "        self.n_head = n_head\n",
    "        head_size = n_embed // n_head\n",
    "        self.scale_factor = head_size ** -0.5\n",
    "        self.key = torch.nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.query = torch.nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embed, n_embed, bias=False)\n",
    "        self.project = torch.nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Memory per batch dim !!! This mem count was for a single db, not a db per batch dim\n",
    "        self.knn = {i: KNN(n_embed, 1024, Path(f'{dbFilePath}/batch_dim-{i}.db')) for i in range(batch_size)} # KNN memory will get or create the files, so we just need to be consistent with the file names.\n",
    "        self.current_file_idxs = None\n",
    "\n",
    "        self.gate_bias = torch.nn.Parameter(torch.randn(self.n_head, 1, 1))\n",
    "    \n",
    "\n",
    "    def forward(self, batch_file_idxs, relative_positions, x, xl_memory = None):\n",
    "\n",
    "        # Clear batch dim's knn memory if file changes\n",
    "        if self.current_file_idxs != None:\n",
    "            for i in range(len(self.current_file_idxs)):\n",
    "                if self.current_file_idxs[i] != batch_file_idxs[i]:\n",
    "                    self.knn[i].clear()\n",
    "\n",
    "        self.current_file_idxs = batch_file_idxs\n",
    "\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # This helps to mitigate drift in the embeddings which can cause the historical keys to become less aligned to the current queries.\n",
    "        q = F.normalize(q, dim=-1)\n",
    "        k = F.normalize(k, dim=-1)\n",
    "\n",
    "        if xl_memory is not None:\n",
    "            k_xl, v_xl = xl_memory.unbind(dim = -2) # assume stacked\n",
    "            k = torch.cat((k_xl, k), dim = -2) # prepend XL memory\n",
    "            v = torch.cat((v_xl, v), dim = -2) # prepend XL memory\n",
    "\n",
    "        ### LOCAL ATTENTION\n",
    "\n",
    "        # Split heads\n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        k = rearrange(k, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "        v = rearrange(v, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "\n",
    "        w = einsum(q, k, 'b h i d, b h j d -> b h i j')\n",
    "        i, j = w.shape[-2:]\n",
    "        \n",
    "        # Add relative positional encoding and scale\n",
    "        w = w + relative_positions[..., -i:, -j:]\n",
    "        w = w * self.scale_factor\n",
    "\n",
    "        mask = torch.logical_not(torch.tril(torch.ones((i,j), dtype = torch.bool, device=device))) # Can't cache this as its shape depends on whether we have XL memory or not.\n",
    "        w = w.masked_fill(mask, float('-inf'))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        weighted_values = w@v # b h t d\n",
    "\n",
    "        ### KNN ATTENTION\n",
    "        knn_mask = torch.tensor([self.knn[i].index.ntotal > 0 for i in range(B)], dtype=torch.bool, device=device)\n",
    "\n",
    "        # Only do knn if there are at least some memories\n",
    "        if knn_mask.any():\n",
    "\n",
    "            t1 = time.time()\n",
    "            print (\"Begin KNN operations\")\n",
    "\n",
    "            # Convert queries to search form\n",
    "            q = rearrange(q, 'b h t d -> b t (h d)')\n",
    "\n",
    "            # KNN returns zeroes if it doesn't have data.\n",
    "            mem_kv = torch.stack([self.knn[i].search(q[i], k = self.k) for i in range(B)], dim = 0) # b, t, k, 2, c\n",
    "            \n",
    "            mem_k, mem_v = mem_kv.unbind(dim = -2)\n",
    "            mem_k = rearrange(mem_k, 'b t k (h d) -> b h t k d', h=self.n_head)\n",
    "            mem_v = rearrange(mem_v, 'b t k (h d) -> b h t k d', h=self.n_head)\n",
    "\n",
    "            # Convert queries to attention form\n",
    "            q = rearrange(q, 'b t (h d) -> b h t d', h = self.n_head)\n",
    "\n",
    "            # Sum over d for each combination of batch, head, time and top k to get qk affinities, and hence weights for each k. resulting in a tensor of shape (b, h, t, k).\n",
    "            mem_w = einsum('b h t d, b h t k d -> b h t k', q, mem_k)\n",
    "            mem_w = mem_w * self.scale_factor\n",
    "            mem_w = F.softmax(mem_w, dim=-1)\n",
    "\n",
    "            # Weighted sum over the top k dimension for each combination of b, h, and t, resulting in a tensor of shape (b, h, t, d). Equivalent to doing w@v for each k and summing.\n",
    "            mem_weighted_values = einsum('b h t k, b h t k d -> b h t d', mem_w, mem_v)\n",
    "\n",
    "            ## Combined attention\n",
    "            \n",
    "            # Assume every memory has content. Empty memories will be masked out below.\n",
    "            combined_weighted_values = mem_weighted_values * self.gate_bias + weighted_values * (1 - self.gate_bias)\n",
    "\n",
    "            # Mask out combined weighted values where knn memory *is* empty and non-combined values where it *is not* empty, then merge them.\n",
    "            combined_weighted_values = combined_weighted_values * knn_mask.view(B, 1, 1, 1) + weighted_values * (~knn_mask).view(B, 1, 1, 1)\n",
    "\n",
    "            # Concat heads\n",
    "            combined_weighted_values = rearrange(combined_weighted_values, 'b h t d -> b t (h d)')\n",
    "            out = self.project(combined_weighted_values)\n",
    "\n",
    "            t2 = time.time()\n",
    "            print (\"End KNN operations, time taken:\", t2-t1)\n",
    "\n",
    "        else:\n",
    "            # Concat heads\n",
    "            weighted_values = rearrange(weighted_values, 'b h t d -> b t (h d)')\n",
    "            out = self.project(weighted_values)\n",
    "\n",
    "\n",
    "        # New XL memories\n",
    "\n",
    "        # Concatenate key and value heads\n",
    "        k = rearrange(k, 'b h t d -> b t (h d)', h = self.n_head)\n",
    "        v = rearrange(v, 'b h t d -> b t (h d)', h = self.n_head)\n",
    "        current_kv = torch.stack((k, v), dim=-2) # b t 2 c\n",
    "\n",
    "        if xl_memory is None:\n",
    "            new_xl_memory = current_kv\n",
    "        else:\n",
    "            new_xl_memory = current_kv[:, -T:]\n",
    "\n",
    "        for i in range(B):\n",
    "            self.knn[i].add(new_xl_memory[i])\n",
    "\n",
    "        return self.dropout(out), new_xl_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_embed, 4 * n_embed), # 4x is a common expansion factor\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(4 * n_embed, n_embed), # Project back to the residual stream\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = XLAttention(\n",
    "                            n_embed,\n",
    "                            n_head,\n",
    "                            dropout)\n",
    "        self.ff = FeedForward(n_embed, dropout)\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(n_embed)\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, rel_pos, x, xl_memories):\n",
    "        # Residual connections\n",
    "        attn_out, new_xl_memories = self.attention(rel_pos, self.layer_norm1(x), xl_memories)\n",
    "        x = x + attn_out\n",
    "        x = x + self.ff(self.layer_norm2(x))\n",
    "        return x, new_xl_memories\n",
    "    \n",
    "class KNNBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dbFilePath, n_embed, n_head, top_k, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = KNN_XLAttention(\n",
    "                            dbFilePath,\n",
    "                            batch_size,\n",
    "                            top_k,\n",
    "                            n_embed,\n",
    "                            n_head,\n",
    "                            dropout)\n",
    "        self.ff = FeedForward(n_embed, dropout)\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(n_embed)\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, batch_file_idxs, relative_positions, x, xl_memory = None):\n",
    "        # Residual connections\n",
    "        attn_out, new_xl_memories = self.attention(batch_file_idxs, relative_positions, self.layer_norm1(x), xl_memory)\n",
    "        x = x + attn_out\n",
    "        x = x + self.ff(self.layer_norm2(x))\n",
    "        return x, new_xl_memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderTransformer_KNN_XL(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dbFilePath,\n",
    "            vocab_size,\n",
    "            batch_size, # required so we can init a memory per batch dim\n",
    "            sample_length,\n",
    "            n_embed = 384, # /6 heads = 64 per head\n",
    "            n_head = 6, \n",
    "            n_layer = 6, \n",
    "            max_bar_position = 1024,\n",
    "            top_k = 5,\n",
    "            dropout = 0.2,\n",
    "            n_rel_pos_buckets = 32,\n",
    "            rel_pos_max_distance = 128):\n",
    "        \n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        scaling_factor = head_size ** 0.5\n",
    "        self.sample_length = sample_length\n",
    "        self.n_layer = n_layer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_bar_position = max_bar_position\n",
    "        self.token_embedding = torch.nn.Embedding(vocab_size, n_embed)\n",
    "        self.rel_pos = XLRelativePosition(n_buckets = n_rel_pos_buckets, max_distance = rel_pos_max_distance, n_head = n_head, scaling_factor = scaling_factor)\n",
    "        self.rel_pos_knn = XLRelativePosition(n_buckets = n_rel_pos_buckets, max_distance = rel_pos_max_distance, n_head = n_head, scaling_factor = scaling_factor)\n",
    "        self.beat_embedding = torch.nn.Embedding(SAMPLES_PER_BAR, n_embed)\n",
    "        self.bar_embedding = torch.nn.Embedding(max_bar_position, n_embed)\n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList([])\n",
    "        for i in range(n_layer):\n",
    "\n",
    "            if self.isKNNLayer(i):\n",
    "                self.blocks.append(KNNBlock(dbFilePath, n_embed, n_head, top_k, dropout))\n",
    "            else:\n",
    "                self.blocks.append(Block(n_embed, n_head, dropout))\n",
    "\n",
    "            \n",
    "        self.layer_norm = torch.nn.LayerNorm(n_embed)\n",
    "        self.lm_head = torch.nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def isKNNLayer(self, i):\n",
    "        return i == self.n_layer - 2\n",
    "    \n",
    "    # Bit of a hack\n",
    "    def clear_knn_memory(self):\n",
    "        knn = self.blocks[-2].attention.knn\n",
    "        for i in range (len(knn)):\n",
    "            print(f'Clearing knn memory {i}')\n",
    "            knn[i].clear()\n",
    "\n",
    "    def forward(self, batch_file_idxs, x, xl_memories=None, targets=None):\n",
    "\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Could split these out in one go using the unbind function\n",
    "        token_idx = x[:, :, 0] # (B,T)\n",
    "        time_idx = x[:, :, 1] # (B,T)\n",
    "\n",
    "        sample_idx = time_idx % SAMPLES_PER_BAR # (B,T)\n",
    "        bar_idx = (time_idx // SAMPLES_PER_BAR) % self.max_bar_position # (B,T)\n",
    "\n",
    "        if xl_memories is None:\n",
    "            xl_memories = (None,) * self.n_layer\n",
    "        else:\n",
    "            xl_memories = xl_memories\n",
    "\n",
    "        rel_pos = self.rel_pos(T)\n",
    "        rel_pos_knn = self.rel_pos_knn(T)\n",
    "\n",
    "        token_embed = self.token_embedding(token_idx) # (B,T,Embed)\n",
    "        bar_embed = self.bar_embedding(bar_idx) # (B,T,Embed)\n",
    "        sample_embed = self.beat_embedding(sample_idx) # (B,T,Embed)\n",
    "\n",
    "        x = token_embed + bar_embed + sample_embed\n",
    "\n",
    "        # Store the XL memories for each pass\n",
    "        new_xl_memories = []\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "\n",
    "            if self.isKNNLayer(i):\n",
    "                x, xl_mem = block(batch_file_idxs, rel_pos_knn, x, xl_memories[i])\n",
    "            else:\n",
    "                x, xl_mem = block(rel_pos, x, xl_memories[i])\n",
    "\n",
    "            new_xl_memories.append(xl_mem.detach())\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        # TODO: Convert this section to use einops rearrange\n",
    "        if targets is None:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "        else:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.reshape(B*T, C) # Flatten all the batches\n",
    "            targets = targets.reshape(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # if len(new_xl_memories) > 0: # Can these ever be empty?\n",
    "        return logits, loss, new_xl_memories\n",
    "        # else:\n",
    "        #     return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, x, max_new_tokens=1024, temperature=1.0):\n",
    "        self.eval()\n",
    "        \n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # We will just have one 'file' per dimension we are generating so that the knn memory is created and persists for the whole generation.\n",
    "        file_idxs = torch.arange(B, device=device)\n",
    "        \n",
    "        xl_memories = None\n",
    "        self.clear_knn_memory()\n",
    "\n",
    "        # print(f'idx: {idx.size()}')\n",
    "\n",
    "        dur_start, _ = vocab.duration_range\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # Get the second to last note index if it exists, otherwise return pad idx\n",
    "            if x.size(1) > 1:\n",
    "                second_to_last_nidx = x[:, -2, 0].unsqueeze(0) # (B, 1)\n",
    "            else:\n",
    "                second_to_last_nidx = torch.stack([torch.tensor([vocab.pad_idx], device=device) for _ in range(B)], dim=0)\n",
    "\n",
    "            # print(f'second_to_last_nidx: {second_to_last_nidx.size()}')\n",
    "            \n",
    "            # Could probably use unbind here\n",
    "            last_nidx = x[:, -1, 0] # (B, 1)\n",
    "            # print(f'last_nidx: {last_nidx.size()}')\n",
    "            last_tidx = x[:, -1, 1] # (B, 1)\n",
    "            # print(f'last_tidx: {last_tidx.size()}')\n",
    "\n",
    "            # If two tokens ago was a separator, the last token was a time-incrementing duration\n",
    "            duration_mask = second_to_last_nidx == vocab.sep_idx # (B, 1)\n",
    "\n",
    "            # Offset the duration idx to get the actual duration, and zero out if the previous token was not a separator\n",
    "            t_inc = (last_nidx - dur_start) * duration_mask\n",
    "            # print(f't_inc: {t_inc.size()}')\n",
    "\n",
    "            # Increment the time index by the duration\n",
    "            tidx_next = last_tidx + t_inc # (B, 1)\n",
    "            # print(f'tidx: {tidx_next.size()}')\n",
    "\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            x_cropped = x if x.size(1) <= self.sample_length else x[:, -self.sample_length:]\n",
    "\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(file_idxs, x_cropped, xl_memories)\n",
    "\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # sample from the distribution\n",
    "            nidx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # print(f'nidx: {nidx_next.size()}')\n",
    "\n",
    "            # Concat with the time index\n",
    "            idx_next = torch.cat((nidx_next, tidx_next), dim=1).unsqueeze(0) # (B, C)\n",
    "            # print(f'idx_next: {idx_next.size()}')\n",
    "\n",
    "            # append sampled index to the running sequence and continue\n",
    "            x = torch.cat((x, idx_next), dim=1) # (B, T+1, C)\n",
    "\n",
    "        self.train()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11347982\n"
     ]
    }
   ],
   "source": [
    "model = DecoderTransformer_KNN_XL(dbFilePath=Path('../data/numpy/knn-demo'), vocab_size=vocab.size, batch_size=batch_size, sample_length=sample_length)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "token_embedding.weight \t torch.Size([392, 384])\n",
      "rel_pos.relative_attention_embedding.weight \t torch.Size([32, 6])\n",
      "rel_pos_knn.relative_attention_embedding.weight \t torch.Size([32, 6])\n",
      "beat_embedding.weight \t torch.Size([32, 384])\n",
      "bar_embedding.weight \t torch.Size([1024, 384])\n",
      "blocks.0.attention.key.weight \t torch.Size([384, 384])\n",
      "blocks.0.attention.query.weight \t torch.Size([384, 384])\n",
      "blocks.0.attention.value.weight \t torch.Size([384, 384])\n",
      "blocks.0.attention.project.weight \t torch.Size([384, 384])\n",
      "blocks.0.attention.project.bias \t torch.Size([384])\n",
      "blocks.0.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.0.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.0.ff.net.2.weight \t torch.Size([384, 1536])\n",
      "blocks.0.ff.net.2.bias \t torch.Size([384])\n",
      "blocks.0.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.0.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.0.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.0.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.1.attention.key.weight \t torch.Size([384, 384])\n",
      "blocks.1.attention.query.weight \t torch.Size([384, 384])\n",
      "blocks.1.attention.value.weight \t torch.Size([384, 384])\n",
      "blocks.1.attention.project.weight \t torch.Size([384, 384])\n",
      "blocks.1.attention.project.bias \t torch.Size([384])\n",
      "blocks.1.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.1.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.1.ff.net.2.weight \t torch.Size([384, 1536])\n",
      "blocks.1.ff.net.2.bias \t torch.Size([384])\n",
      "blocks.1.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.1.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.1.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.1.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.2.attention.key.weight \t torch.Size([384, 384])\n",
      "blocks.2.attention.query.weight \t torch.Size([384, 384])\n",
      "blocks.2.attention.value.weight \t torch.Size([384, 384])\n",
      "blocks.2.attention.project.weight \t torch.Size([384, 384])\n",
      "blocks.2.attention.project.bias \t torch.Size([384])\n",
      "blocks.2.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.2.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.2.ff.net.2.weight \t torch.Size([384, 1536])\n",
      "blocks.2.ff.net.2.bias \t torch.Size([384])\n",
      "blocks.2.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.2.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.2.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.2.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.3.attention.key.weight \t torch.Size([384, 384])\n",
      "blocks.3.attention.query.weight \t torch.Size([384, 384])\n",
      "blocks.3.attention.value.weight \t torch.Size([384, 384])\n",
      "blocks.3.attention.project.weight \t torch.Size([384, 384])\n",
      "blocks.3.attention.project.bias \t torch.Size([384])\n",
      "blocks.3.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.3.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.3.ff.net.2.weight \t torch.Size([384, 1536])\n",
      "blocks.3.ff.net.2.bias \t torch.Size([384])\n",
      "blocks.3.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.3.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.3.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.3.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.4.attention.gate_bias \t torch.Size([6, 1, 1])\n",
      "blocks.4.attention.key.weight \t torch.Size([384, 384])\n",
      "blocks.4.attention.query.weight \t torch.Size([384, 384])\n",
      "blocks.4.attention.value.weight \t torch.Size([384, 384])\n",
      "blocks.4.attention.project.weight \t torch.Size([384, 384])\n",
      "blocks.4.attention.project.bias \t torch.Size([384])\n",
      "blocks.4.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.4.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.4.ff.net.2.weight \t torch.Size([384, 1536])\n",
      "blocks.4.ff.net.2.bias \t torch.Size([384])\n",
      "blocks.4.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.4.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.4.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.4.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.5.attention.key.weight \t torch.Size([384, 384])\n",
      "blocks.5.attention.query.weight \t torch.Size([384, 384])\n",
      "blocks.5.attention.value.weight \t torch.Size([384, 384])\n",
      "blocks.5.attention.project.weight \t torch.Size([384, 384])\n",
      "blocks.5.attention.project.bias \t torch.Size([384])\n",
      "blocks.5.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.5.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.5.ff.net.2.weight \t torch.Size([384, 1536])\n",
      "blocks.5.ff.net.2.bias \t torch.Size([384])\n",
      "blocks.5.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.5.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.5.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.5.layer_norm2.bias \t torch.Size([384])\n",
      "layer_norm.weight \t torch.Size([384])\n",
      "layer_norm.bias \t torch.Size([384])\n",
      "lm_head.weight \t torch.Size([392, 384])\n",
      "lm_head.bias \t torch.Size([392])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-4\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 100\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Estimating loss\")\n",
    "    \n",
    "    xl_memories = None\n",
    "    print(\"Clearing memories\")\n",
    "    model.clear_knn_memory()\n",
    "    print(\"Memories cleared\")\n",
    "    \n",
    "    losses = torch.zeros(eval_iters)\n",
    "\n",
    "    for k in range(eval_iters):\n",
    "        file_idxs, batch = next(dataloader)\n",
    "        X, Y = batch[:, :-1], batch[:, 1:, 0] # drop absolute position from Y\n",
    "        _, loss, xl_memories = model(file_idxs, X, xl_memories, Y)\n",
    "        losses[k] = loss.item()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'midi_transformer_knn_xl'\n",
    "model_load_path = Path(f'../data/model/{model_name}.tar')\n",
    "model_save_path = Path(f'../data/model/{model_name}.tar')\n",
    "tensorboard_writer = SummaryWriter(f\"../tensorboard/{model_name}\")\n",
    "\n",
    "average_log_losses = {\n",
    "    \"train\" : [],\n",
    "    \"val\" : []\n",
    "}\n",
    "\n",
    "def save_checkpoint(iter):\n",
    "    train_loss = estimate_loss(train_dataloader)\n",
    "    val_loss = estimate_loss(valid_dataloader)\n",
    "    tensorboard_writer.add_scalar('Loss/train', train_loss, iter)\n",
    "    tensorboard_writer.add_scalar('Loss/val', val_loss, iter)\n",
    "    train_log_loss = train_loss.log10().item()\n",
    "    val_log_loss = val_loss.log10().item()\n",
    "    average_log_losses['train'].append(train_log_loss)\n",
    "    average_log_losses['val'].append(val_log_loss)\n",
    "    print(f'Iteration {iter}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    torch.save({\n",
    "        'iter': iter,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'losses': average_log_losses,\n",
    "    }, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 100\n",
    "iterations = 100000\n",
    "start_iter = 0\n",
    "\n",
    "if model_load_path.exists():\n",
    "    checkpoint = torch.load(model_load_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    average_log_losses = checkpoint['losses']\n",
    "    iter = checkpoint['iter']\n",
    "    start_iter = iter + 1\n",
    "    print(f\"Loaded model from iteration {iter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 iterations\n",
      "Skipping memory clearing because it will be done immediately when evaluating\n",
      "Iteration 0\n",
      "Estimating loss\n",
      "Clearing memories\n",
      "Clearing knn memory 0\n",
      "Clearing knn memory 1\n",
      "Clearing knn memory 2\n",
      "Clearing knn memory 3\n",
      "Clearing knn memory 4\n",
      "Clearing knn memory 5\n",
      "Clearing knn memory 6\n",
      "Clearing knn memory 7\n",
      "Clearing knn memory 8\n",
      "Clearing knn memory 9\n",
      "Clearing knn memory 10\n",
      "Clearing knn memory 11\n",
      "Clearing knn memory 12\n",
      "Clearing knn memory 13\n",
      "Clearing knn memory 14\n",
      "Clearing knn memory 15\n",
      "Clearing knn memory 16\n",
      "Clearing knn memory 17\n",
      "Clearing knn memory 18\n",
      "Clearing knn memory 19\n",
      "Clearing knn memory 20\n",
      "Clearing knn memory 21\n",
      "Clearing knn memory 22\n",
      "Clearing knn memory 23\n",
      "Clearing knn memory 24\n",
      "Clearing knn memory 25\n",
      "Clearing knn memory 26\n",
      "Clearing knn memory 27\n",
      "Clearing knn memory 28\n",
      "Clearing knn memory 29\n",
      "Clearing knn memory 30\n",
      "Clearing knn memory 31\n",
      "Clearing knn memory 32\n",
      "Clearing knn memory 33\n",
      "Clearing knn memory 34\n",
      "Clearing knn memory 35\n",
      "Clearing knn memory 36\n",
      "Clearing knn memory 37\n",
      "Clearing knn memory 38\n",
      "Clearing knn memory 39\n",
      "Clearing knn memory 40\n",
      "Clearing knn memory 41\n",
      "Clearing knn memory 42\n",
      "Clearing knn memory 43\n",
      "Clearing knn memory 44\n",
      "Clearing knn memory 45\n",
      "Clearing knn memory 46\n",
      "Clearing knn memory 47\n",
      "Clearing knn memory 48\n",
      "Clearing knn memory 49\n",
      "Clearing knn memory 50\n",
      "Clearing knn memory 51\n",
      "Clearing knn memory 52\n",
      "Clearing knn memory 53\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "remaining_iters = iterations - start_iter\n",
    "if remaining_iters != -1:\n",
    "\n",
    "    # Start with no memory, from the first batch.\n",
    "    # Could load memories and offset batch index if we wanted to resume training with the same memories.\n",
    "\n",
    "    print(f\"Training for {remaining_iters} iterations\")\n",
    "    \n",
    "    xl_memories = None\n",
    "    if (remaining_iters + start_iter) % eval_interval != 0:\n",
    "        print(\"Clearing KNN memories\")\n",
    "        model.clear_knn_memory()\n",
    "        print(\"Memories cleared\")\n",
    "    else:\n",
    "        print(\"Skipping memory clearing because it will be done immediately when evaluating\")\n",
    "    \n",
    "\n",
    "    for iter in range(remaining_iters):\n",
    "\n",
    "        offset_iter = iter + start_iter\n",
    "\n",
    "        print(f\"Iteration {offset_iter}\")\n",
    "\n",
    "        if offset_iter % eval_interval == 0:\n",
    "            save_checkpoint(offset_iter)\n",
    "\n",
    "        # Configure minibatch\n",
    "        file_idxs, batch = next(train_dataloader)\n",
    "        \n",
    "        X, Y = batch[:, :-1], batch[:, 1:, 0]\n",
    "\n",
    "        # Forward pass\n",
    "        logits, loss, xl_memories = model(file_idxs, X, xl_memories, Y)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    save_checkpoint(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final training loss:', 10 ** average_log_losses['train'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final validation loss:', 10 ** average_log_losses['val'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(average_log_losses['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(average_log_losses['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_idx = torch.zeros((1,1,2), dtype=torch.long, device=device)\n",
    "generated_tokens = model.generate(init_idx, max_new_tokens=512).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens[0, :24, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = generated_tokens[0, :, 0]\n",
    "vocab.to_tokens(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_stream = idx_to_stream_enc(np.array(score), vocab)\n",
    "generated_stream.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_stream.show('midi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
