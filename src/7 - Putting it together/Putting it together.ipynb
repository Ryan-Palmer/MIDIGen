{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'7 - Putting it together')\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "import music21 as m21\n",
    "musescore_path = '/usr/bin/mscore'\n",
    "m21.environment.set('musicxmlPath', musescore_path)\n",
    "m21.environment.set('musescoreDirectPNGPath', musescore_path)\n",
    "from midi_encoding import *\n",
    "from data_loading import *\n",
    "from model import *\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct  5 13:02:05 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.51.01              Driver Version: 565.90         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0  On |                  Off |\n",
      "| 30%   31C    P0             45W /  450W |    2597MiB /  24564MiB |     19%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A        29      G   /Xwayland                                   N/A      |\n",
      "|    0   N/A  N/A        41      G   /Xwayland                                   N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178261"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'lakh_full_piano-vg_large'\n",
    "midi_path = Path(f'../data/midi/{dataset_name}')\n",
    "score_path = Path(f'../data/numpy/{dataset_name}')\n",
    "midi_file_paths = list(midi_path.rglob('*.mid')) #[f for f in os.listdir(midi_path) if os.path.isfile(os.path.join(midi_path, f))]\n",
    "\n",
    "# Ensure files are shuffled directly after assignment.\n",
    "# If they are shuffled in a different cell, and that cell is run multiple times, the order will change as we are shuffling the already-shuffled list.\n",
    "random.seed(42)\n",
    "random.shuffle(midi_file_paths)\n",
    "\n",
    "len(midi_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file names: 142608, Valid file names: 17826, Test file names: 17827\n"
     ]
    }
   ],
   "source": [
    "n1 = int(0.8 * len(midi_file_paths))\n",
    "n2 = int(0.9 * len(midi_file_paths))\n",
    "train_filepaths = midi_file_paths[:n1]\n",
    "valid_filepaths = midi_file_paths[n1:n2]\n",
    "test_filepaths = midi_file_paths[n2:]\n",
    "\n",
    "print(f'Train file names: {len(train_filepaths)}, Valid file names: {len(valid_filepaths)}, Test file names: {len(test_filepaths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 389\n",
    "vocab_name = f'{dataset_name}-actions_{max_vocab_size}'\n",
    "vocab_state_dict_path = Path(f'../data/vocab/{vocab_name}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MidiDataset says \"Look for the given filenames at the given score path. Load if they exist, if not create them'.\n",
    "\n",
    "We can use this to encode with the trained vocab if we pass it in as a param."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocab...\n",
      "Loaded vocab with actions:\n",
      "[]\n",
      "Trained vocab size: 389\n"
     ]
    }
   ],
   "source": [
    "sample_length = 256\n",
    "max_file_length = 32\n",
    "\n",
    "if not vocab_state_dict_path.exists():\n",
    "    \n",
    "    # Load (and / or create) dataset of unmerged samples\n",
    "    untrained_vocab = MusicVocab()\n",
    "    untrained_vocab_size = untrained_vocab.initial_size\n",
    "    untrained_vocab.train(None, untrained_vocab_size) # Required to init actions, not adding any tokens\n",
    "    print(f'Untrained vocab size: {untrained_vocab_size}')\n",
    "    vocab_training_dataset = MidiDataset(untrained_vocab, midi_file_paths, score_path, sample_length, max_file_length)\n",
    "    print(f\"Loading unmerged samples...\")\n",
    "    vocab_training_dataset.load_samples(\"cpu\")\n",
    "\n",
    "    # Train the vocab on the unmerged dataset, so it can learn the merges\n",
    "    print(f\"Training vocab, adding {max_vocab_size - untrained_vocab_size} tokens...\")\n",
    "    trained_vocab = MusicVocab()\n",
    "    trained_vocab.train(vocab_training_dataset, max_vocab_size=max_vocab_size)\n",
    "    print(f'Trained vocab size: {trained_vocab.size}')\n",
    "    print(f\"Saving vocab with actions:\\n{trained_vocab.actions}\")\n",
    "    trained_vocab.save(vocab_state_dict_path)\n",
    "else:\n",
    "    print(f\"Loading vocab...\")\n",
    "    trained_vocab = MusicVocab()\n",
    "    trained_vocab.load(vocab_state_dict_path)\n",
    "    print(f\"Loaded vocab with actions:\\n{trained_vocab.actions}\")\n",
    "    print(f'Trained vocab size: {trained_vocab.size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nested/__init__.py:166: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905979055/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  return _nested.nested_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading valid samples\n",
      "Encoded file is empty: ../data/numpy/lakh_full_piano-vg_large-actions_389/fa6efa8b02ca438879770f538d4a50c1.npy\n",
      "Loading test samples\n"
     ]
    }
   ],
   "source": [
    "merged_score_path = Path(f'../data/numpy/{vocab_name}')\n",
    "os.makedirs(merged_score_path, exist_ok=True)\n",
    "\n",
    "# Use the trained vocab to load GPU datasets, which will create merged samples if we pass a new path\n",
    "train_dataset = MidiDataset(trained_vocab, train_filepaths, merged_score_path, sample_length, max_file_length)\n",
    "valid_dataset = MidiDataset(trained_vocab, valid_filepaths, merged_score_path, sample_length, max_file_length)\n",
    "test_dataset = MidiDataset(trained_vocab, test_filepaths, merged_score_path, sample_length, max_file_length)\n",
    "\n",
    "print(f'Loading train samples')\n",
    "train_dataset.load_samples(False, device)\n",
    "\n",
    "print(f'Loading valid samples')\n",
    "valid_dataset.load_samples(False, device)\n",
    "\n",
    "print(f'Loading test samples')\n",
    "test_dataset.load_samples(False, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 73081, Valid files: 8978, Test files: 9082\n"
     ]
    }
   ],
   "source": [
    "print(f'Train files: {len(train_dataset.file_lengths)}, Valid files: {len(valid_dataset.file_lengths)}, Test files: {len(test_dataset.file_lengths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing indices\n"
     ]
    }
   ],
   "source": [
    "# Batch size can be changed for a second phase of training quite quickly, it only requires re-computing the indices, not re-loading the data.\n",
    "batch_size = 32\n",
    "train_sampler = ContiguousBatchSampler(train_dataset)\n",
    "valid_sampler = ContiguousBatchSampler(valid_dataset)\n",
    "test_sampler = ContiguousBatchSampler(test_dataset)\n",
    "\n",
    "print(f'Precomputing indices')\n",
    "train_sampler.precompute_indices(batch_size)\n",
    "valid_sampler.precompute_indices(batch_size)\n",
    "test_sampler.precompute_indices(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'midi_transformer_knn-xl_{vocab_name}-large-3'\n",
    "model_load_path = Path(f'../data/checkpoints/{model_name}.dat')\n",
    "model_save_path = Path(f'../data/checkpoints/{model_name}.dat')\n",
    "log_dir = Path(f'../tensorboard/{model_name}')\n",
    "tensorboard_writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26147725\n"
     ]
    }
   ],
   "source": [
    "model = DecoderTransformer_KNN_XL(vocab=trained_vocab, sample_length=sample_length, max_file_length=max_file_length, device=device, use_knn=True, n_embed=512, n_head=8, n_layer=8)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "token_embedding.weight \t torch.Size([389, 512])\n",
      "rel_pos.relative_attention_embedding.weight \t torch.Size([32, 8])\n",
      "rel_pos_knn.relative_attention_embedding.weight \t torch.Size([32, 8])\n",
      "beat_embedding.weight \t torch.Size([32, 512])\n",
      "bar_embedding.weight \t torch.Size([1024, 512])\n",
      "blocks.0.attention.key.weight \t torch.Size([512, 512])\n",
      "blocks.0.attention.query.weight \t torch.Size([512, 512])\n",
      "blocks.0.attention.value.weight \t torch.Size([512, 512])\n",
      "blocks.0.attention.project.weight \t torch.Size([512, 512])\n",
      "blocks.0.attention.project.bias \t torch.Size([512])\n",
      "blocks.0.ff.net.0.weight \t torch.Size([2048, 512])\n",
      "blocks.0.ff.net.0.bias \t torch.Size([2048])\n",
      "blocks.0.ff.net.3.weight \t torch.Size([512, 2048])\n",
      "blocks.0.ff.net.3.bias \t torch.Size([512])\n",
      "blocks.0.layer_norm1.weight \t torch.Size([512])\n",
      "blocks.0.layer_norm1.bias \t torch.Size([512])\n",
      "blocks.0.layer_norm2.weight \t torch.Size([512])\n",
      "blocks.0.layer_norm2.bias \t torch.Size([512])\n",
      "blocks.1.attention.key.weight \t torch.Size([512, 512])\n",
      "blocks.1.attention.query.weight \t torch.Size([512, 512])\n",
      "blocks.1.attention.value.weight \t torch.Size([512, 512])\n",
      "blocks.1.attention.project.weight \t torch.Size([512, 512])\n",
      "blocks.1.attention.project.bias \t torch.Size([512])\n",
      "blocks.1.ff.net.0.weight \t torch.Size([2048, 512])\n",
      "blocks.1.ff.net.0.bias \t torch.Size([2048])\n",
      "blocks.1.ff.net.3.weight \t torch.Size([512, 2048])\n",
      "blocks.1.ff.net.3.bias \t torch.Size([512])\n",
      "blocks.1.layer_norm1.weight \t torch.Size([512])\n",
      "blocks.1.layer_norm1.bias \t torch.Size([512])\n",
      "blocks.1.layer_norm2.weight \t torch.Size([512])\n",
      "blocks.1.layer_norm2.bias \t torch.Size([512])\n",
      "blocks.2.attention.key.weight \t torch.Size([512, 512])\n",
      "blocks.2.attention.query.weight \t torch.Size([512, 512])\n",
      "blocks.2.attention.value.weight \t torch.Size([512, 512])\n",
      "blocks.2.attention.project.weight \t torch.Size([512, 512])\n",
      "blocks.2.attention.project.bias \t torch.Size([512])\n",
      "blocks.2.ff.net.0.weight \t torch.Size([2048, 512])\n",
      "blocks.2.ff.net.0.bias \t torch.Size([2048])\n",
      "blocks.2.ff.net.3.weight \t torch.Size([512, 2048])\n",
      "blocks.2.ff.net.3.bias \t torch.Size([512])\n",
      "blocks.2.layer_norm1.weight \t torch.Size([512])\n",
      "blocks.2.layer_norm1.bias \t torch.Size([512])\n",
      "blocks.2.layer_norm2.weight \t torch.Size([512])\n",
      "blocks.2.layer_norm2.bias \t torch.Size([512])\n",
      "blocks.3.attention.key.weight \t torch.Size([512, 512])\n",
      "blocks.3.attention.query.weight \t torch.Size([512, 512])\n",
      "blocks.3.attention.value.weight \t torch.Size([512, 512])\n",
      "blocks.3.attention.project.weight \t torch.Size([512, 512])\n",
      "blocks.3.attention.project.bias \t torch.Size([512])\n",
      "blocks.3.ff.net.0.weight \t torch.Size([2048, 512])\n",
      "blocks.3.ff.net.0.bias \t torch.Size([2048])\n",
      "blocks.3.ff.net.3.weight \t torch.Size([512, 2048])\n",
      "blocks.3.ff.net.3.bias \t torch.Size([512])\n",
      "blocks.3.layer_norm1.weight \t torch.Size([512])\n",
      "blocks.3.layer_norm1.bias \t torch.Size([512])\n",
      "blocks.3.layer_norm2.weight \t torch.Size([512])\n",
      "blocks.3.layer_norm2.bias \t torch.Size([512])\n",
      "blocks.4.attention.gate_bias \t torch.Size([8, 1, 1])\n",
      "blocks.4.attention.key.weight \t torch.Size([512, 512])\n",
      "blocks.4.attention.query.weight \t torch.Size([512, 512])\n",
      "blocks.4.attention.value.weight \t torch.Size([512, 512])\n",
      "blocks.4.attention.project.weight \t torch.Size([512, 512])\n",
      "blocks.4.attention.project.bias \t torch.Size([512])\n",
      "blocks.4.ff.net.0.weight \t torch.Size([2048, 512])\n",
      "blocks.4.ff.net.0.bias \t torch.Size([2048])\n",
      "blocks.4.ff.net.3.weight \t torch.Size([512, 2048])\n",
      "blocks.4.ff.net.3.bias \t torch.Size([512])\n",
      "blocks.4.layer_norm1.weight \t torch.Size([512])\n",
      "blocks.4.layer_norm1.bias \t torch.Size([512])\n",
      "blocks.4.layer_norm2.weight \t torch.Size([512])\n",
      "blocks.4.layer_norm2.bias \t torch.Size([512])\n",
      "blocks.5.attention.key.weight \t torch.Size([512, 512])\n",
      "blocks.5.attention.query.weight \t torch.Size([512, 512])\n",
      "blocks.5.attention.value.weight \t torch.Size([512, 512])\n",
      "blocks.5.attention.project.weight \t torch.Size([512, 512])\n",
      "blocks.5.attention.project.bias \t torch.Size([512])\n",
      "blocks.5.ff.net.0.weight \t torch.Size([2048, 512])\n",
      "blocks.5.ff.net.0.bias \t torch.Size([2048])\n",
      "blocks.5.ff.net.3.weight \t torch.Size([512, 2048])\n",
      "blocks.5.ff.net.3.bias \t torch.Size([512])\n",
      "blocks.5.layer_norm1.weight \t torch.Size([512])\n",
      "blocks.5.layer_norm1.bias \t torch.Size([512])\n",
      "blocks.5.layer_norm2.weight \t torch.Size([512])\n",
      "blocks.5.layer_norm2.bias \t torch.Size([512])\n",
      "blocks.6.attention.key.weight \t torch.Size([512, 512])\n",
      "blocks.6.attention.query.weight \t torch.Size([512, 512])\n",
      "blocks.6.attention.value.weight \t torch.Size([512, 512])\n",
      "blocks.6.attention.project.weight \t torch.Size([512, 512])\n",
      "blocks.6.attention.project.bias \t torch.Size([512])\n",
      "blocks.6.ff.net.0.weight \t torch.Size([2048, 512])\n",
      "blocks.6.ff.net.0.bias \t torch.Size([2048])\n",
      "blocks.6.ff.net.3.weight \t torch.Size([512, 2048])\n",
      "blocks.6.ff.net.3.bias \t torch.Size([512])\n",
      "blocks.6.layer_norm1.weight \t torch.Size([512])\n",
      "blocks.6.layer_norm1.bias \t torch.Size([512])\n",
      "blocks.6.layer_norm2.weight \t torch.Size([512])\n",
      "blocks.6.layer_norm2.bias \t torch.Size([512])\n",
      "blocks.7.attention.key.weight \t torch.Size([512, 512])\n",
      "blocks.7.attention.query.weight \t torch.Size([512, 512])\n",
      "blocks.7.attention.value.weight \t torch.Size([512, 512])\n",
      "blocks.7.attention.project.weight \t torch.Size([512, 512])\n",
      "blocks.7.attention.project.bias \t torch.Size([512])\n",
      "blocks.7.ff.net.0.weight \t torch.Size([2048, 512])\n",
      "blocks.7.ff.net.0.bias \t torch.Size([2048])\n",
      "blocks.7.ff.net.3.weight \t torch.Size([512, 2048])\n",
      "blocks.7.ff.net.3.bias \t torch.Size([512])\n",
      "blocks.7.layer_norm1.weight \t torch.Size([512])\n",
      "blocks.7.layer_norm1.bias \t torch.Size([512])\n",
      "blocks.7.layer_norm2.weight \t torch.Size([512])\n",
      "blocks.7.layer_norm2.bias \t torch.Size([512])\n",
      "layer_norm.weight \t torch.Size([512])\n",
      "layer_norm.bias \t torch.Size([512])\n",
      "lm_head.weight \t torch.Size([389, 512])\n",
      "lm_head.bias \t torch.Size([389])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-4\n",
    "weight_decay = 1e-3\n",
    "eval_iters = 100\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    xl_memories = None\n",
    "    losses = torch.zeros(eval_iters)\n",
    "\n",
    "    for k in range(eval_iters):\n",
    "        file_idxs, batch = next(data_loader)\n",
    "        X, Y = batch[:, :-1], batch[:, 1:, 0] # drop absolute position from Y\n",
    "        _, loss, xl_memories = model(file_idxs, X, xl_memories, Y)\n",
    "        losses[k] = loss.item()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_log_losses = {  \n",
    "    \"train\" : [],\n",
    "    \"val\" : []\n",
    "}\n",
    "\n",
    "epochs = 0\n",
    "\n",
    "def save_checkpoint(iterations):\n",
    "    train_loss = estimate_loss(cycle(DataLoader(train_dataset, batch_sampler=train_sampler)))\n",
    "    val_loss = estimate_loss(cycle(DataLoader(valid_dataset, batch_sampler=valid_sampler)))\n",
    "    tensorboard_writer.add_scalar('Loss/train', train_loss, iterations)\n",
    "    tensorboard_writer.add_scalar('Loss/val', val_loss, iterations)\n",
    "    train_log_loss = train_loss.log10().item()\n",
    "    val_log_loss = val_loss.log10().item()\n",
    "    average_log_losses['train'].append(train_log_loss)\n",
    "    average_log_losses['val'].append(val_log_loss)\n",
    "    print(f'Epoch {epochs} / Iteration {iterations}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    torch.save({\n",
    "        'iter': iterations,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'vocab_state_dict': trained_vocab.state_dict(),\n",
    "        'losses': average_log_losses,\n",
    "        'epochs': epochs\n",
    "    }, model_save_path)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from iteration 16200\n"
     ]
    }
   ],
   "source": [
    "eval_interval = 200\n",
    "total_iterations = 300000\n",
    "start_iter = 0\n",
    "\n",
    "if model_load_path.exists():\n",
    "    checkpoint = torch.load(model_load_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    average_log_losses = checkpoint['losses']\n",
    "    iterations = checkpoint['iter']\n",
    "    epochs = checkpoint['epochs']\n",
    "    start_iter = iterations + 1\n",
    "    print(f\"Loaded model from iteration {iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training from epoch 1 for 283799 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  x.storage().data_ptr() + x.storage_offset() * 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 / Iteration 16400: Train Loss: 1.4773, Val Loss: 1.5206\n",
      "Epoch 2 / Iteration 16600: Train Loss: 1.4811, Val Loss: 1.5157\n",
      "Epoch 2 / Iteration 16800: Train Loss: 1.4761, Val Loss: 1.5072\n",
      "Epoch 2 / Iteration 17000: Train Loss: 1.4765, Val Loss: 1.5056\n",
      "Epoch 2 / Iteration 17200: Train Loss: 1.4763, Val Loss: 1.5024\n",
      "Epoch 2 / Iteration 17400: Train Loss: 1.4706, Val Loss: 1.5035\n",
      "Epoch 2 / Iteration 17600: Train Loss: 1.4825, Val Loss: 1.5092\n",
      "Epoch 2 / Iteration 17800: Train Loss: 1.4785, Val Loss: 1.5052\n",
      "Epoch 2 / Iteration 18000: Train Loss: 1.4720, Val Loss: 1.5017\n",
      "Epoch 2 / Iteration 18200: Train Loss: 1.4681, Val Loss: 1.4995\n",
      "Epoch 2 / Iteration 18400: Train Loss: 1.4688, Val Loss: 1.4998\n",
      "Epoch 2 / Iteration 18600: Train Loss: 1.4565, Val Loss: 1.4830\n",
      "Epoch 2 / Iteration 18800: Train Loss: 1.4682, Val Loss: 1.4985\n",
      "Epoch 2 / Iteration 19000: Train Loss: 1.4727, Val Loss: 1.4998\n",
      "Epoch 2 / Iteration 19200: Train Loss: 1.4552, Val Loss: 1.4875\n",
      "Epoch 2 / Iteration 19400: Train Loss: 1.4670, Val Loss: 1.4946\n",
      "Epoch 2 / Iteration 19600: Train Loss: 1.4600, Val Loss: 1.4865\n",
      "Epoch 2 / Iteration 19800: Train Loss: 1.4628, Val Loss: 1.4901\n",
      "Epoch 2 / Iteration 20000: Train Loss: 1.4523, Val Loss: 1.4797\n",
      "Epoch 2 / Iteration 20200: Train Loss: 1.4555, Val Loss: 1.4822\n",
      "Epoch 2 / Iteration 20400: Train Loss: 1.4586, Val Loss: 1.4934\n",
      "Epoch 2 / Iteration 20600: Train Loss: 1.4663, Val Loss: 1.4976\n",
      "Epoch 2 / Iteration 20800: Train Loss: 1.4466, Val Loss: 1.4779\n",
      "Epoch 2 / Iteration 21000: Train Loss: 1.4576, Val Loss: 1.4849\n",
      "Epoch 2 / Iteration 21200: Train Loss: 1.4429, Val Loss: 1.4745\n",
      "Epoch 2 / Iteration 21400: Train Loss: 1.4459, Val Loss: 1.4763\n",
      "Epoch 2 / Iteration 21600: Train Loss: 1.4537, Val Loss: 1.4854\n",
      "Epoch 2 / Iteration 21800: Train Loss: 1.4401, Val Loss: 1.4707\n",
      "Epoch 2 / Iteration 22000: Train Loss: 1.4505, Val Loss: 1.4779\n",
      "Epoch 2 / Iteration 22200: Train Loss: 1.4461, Val Loss: 1.4758\n",
      "Epoch 2 / Iteration 22400: Train Loss: 1.4474, Val Loss: 1.4748\n",
      "Epoch 2 / Iteration 22600: Train Loss: 1.4545, Val Loss: 1.4818\n",
      "Epoch 2 / Iteration 22800: Train Loss: 1.4425, Val Loss: 1.4742\n",
      "Epoch 2 / Iteration 23000: Train Loss: 1.4467, Val Loss: 1.4803\n",
      "Epoch 2 / Iteration 23200: Train Loss: 1.4379, Val Loss: 1.4652\n",
      "Epoch 2 / Iteration 23400: Train Loss: 1.4407, Val Loss: 1.4708\n",
      "Epoch 2 / Iteration 23600: Train Loss: 1.4299, Val Loss: 1.4591\n",
      "Epoch 2 / Iteration 23800: Train Loss: 1.4464, Val Loss: 1.4743\n",
      "Epoch 2 / Iteration 24000: Train Loss: 1.4413, Val Loss: 1.4760\n",
      "Epoch 2 / Iteration 24200: Train Loss: 1.4478, Val Loss: 1.4826\n",
      "Epoch 2 / Iteration 24400: Train Loss: 1.4302, Val Loss: 1.4645\n",
      "Epoch 2 / Iteration 24600: Train Loss: 1.4346, Val Loss: 1.4666\n",
      "Epoch 2 / Iteration 24800: Train Loss: 1.4372, Val Loss: 1.4654\n",
      "Epoch 2 / Iteration 25000: Train Loss: 1.4469, Val Loss: 1.4750\n",
      "Epoch 2 / Iteration 25200: Train Loss: 1.4459, Val Loss: 1.4760\n",
      "Epoch 2 / Iteration 25400: Train Loss: 1.4352, Val Loss: 1.4672\n",
      "Epoch 2 / Iteration 25600: Train Loss: 1.4301, Val Loss: 1.4633\n",
      "Epoch 2 / Iteration 25800: Train Loss: 1.4434, Val Loss: 1.4741\n",
      "Epoch 2 / Iteration 26000: Train Loss: 1.4360, Val Loss: 1.4673\n",
      "Epoch 2 / Iteration 26200: Train Loss: 1.4350, Val Loss: 1.4665\n",
      "Epoch 2 / Iteration 26400: Train Loss: 1.4317, Val Loss: 1.4648\n",
      "Epoch 2 / Iteration 26600: Train Loss: 1.4282, Val Loss: 1.4642\n",
      "Epoch 2 / Iteration 26800: Train Loss: 1.4267, Val Loss: 1.4594\n",
      "Epoch 2 / Iteration 27000: Train Loss: 1.4373, Val Loss: 1.4682\n",
      "Epoch 2 / Iteration 27200: Train Loss: 1.4250, Val Loss: 1.4564\n",
      "Epoch 2 / Iteration 27400: Train Loss: 1.4140, Val Loss: 1.4511\n",
      "Epoch 2 / Iteration 27600: Train Loss: 1.4180, Val Loss: 1.4590\n",
      "Epoch 2 / Iteration 27800: Train Loss: 1.4304, Val Loss: 1.4627\n",
      "Epoch 2 / Iteration 28000: Train Loss: 1.4146, Val Loss: 1.4473\n",
      "Epoch 2 / Iteration 28200: Train Loss: 1.4190, Val Loss: 1.4564\n",
      "Epoch 2 / Iteration 28400: Train Loss: 1.4133, Val Loss: 1.4477\n",
      "Epoch 2 / Iteration 28600: Train Loss: 1.4224, Val Loss: 1.4542\n",
      "Epoch 2 / Iteration 28800: Train Loss: 1.4175, Val Loss: 1.4528\n",
      "Epoch 2 / Iteration 29000: Train Loss: 1.4090, Val Loss: 1.4411\n",
      "Epoch 2 / Iteration 29200: Train Loss: 1.4118, Val Loss: 1.4439\n",
      "Epoch 2 / Iteration 29400: Train Loss: 1.4176, Val Loss: 1.4569\n",
      "Epoch 2 / Iteration 29600: Train Loss: 1.4146, Val Loss: 1.4483\n",
      "Epoch 2 / Iteration 29800: Train Loss: 1.4247, Val Loss: 1.4583\n",
      "Epoch 2 / Iteration 30000: Train Loss: 1.4200, Val Loss: 1.4535\n",
      "Epoch 2 / Iteration 30200: Train Loss: 1.4116, Val Loss: 1.4415\n",
      "Epoch 2 / Iteration 30400: Train Loss: 1.4095, Val Loss: 1.4463\n",
      "Epoch 2 / Iteration 30600: Train Loss: 1.4162, Val Loss: 1.4501\n",
      "Epoch 2 / Iteration 30800: Train Loss: 1.4125, Val Loss: 1.4478\n",
      "Epoch 2 / Iteration 31000: Train Loss: 1.4109, Val Loss: 1.4475\n",
      "Epoch 2 / Iteration 31200: Train Loss: 1.4147, Val Loss: 1.4509\n",
      "Epoch 2 / Iteration 31400: Train Loss: 1.4092, Val Loss: 1.4483\n",
      "Epoch 2 / Iteration 31600: Train Loss: 1.4213, Val Loss: 1.4561\n",
      "Epoch 2 / Iteration 31800: Train Loss: 1.4078, Val Loss: 1.4451\n",
      "Epoch 2 / Iteration 32000: Train Loss: 1.4142, Val Loss: 1.4512\n",
      "Epoch 2 / Iteration 32200: Train Loss: 1.4104, Val Loss: 1.4439\n",
      "Epoch 2 / Iteration 32400: Train Loss: 1.4035, Val Loss: 1.4381\n",
      "Epoch 2 / Iteration 32600: Train Loss: 1.4210, Val Loss: 1.4535\n",
      "Epoch 2 / Iteration 32800: Train Loss: 1.4069, Val Loss: 1.4420\n",
      "Epoch 2 / Iteration 33000: Train Loss: 1.4022, Val Loss: 1.4358\n",
      "Epoch 2 / Iteration 33200: Train Loss: 1.4108, Val Loss: 1.4384\n",
      "Epoch 2 / Iteration 33400: Train Loss: 1.4119, Val Loss: 1.4497\n",
      "Epoch 2 / Iteration 33600: Train Loss: 1.4119, Val Loss: 1.4400\n",
      "Epoch 2 / Iteration 33800: Train Loss: 1.4124, Val Loss: 1.4411\n",
      "Epoch 2 / Iteration 34000: Train Loss: 1.4122, Val Loss: 1.4454\n",
      "Epoch 2 / Iteration 34200: Train Loss: 1.4144, Val Loss: 1.4497\n",
      "Epoch 2 / Iteration 34400: Train Loss: 1.4128, Val Loss: 1.4480\n",
      "Epoch 2 / Iteration 34600: Train Loss: 1.4112, Val Loss: 1.4447\n",
      "Epoch 2 / Iteration 34800: Train Loss: 1.4207, Val Loss: 1.4567\n",
      "Epoch 2 / Iteration 35000: Train Loss: 1.4098, Val Loss: 1.4436\n",
      "Epoch 2 / Iteration 35200: Train Loss: 1.3981, Val Loss: 1.4333\n",
      "Epoch 2 / Iteration 35400: Train Loss: 1.4057, Val Loss: 1.4351\n",
      "Epoch 2 / Iteration 35600: Train Loss: 1.4016, Val Loss: 1.4344\n",
      "Epoch 2 / Iteration 35800: Train Loss: 1.4082, Val Loss: 1.4429\n",
      "Epoch 2 / Iteration 36000: Train Loss: 1.4063, Val Loss: 1.4413\n",
      "Epoch 2 / Iteration 36200: Train Loss: 1.3954, Val Loss: 1.4291\n",
      "Epoch 2 / Iteration 36400: Train Loss: 1.4095, Val Loss: 1.4465\n",
      "Epoch 2 / Iteration 36600: Train Loss: 1.4096, Val Loss: 1.4384\n",
      "Epoch 2 / Iteration 36800: Train Loss: 1.4096, Val Loss: 1.4423\n",
      "Epoch 2 / Iteration 37000: Train Loss: 1.4056, Val Loss: 1.4363\n",
      "Epoch 2 / Iteration 37200: Train Loss: 1.3974, Val Loss: 1.4271\n",
      "Epoch 2 / Iteration 37400: Train Loss: 1.4090, Val Loss: 1.4421\n",
      "Epoch 2 / Iteration 37600: Train Loss: 1.4061, Val Loss: 1.4366\n",
      "Epoch 2 / Iteration 37800: Train Loss: 1.4007, Val Loss: 1.4322\n",
      "Epoch 2 / Iteration 38000: Train Loss: 1.3968, Val Loss: 1.4253\n",
      "Epoch 2 / Iteration 38200: Train Loss: 1.4049, Val Loss: 1.4339\n",
      "Epoch 2 / Iteration 38400: Train Loss: 1.3955, Val Loss: 1.4285\n",
      "Epoch 2 / Iteration 38600: Train Loss: 1.4104, Val Loss: 1.4407\n",
      "Epoch 2 / Iteration 38800: Train Loss: 1.4038, Val Loss: 1.4317\n",
      "Epoch 2 / Iteration 39000: Train Loss: 1.4035, Val Loss: 1.4346\n",
      "Epoch 2 / Iteration 39200: Train Loss: 1.3993, Val Loss: 1.4294\n",
      "Epoch 2 / Iteration 39400: Train Loss: 1.4031, Val Loss: 1.4395\n",
      "Epoch 2 / Iteration 39600: Train Loss: 1.4026, Val Loss: 1.4316\n",
      "Epoch 2 / Iteration 39800: Train Loss: 1.4006, Val Loss: 1.4343\n",
      "Epoch 2 / Iteration 40000: Train Loss: 1.3957, Val Loss: 1.4293\n",
      "Epoch 2 / Iteration 40200: Train Loss: 1.3905, Val Loss: 1.4286\n",
      "Epoch 2 / Iteration 40400: Train Loss: 1.3998, Val Loss: 1.4315\n",
      "Epoch 2 / Iteration 40600: Train Loss: 1.3955, Val Loss: 1.4295\n",
      "Epoch 2 / Iteration 40800: Train Loss: 1.3969, Val Loss: 1.4329\n",
      "Epoch 2 / Iteration 41000: Train Loss: 1.4016, Val Loss: 1.4348\n",
      "Epoch 2 / Iteration 41200: Train Loss: 1.3999, Val Loss: 1.4359\n",
      "Epoch 2 / Iteration 41400: Train Loss: 1.4021, Val Loss: 1.4363\n",
      "Epoch 2 / Iteration 41600: Train Loss: 1.3995, Val Loss: 1.4302\n",
      "Epoch 2 / Iteration 41800: Train Loss: 1.3942, Val Loss: 1.4250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m batch[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], batch[:, \u001b[38;5;241m1\u001b[39m:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m logits, loss, xl_memories \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_idxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxl_memories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/src/7 - Putting it together/model.py:482\u001b[0m, in \u001b[0;36mDecoderTransformer_KNN_XL.forward\u001b[0;34m(self, batch_file_idxs, x, xl_memories, targets)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m current_file_idx \u001b[38;5;241m!=\u001b[39m batch_file_idxs[batch_dim]:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;66;03m# print(f\"Clearing XL mem for batch dim {batch_dim}\")\u001b[39;00m\n\u001b[1;32m    481\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layer):\n\u001b[0;32m--> 482\u001b[0m                 xl_memories[layer][batch_dim] \u001b[38;5;241m=\u001b[39m \u001b[43mempty_mem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_file_idxs \u001b[38;5;241m=\u001b[39m batch_file_idxs\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# Store the XL memories for each pass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "remaining_iters = total_iterations - start_iter\n",
    "if remaining_iters != -1:\n",
    "    print(f\"Training from epoch {epochs} for {remaining_iters} iterations\")\n",
    "\n",
    "    train_data_loader = cycle(DataLoader(train_dataset, batch_sampler=train_sampler))\n",
    "    xl_memories = None\n",
    "    initial_file_idxs = None\n",
    "    offset_iter = start_iter\n",
    "\n",
    "    for iteration in range(remaining_iters):\n",
    "        offset_iter = iteration + start_iter\n",
    "\n",
    "        if offset_iter % eval_interval == 0:\n",
    "            # Because we don't explicitly clear xl and knn mem here, there is always a risk that the eval loop leaves the file idx\n",
    "            # the same as the train loop, but with memories of the 'future' which aren't cleared. It could also break the epoch counter.\n",
    "            # The risk would be much smaller with a bigger data set, but with vg_large we loop through the data quite quickly.\n",
    "            save_checkpoint(offset_iter)\n",
    "\n",
    "        # Configure minibatch\n",
    "        file_idxs, batch = next(train_data_loader)\n",
    "\n",
    "        if initial_file_idxs is None:\n",
    "            initial_file_idxs = file_idxs\n",
    "\n",
    "        if torch.equal(initial_file_idxs, file_idxs):\n",
    "            epochs += 1\n",
    "        \n",
    "        X, Y = batch[:, :-1], batch[:, 1:, 0]\n",
    "\n",
    "        # Forward pass\n",
    "        logits, loss, xl_memories = model(file_idxs, X, xl_memories, Y)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    save_checkpoint(offset_iter + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final training loss:', 10 ** average_log_losses['train'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final validation loss:', 10 ** average_log_losses['val'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(average_log_losses['train'][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(average_log_losses['val'][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_idx = torch.zeros((1,1,2), dtype=torch.long, device=device)\n",
    "generated_tokens = model.generate(init_idx, max_new_tokens=512).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_vocab.itos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = generated_tokens[0, :, 0]\n",
    "trained_vocab.to_tokens(score.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_stream = idx_to_stream_enc(np.array(score), trained_vocab)\n",
    "generated_stream.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_stream.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_path = random.choice(test_filepaths)\n",
    "print(random_test_path)\n",
    "random_test_idx_score = midifile_to_idx_score(random_test_path, trained_vocab)\n",
    "random_test_intro = random_test_idx_score[:sample_length]\n",
    "random_test_intro_stream = idx_to_stream_enc(np.array(random_test_intro[:, 0]), trained_vocab)\n",
    "random_test_intro_stream.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_intro_stream.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_init = torch.tensor(random_test_intro, device=device).unsqueeze(0)\n",
    "random_test_continued = model.generate(random_test_init, max_new_tokens=512).cpu()[0, :, 0]\n",
    "random_test_continued_stream = idx_to_stream_enc(np.array(random_test_continued), trained_vocab)\n",
    "random_test_continued_stream.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_continued_stream.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_timestep(data, include_position=False):\n",
    "    grouped = {}\n",
    "    for idx, position in data:\n",
    "        if position not in grouped:\n",
    "            grouped[position] = []\n",
    "        grouped[position].append(idx)\n",
    "    \n",
    "    if include_position:\n",
    "        result = [(tuple(values), position) for position, values in grouped.items()]\n",
    "    else:\n",
    "        result = [tuple(values) for values in grouped.values()]\n",
    "    return result\n",
    "\n",
    "data = [[1, 0], [2, 0], [3, 0], [4, 1], [5, 1], [6, 2], [7, 2], [8, 2], [9, 2]]\n",
    "\n",
    "grouped_data = group_by_timestep(data, include_position=False)\n",
    "print(grouped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = group_by_timestep(data, include_position=True)\n",
    "print(grouped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = {}\n",
    "grouped_idxs = [(1, 2, 3), (4, 5), (6, 7, 8, 9), (1, 2, 3), (1, 2, 3), (6, 7, 8, 9), (4, 5), (4, 5), (4, 5)]\n",
    "for action in grouped_idxs:\n",
    "    actions[action] = actions.get(action, 0) + 1\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_actions = {k: v for k, v in sorted(actions.items(), key=lambda item: item[1], reverse=True)}\n",
    "sorted_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_actions = list(sorted_actions.keys())[:2]\n",
    "top_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_size = 10\n",
    "\n",
    "def try_replace(action, position):\n",
    "    if action in top_actions:\n",
    "        return [[initial_size + top_actions.index(action)], position]\n",
    "    else:\n",
    "        return [list(action), position]\n",
    "        \n",
    "replaced_score = [try_replace(action, position) for action, position in grouped_data]\n",
    "replaced_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_idx_score = []\n",
    "pos_score = []\n",
    "for (action, position) in replaced_score:\n",
    "    for index in action:\n",
    "        note_idx_score.append(index)\n",
    "        pos_score.append(position)\n",
    "note_idx_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, action in enumerate(top_actions):\n",
    "   print(' '.join([str(a) for a in action]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
