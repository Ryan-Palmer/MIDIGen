{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'7 - Putting it together')\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "import music21 as m21\n",
    "musescore_path = '/usr/bin/mscore'\n",
    "m21.environment.set('musicxmlPath', musescore_path)\n",
    "m21.environment.set('musescoreDirectPNGPath', musescore_path)\n",
    "from midi_encoding import *\n",
    "from data_loading import *\n",
    "from model import *\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 21 15:22:23 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0  On |                  Off |\n",
      "| 30%   32C    P0             48W /  450W |    2133MiB /  24564MiB |     33%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A        29      G   /Xwayland                                   N/A      |\n",
      "|    0   N/A  N/A        34      G   /Xwayland                                   N/A      |\n",
      "|    0   N/A  N/A        36      G   /Xwayland                                   N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16977"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'lakh_clean_piano_128'\n",
    "midi_path = Path(f'../data/midi/{dataset_name}')\n",
    "score_path = Path(f'../data/numpy/{dataset_name}')\n",
    "midi_file_paths = list(midi_path.rglob('*.mid')) #[f for f in os.listdir(midi_path) if os.path.isfile(os.path.join(midi_path, f))]\n",
    "\n",
    "# Ensure files are shuffled directly after assignment.\n",
    "# If they are shuffled in a different cell, and that cell is run multiple times, the order will change as we are shuffling the already-shuffled list.\n",
    "random.seed(42)\n",
    "random.shuffle(midi_file_paths)\n",
    "\n",
    "len(midi_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file names: 13581, Valid file names: 1698, Test file names: 1698\n"
     ]
    }
   ],
   "source": [
    "n1 = int(0.8 * len(midi_file_paths))\n",
    "n2 = int(0.9 * len(midi_file_paths))\n",
    "train_filepaths = midi_file_paths[:n1]\n",
    "valid_filepaths = midi_file_paths[n1:n2]\n",
    "test_filepaths = midi_file_paths[n2:]\n",
    "\n",
    "print(f'Train file names: {len(train_filepaths)}, Valid file names: {len(valid_filepaths)}, Test file names: {len(test_filepaths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 389\n",
    "vocab_name = f'{dataset_name}-actions_{max_vocab_size}'\n",
    "vocab_state_dict_path = Path(f'../data/vocab/{vocab_name}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MidiDataset says \"Look for the given filenames at the given score path. Load if they exist, if not create them'.\n",
    "\n",
    "We can use this to encode with the trained vocab if we pass it in as a param."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained vocab size: 1157\n",
      "Loading unmerged samples...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: \"../data/numpy/lakh_clean_piano_128/['Gee']npy\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m vocab_training_dataset \u001b[38;5;241m=\u001b[39m MidiDataset(untrained_vocab, midi_file_paths, score_path, sample_length, max_file_length)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading unmerged samples...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mvocab_training_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Train the vocab on the unmerged dataset, so it can learn the merges\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining vocab, adding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_vocab_size\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39muntrained_vocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/src/7 - Putting it together/data_loading.py:47\u001b[0m, in \u001b[0;36mMidiDataset.load_samples\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m     45\u001b[0m file_name \u001b[38;5;241m=\u001b[39m midi_file_path\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m     46\u001b[0m encoded_file_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[file_name[:\u001b[38;5;241m3\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mnpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m idx_score \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m samples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Split idx_score into blocks of size sample_length, padding the last blocks if necessary\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: \"../data/numpy/lakh_clean_piano_128/['Gee']npy\""
     ]
    }
   ],
   "source": [
    "sample_length = 256\n",
    "max_file_length = 32\n",
    "\n",
    "if not vocab_state_dict_path.exists():\n",
    "    \n",
    "    # Load (and / or create) dataset of unmerged samples\n",
    "    untrained_vocab = MusicVocab()\n",
    "    untrained_vocab_size = untrained_vocab.initial_size\n",
    "    print(f'Untrained vocab size: {untrained_vocab_size}')\n",
    "    vocab_training_dataset = MidiDataset(untrained_vocab, midi_file_paths, score_path, sample_length, max_file_length)\n",
    "    print(f\"Loading unmerged samples...\")\n",
    "    vocab_training_dataset.load_samples(\"cpu\")\n",
    "\n",
    "    # Train the vocab on the unmerged dataset, so it can learn the merges\n",
    "    print(f\"Training vocab, adding {max_vocab_size - untrained_vocab_size} tokens...\")\n",
    "    trained_vocab = MusicVocab()\n",
    "    trained_vocab.train(vocab_training_dataset, max_vocab_size=max_vocab_size)\n",
    "    print(f'Trained vocab size: {trained_vocab.size}')\n",
    "    print(f\"Saving vocab with actions:\\n{trained_vocab.actions}\")\n",
    "    trained_vocab.save(vocab_state_dict_path)\n",
    "else:\n",
    "    print(f\"Loading vocab...\")\n",
    "    trained_vocab = MusicVocab()\n",
    "    trained_vocab.load(vocab_state_dict_path)\n",
    "    print(f\"Loaded vocab with actions:\\n{trained_vocab.actions}\")\n",
    "    print(f'Trained vocab size: {trained_vocab.size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_score_path = Path(f'../data/numpy/{vocab_name}')\n",
    "os.makedirs(merged_score_path, exist_ok=True)\n",
    "\n",
    "# Use the trained vocab to load GPU datasets, which will create merged samples if we pass a new path\n",
    "train_dataset = MidiDataset(trained_vocab, train_filepaths, merged_score_path, sample_length, max_file_length)\n",
    "valid_dataset = MidiDataset(trained_vocab, valid_filepaths, merged_score_path, sample_length, max_file_length)\n",
    "test_dataset = MidiDataset(trained_vocab, test_filepaths, merged_score_path, sample_length, max_file_length)\n",
    "\n",
    "print(f'Loading train samples')\n",
    "train_dataset.load_samples(device)\n",
    "\n",
    "print(f'Loading valid samples')\n",
    "valid_dataset.load_samples(device)\n",
    "\n",
    "print(f'Loading test samples')\n",
    "test_dataset.load_samples(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train files: {len(train_dataset.file_lengths)}, Valid files: {len(valid_dataset.file_lengths)}, Test files: {len(test_dataset.file_lengths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size can be changed for a second phase of training quite quickly, it only requires re-computing the indices, not re-loading the data.\n",
    "batch_size = 32\n",
    "train_sampler = ContiguousBatchSampler(train_dataset)\n",
    "valid_sampler = ContiguousBatchSampler(valid_dataset)\n",
    "test_sampler = ContiguousBatchSampler(test_dataset)\n",
    "\n",
    "print(f'Precomputing indices')\n",
    "train_sampler.precompute_indices(batch_size)\n",
    "valid_sampler.precompute_indices(batch_size)\n",
    "test_sampler.precompute_indices(batch_size)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_sampler=train_sampler)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_sampler=valid_sampler)\n",
    "test_data_loader = DataLoader(test_dataset, batch_sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'midi_transformer_knn-xl_{vocab_name}'\n",
    "model_load_path = Path(f'../data/checkpoints/{model_name}.dat')\n",
    "model_save_path = Path(f'../data/checkpoints/{model_name}.dat')\n",
    "log_dir = Path(f'../tensorboard/{model_name}')\n",
    "tensorboard_writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecoderTransformer_KNN_XL(vocab=trained_vocab, sample_length=sample_length, max_file_length=max_file_length, device=device, use_knn=True, n_embed = 384, n_head = 6, n_layer = 6)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-4\n",
    "weight_decay = 1e-3\n",
    "eval_iters = 100\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(data_loader):\n",
    "    model.eval()\n",
    "        \n",
    "    xl_memories = None\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    \n",
    "    # Not working\n",
    "    # Start at a random point in the data, making sure we have enough data to evaluate\n",
    "    # offset = random.randint(0, len(data_loader.dataset) - (eval_iters + 1))\n",
    "    # data_iter = itertools.islice(iter(data_loader), offset, None)\n",
    "\n",
    "    data_iter = iter(data_loader)\n",
    "\n",
    "    for k in range(eval_iters):\n",
    "        file_idxs, batch = next(data_iter)\n",
    "        X, Y = batch[:, :-1], batch[:, 1:, 0] # drop absolute position from Y\n",
    "        _, loss, xl_memories = model(file_idxs, X, xl_memories, Y)\n",
    "        losses[k] = loss.item()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_log_losses = {  \n",
    "    \"train\" : [],\n",
    "    \"val\" : []\n",
    "}\n",
    "\n",
    "epochs = 0\n",
    "\n",
    "def save_checkpoint(iterations):\n",
    "    train_loss = estimate_loss(train_data_loader)\n",
    "    val_loss = estimate_loss(valid_data_loader)\n",
    "    tensorboard_writer.add_scalar('Loss/train', train_loss, iterations)\n",
    "    tensorboard_writer.add_scalar('Loss/val', val_loss, iterations)\n",
    "    train_log_loss = train_loss.log10().item()\n",
    "    val_log_loss = val_loss.log10().item()\n",
    "    average_log_losses['train'].append(train_log_loss)\n",
    "    average_log_losses['val'].append(val_log_loss)\n",
    "    print(f'Epoch {epochs} / Iteration {iterations}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    torch.save({\n",
    "        'iter': iterations,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'vocab_state_dict': trained_vocab.state_dict(),\n",
    "        'losses': average_log_losses,\n",
    "        'epochs': epochs\n",
    "    }, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 200\n",
    "total_iterations = 100000\n",
    "start_iter = 0\n",
    "\n",
    "if model_load_path.exists():\n",
    "    checkpoint = torch.load(model_load_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    average_log_losses = checkpoint['losses']\n",
    "    iterations = checkpoint['iter']\n",
    "    epochs = checkpoint['epochs']\n",
    "    start_iter = iterations + 1\n",
    "    print(f\"Loaded model from iteration {iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "remaining_iters = total_iterations - start_iter\n",
    "if remaining_iters != -1:\n",
    "\n",
    "    print(f\"Training from epoch {epochs} for {remaining_iters} iterations\")\n",
    "    \n",
    "    xl_memories = None\n",
    "    initial_file_idxs = None\n",
    "    train_data = iter(train_data_loader)\n",
    "    offset_iter = start_iter\n",
    "\n",
    "    for iteration in range(remaining_iters):\n",
    "        offset_iter = iteration + start_iter\n",
    "\n",
    "        if offset_iter % eval_interval == 0:\n",
    "            # Because we don't explicitly clear xl and knn mem here, there is always a risk that the eval loop leaves the file idx\n",
    "            # the same as the train loop, but with memories of the 'future' which aren't cleared. It could also break the epoch counter.\n",
    "            # The risk would be much smaller with a bigger data set, but with vg_large we loop through the data quite quickly.\n",
    "            save_checkpoint(offset_iter)\n",
    "\n",
    "        # Configure minibatch\n",
    "        file_idxs, batch = next(train_data)\n",
    "\n",
    "        if initial_file_idxs is None:\n",
    "            initial_file_idxs = file_idxs\n",
    "\n",
    "        if torch.equal(initial_file_idxs, file_idxs):\n",
    "            epochs += 1\n",
    "        \n",
    "        X, Y = batch[:, :-1], batch[:, 1:, 0]\n",
    "\n",
    "        # Forward pass\n",
    "        logits, loss, xl_memories = model(file_idxs, X, xl_memories, Y)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    save_checkpoint(offset_iter + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final training loss:', 10 ** average_log_losses['train'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final validation loss:', 10 ** average_log_losses['val'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(average_log_losses['train'][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(average_log_losses['val'][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_idx = torch.zeros((1,1,2), dtype=torch.long, device=device)\n",
    "generated_tokens = model.generate(init_idx, max_new_tokens=512).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_vocab.itos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = generated_tokens[0, :, 0]\n",
    "trained_vocab.to_tokens(score.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_stream = idx_to_stream_enc(np.array(score), trained_vocab)\n",
    "generated_stream.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_stream.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_midi_file = random.choice(test_filepaths)\n",
    "random_test_path = Path(midi_path, random_test_midi_file)\n",
    "random_test_idx_score = midifile_to_idx_score(random_test_path, trained_vocab)\n",
    "random_test_intro = random_test_idx_score[:sample_length]\n",
    "random_test_intro_stream = idx_to_stream_enc(np.array(random_test_intro[:, 0]), trained_vocab)\n",
    "random_test_intro_stream.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_intro_stream.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_init = torch.tensor(random_test_intro, device=device).unsqueeze(0)\n",
    "random_test_continued = model.generate(random_test_init, max_new_tokens=512).cpu()[0, :, 0]\n",
    "random_test_continued_stream = idx_to_stream_enc(np.array(random_test_continued), trained_vocab)\n",
    "random_test_continued_stream.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_continued_stream.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_timestep(data, include_position=False):\n",
    "    grouped = {}\n",
    "    for idx, position in data:\n",
    "        if position not in grouped:\n",
    "            grouped[position] = []\n",
    "        grouped[position].append(idx)\n",
    "    \n",
    "    if include_position:\n",
    "        result = [(tuple(values), position) for position, values in grouped.items()]\n",
    "    else:\n",
    "        result = [tuple(values) for values in grouped.values()]\n",
    "    return result\n",
    "\n",
    "data = [[1, 0], [2, 0], [3, 0], [4, 1], [5, 1], [6, 2], [7, 2], [8, 2], [9, 2]]\n",
    "\n",
    "grouped_data = group_by_timestep(data, include_position=False)\n",
    "print(grouped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = group_by_timestep(data, include_position=True)\n",
    "print(grouped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = {}\n",
    "grouped_idxs = [(1, 2, 3), (4, 5), (6, 7, 8, 9), (1, 2, 3), (1, 2, 3), (6, 7, 8, 9), (4, 5), (4, 5), (4, 5)]\n",
    "for action in grouped_idxs:\n",
    "    actions[action] = actions.get(action, 0) + 1\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_actions = {k: v for k, v in sorted(actions.items(), key=lambda item: item[1], reverse=True)}\n",
    "sorted_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_actions = list(sorted_actions.keys())[:2]\n",
    "top_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_size = 10\n",
    "\n",
    "def try_replace(action, position):\n",
    "    if action in top_actions:\n",
    "        return [[initial_size + top_actions.index(action)], position]\n",
    "    else:\n",
    "        return [list(action), position]\n",
    "        \n",
    "replaced_score = [try_replace(action, position) for action, position in grouped_data]\n",
    "replaced_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_idx_score = []\n",
    "pos_score = []\n",
    "for (action, position) in replaced_score:\n",
    "    for index in action:\n",
    "        note_idx_score.append(index)\n",
    "        pos_score.append(position)\n",
    "note_idx_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, action in enumerate(top_actions):\n",
    "   print(' '.join([str(a) for a in action]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
