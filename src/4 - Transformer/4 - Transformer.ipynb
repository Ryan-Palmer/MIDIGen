{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'4 - Transformer')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from itertools import chain\n",
    "from itertools import groupby\n",
    "from functools import reduce\n",
    "from typing import Collection, List\n",
    "from pathlib import Path\n",
    "import music21 as m21\n",
    "musescore_path = '/usr/bin/mscore'\n",
    "m21.environment.set('musicxmlPath', musescore_path)\n",
    "m21.environment.set('musescoreDirectPNGPath', musescore_path)\n",
    "from midi_encoding import *\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4090.\n"
     ]
    }
   ],
   "source": [
    "if device == \"cuda\":\n",
    "    print(f\"Device: {torch.cuda.get_device_name()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midi_encoding import *\n",
    "\n",
    "vocab = MusicVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3839"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vg_large_path = Path('../data/midi/vg_large')\n",
    "vg_large_file_names = [f for f in os.listdir(vg_large_path) if os.path.isfile(os.path.join(vg_large_path, f))]\n",
    "len(vg_large_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "Previously we used a fixed context window to split out data into blocks, e.g. 'if we see this block of 8 tokens, what is the ninth?'.\n",
    "\n",
    "That meant we always had to init the generation with an entire block of context, mostly made of padding. We also needed to add these 'padding init' blocks to training so that the network had seen them before inference time.\n",
    "\n",
    "Now that we are working with transformers, we will take each block and split it into samples of varying lengths, e.g.\n",
    "\n",
    "Data - (1, 2, 3, 4)\n",
    "\n",
    "X(1) -> Y(2)\n",
    "\n",
    "X(1,2) -> Y(3)\n",
    "\n",
    "X(1,2,3) -> Y(4)\n",
    "\n",
    "Andrej loads the entire text dataset into a single 1D tensor, then randomly indexes into it to choose a block.\n",
    "\n",
    "Because I want to prevent blocks of performances overlapping (i.e. I never want a block with a token *after* `<|eos|>`), I don't want to just append all the sample data into a flat tensor. Instead I will block encode the data upfront and then randomly index into the resulting 2D tensor of blocks.\n",
    "\n",
    "This is similar to the approach taken in previous notebooks. One difference here is that I am going to encode the entire dataset at once and *then* split into train / test / validation data, rather than split by filenames, because it occured to me that the MIDI tracks might have very different lengths.\n",
    "\n",
    "> I assume this is very inefficient in terms of memory as we need to create every block up front and load them into memory, which means every token appears 7 times as it moves through the block window rather than once. We also have to effectively load the sequence twice (`X` and `Y`) for the reason outlined below, so naively that is 14X the data in memory? It may be that when we move to larger datasets that we need to adopt the single-sequence approach.\n",
    "\n",
    "### Labels\n",
    "\n",
    "Because we are training every position in the sequence at once, we need a block of labels for each block of data.\n",
    "\n",
    "This is just the next block in the array (i.e. `X[1]` is the labels for `X[0]`, and `X[2]` is the labels for `X[1]` etc etc), so ordinarily there is no point in encoding them separately.\n",
    "\n",
    "However, in our case where we are encoding the blocks ahead of time this 'next block is the labels' approach would fall over at song boundaries, as the labels for the last block of a song are certainly not the first block of the next song. \n",
    "\n",
    "### Position\n",
    "\n",
    "We are going to need to add positional encodings to our data.\n",
    "\n",
    "Transformers employ a position embedding layer to distinguish between positions in a sequence.\n",
    "\n",
    "In addition to this, we want to create embeddings for the bar and beat.\n",
    "\n",
    "These can all be added together to get a hybrid 'position' vector.\n",
    "\n",
    "To allow us to calculate the bar and beat we need the absolute position of each token, which we can encode at the same time as the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33384/2408250059.py:22: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905979055/work/torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  return torch.tensor(xs, device=device), torch.tensor(ys, device=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([11569272, 8, 2]), torch.Size([11569272, 8]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block_encode(file_names, vocab, block_size):\n",
    "    # We can't shuffle the blocks afterwards as we rely on them being contiguous in order to get the targets\n",
    "    # If we didn't shuffle at all then the train and test splits would contain specific game soundtracks rather than a mixture\n",
    "    random.shuffle(file_names)\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for file_name in file_names:\n",
    "        file_path = Path(vg_large_path, file_name)\n",
    "        idx_pos_score = midifile_to_idx_score(file_path, vocab)\n",
    "        for i in range(0, len(idx_pos_score) - block_size, 1):\n",
    "            xs.append(idx_pos_score[i:i+block_size])\n",
    "            ys.append(idx_pos_score[i+1:i+1+block_size][:, 0]) # Offset by one and ignore Position\n",
    "    return np.stack(xs), ys # stack xs to create 2D tensor\n",
    "\n",
    "def load_or_create(file_names, vocab, sample_path, label_path):\n",
    "    if sample_path.exists() and label_path.exists():\n",
    "        xs, ys = np.load(sample_path, allow_pickle=True), np.load(label_path, allow_pickle=True)\n",
    "    else:\n",
    "        xs, ys = block_encode(file_names, vocab, block_size)\n",
    "        np.save(sample_path, xs)\n",
    "        np.save(label_path, ys)\n",
    "    return torch.tensor(xs, device=device), torch.tensor(ys, device=device)\n",
    "\n",
    "block_size = 8\n",
    "vg_large_samples_path = Path(f'../data/numpy/vg_large/block_{block_size}_samples_with_position_all.npy')\n",
    "vg_large_labels_path = Path(f'../data/numpy/vg_large/block_{block_size}_labels_all.npy')\n",
    "\n",
    "X, Y = load_or_create(vg_large_file_names, vocab, vg_large_samples_path, vg_large_labels_path)\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|sos|> n84 d1 n60 d1 n55 d1 <|sep|>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.to_tokens(X[0][:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n84 d1 n60 d1 n55 d1 <|sep|> d1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.to_tokens(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n48 d2 <|sep|> d2 n50 d2 n45 d2'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.to_tokens(X[-1][:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d2 <|sep|> d2 n50 d2 n45 d2 <|eos|>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.to_tokens(Y[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
