{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'4 - Transformer')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from itertools import chain\n",
    "from itertools import groupby\n",
    "from functools import reduce\n",
    "from typing import Collection, List\n",
    "from pathlib import Path\n",
    "import music21 as m21\n",
    "musescore_path = '/usr/bin/mscore'\n",
    "m21.environment.set('musicxmlPath', musescore_path)\n",
    "m21.environment.set('musescoreDirectPNGPath', musescore_path)\n",
    "from midi_encoding import *\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4090.\n"
     ]
    }
   ],
   "source": [
    "if device == \"cuda\":\n",
    "    print(f\"Device: {torch.cuda.get_device_name()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 389\n",
      "Padding vocab 5\n",
      "Padded vocab length: 392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = MusicVocab()\n",
    "vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3839"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vg_large_path = Path('../data/midi/vg_large')\n",
    "vg_large_file_names = [f for f in os.listdir(vg_large_path) if os.path.isfile(os.path.join(vg_large_path, f))]\n",
    "len(vg_large_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "Previously we used a fixed context window to split out data into blocks, e.g. 'if we see this block of 8 tokens, what is the ninth?'.\n",
    "\n",
    "That meant we always had to init the generation with an entire block of context, mostly made of padding. We also needed to add these 'padding init' blocks to training so that the network had seen them before inference time.\n",
    "\n",
    "Now that we are working with transformers, we will take each block and split it into samples of varying lengths, e.g.\n",
    "\n",
    "Data - (1, 2, 3, 4)\n",
    "\n",
    "X(1) -> Y(2)\n",
    "\n",
    "X(1,2) -> Y(3)\n",
    "\n",
    "X(1,2,3) -> Y(4)\n",
    "\n",
    "Because we are going to drastically scale up our block size, it is no longer feasible to pre-block encode the data as every token is duplicated up to block size.\n",
    "\n",
    "We will need to load a single 1D tensor, then randomly indexes into it to choose a block.\n",
    "\n",
    "This does mean that at boundaries we will have blocks of performances overlapping (i.e. there will be blocks with a token *after* `<|eos|>`), but hopefully the network will realise that data before and after is correlated.\n",
    "\n",
    "Alternatively, we could try re-sampling if we get a block with `<|eos|>` anywhere other than the end?\n",
    "\n",
    "In this notebook we are going to encode the entire dataset at once and *then* split into train / test / validation data, rather than first split by filenames, because it occured to me that the MIDI tracks might have very different lengths.\n",
    "\n",
    "\n",
    "### Labels\n",
    "\n",
    "Because we are training every position in the sequence at once, we need a block of labels for each block of data.\n",
    "\n",
    "This is just the next block in the array (i.e. `X[1]` is the labels for `X[0]`, and `X[2]` is the labels for `X[1]` etc etc), so there is no point in encoding them separately.\n",
    "\n",
    "### Position\n",
    "\n",
    "We are going to need to add positional encodings to our data.\n",
    "\n",
    "Transformers employ a position embedding layer to distinguish between positions in a sequence.\n",
    "\n",
    "In addition to this, we want to create embeddings for the bar and beat.\n",
    "\n",
    "These can all be added together to get a hybrid 'position' vector.\n",
    "\n",
    "To allow us to calculate the bar and beat we need the absolute position of each token, which we can encode at the same time as the batches.\n",
    "\n",
    "> See `sparse_to_position_enc` - this is where we throw away timestep info, so we now 'snaphot' tidx and package alongside the token values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8781624])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def idx_encode(file_names):\n",
    "    # We can't shuffle the blocks afterwards as we rely on them being contiguous in order to get the targets\n",
    "    # If we didn't shuffle at all then the train and test splits would contain specific game soundtracks rather than a mixture\n",
    "    random.shuffle(file_names)\n",
    "    xs = []\n",
    "    for file_name in file_names:\n",
    "        file_path = Path(vg_large_path, file_name)\n",
    "        idx_score = midifile_to_idx_score(file_path, vocab, False)\n",
    "        if idx_score is not None: # Files which aren't 4/4 are ignored\n",
    "            xs.append(idx_score)\n",
    "    return np.concat(xs) # concat scores to create 1D tensor\n",
    "\n",
    "def load_or_create(file_names, sample_path):\n",
    "    if sample_path.exists():\n",
    "        xs = np.load(sample_path, allow_pickle=True)\n",
    "    else:\n",
    "        xs = idx_encode(file_names)\n",
    "        np.save(sample_path, xs)\n",
    "\n",
    "    return torch.tensor(xs, device=device)\n",
    "\n",
    "vg_large_samples_path = Path(f'../data/numpy/vg_large/samples_all.npy')\n",
    "\n",
    "data = load_or_create(vg_large_file_names, vg_large_samples_path)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data[0:-1, :]\n",
    "# Y = data[1:, 0] # Drop position from Y\n",
    "X = data[0:-1]\n",
    "Y = data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,  65, 140,  58, 140,  54, 140,   3, 140,  65, 140,  58],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  3, 134,  88, 136,  83, 136,  52, 136,   3, 136,  47, 136],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|sos|> n61 d8 n54 d8 n50 d8 <|sep|> d8 n61 d8 n54'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab.to_tokens(X[:, 0][:12])\n",
    "vocab.to_tokens(X[:][:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n61 d8 n54 d8 n50 d8 <|sep|> d8 n61 d8 n54 d8'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.to_tokens(Y[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|sep|> d2 n84 d4 n79 d4 n48 d4 <|sep|> d4 n43 d4'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab.to_tokens(X[:, 0][-12:])\n",
    "vocab.to_tokens(X[:][-12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d2 n84 d4 n79 d4 n48 d4 <|sep|> d4 n43 d4 <|eos|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.to_tokens(Y[-12:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to build a decoder-only transformer with positional embeddings.\n",
    "\n",
    "We will start by creating simple custom PyTorch modules following Karpathy in order to fully understand the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed = 384 # /6 heads = 64 per head\n",
    "block_size = 256\n",
    "dropout = 0.2\n",
    "\n",
    "class SelfAttentionHead(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = torch.nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = torch.nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Key - What it offers\n",
    "        k = self.key(x) # (B, T, C)\n",
    "\n",
    "        # Query - What it looks for\n",
    "        q = self.query(x) # (B, T, C)\n",
    "\n",
    "        # Attention scores\n",
    "        w = q @ k.transpose(-2, -1) * C ** -0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "\n",
    "        w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) Mask the future positions \n",
    "        w = F.softmax(w, dim=-1) # (B, T, T)\n",
    "        w = self.dropout(w)\n",
    "\n",
    "        # Value - What it has to communicate\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        \n",
    "        # Scale the values by the attention weights\n",
    "        return w @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "    \n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = torch.nn.ModuleList([SelfAttentionHead(head_size) for _ in range(n_heads)])\n",
    "        self.proj = torch.nn.Linear(n_heads * head_size, n_embed) \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Concat over the channel dimension\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # Could all be done in parallel by adding a head dimension\n",
    "        out = self.proj(out)\n",
    "        return self.dropout(out)\n",
    "    \n",
    "class FeedForward(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_embed, 4 * n_embed), # 4x is a common expansion factor\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(4 * n_embed, n_embed), # Project back to the residual stream\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.self_attn = MultiHeadAttention(n_head, head_size)\n",
    "        self.ff = FeedForward(n_embed)\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(n_embed)\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual connections\n",
    "        x = x + self.self_attn(self.layer_norm1(x))\n",
    "        x = x + self.ff(self.layer_norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_bar_position = 1024 # 4 beats per bar = 4096 beats, which at 120bpm is 34 minutes. This will hopefully be enough for any song in the training set.\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "\n",
    "class DecoderTransformer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding = torch.nn.Embedding(vocab_size, n_embed)\n",
    "        self.positional_embedding = torch.nn.Embedding(block_size, n_embed)\n",
    "        # self.beat_embedding = torch.nn.Embedding(BEATS_PER_MEASURE, n_embed) # 4 beats per measure\n",
    "        # self.bar_embedding = torch.nn.Embedding(max_bar_position, n_embed)\n",
    "        self.blocks = torch.nn.Sequential(*[Block(n_embed, n_head) for _ in range(n_layer)])\n",
    "        self.layer_norm = torch.nn.LayerNorm(n_embed)\n",
    "        self.lm_head = torch.nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # B, T, E = idx.size()\n",
    "\n",
    "        # token_idx = idx[:, :, 0] # (B,T)\n",
    "        # time_idx = idx[:, :, 1] # (B,T)\n",
    "        # bar_idx = time_idx % SAMPLES_PER_BAR # (B,T)\n",
    "        # beat_idx = (time_idx // SAMPLES_PER_BAR) % max_bar_position # (B,T)\n",
    "\n",
    "        # token_embed = self.token_embedding(token_idx) # (B,T,Embed)\n",
    "        # pos_embed = self.positional_embedding(torch.arange(T, device=device)) # (T,Embed)\n",
    "        # bar_embed = self.bar_embedding(bar_idx) # (B,T,Embed)\n",
    "        # beat_embed = self.beat_embedding(beat_idx) # (B,T,Embed)\n",
    "\n",
    "        B, T = idx.size()\n",
    "\n",
    "        token_embed = self.token_embedding(idx) # (B,T,Embed)\n",
    "        pos_embed = self.positional_embedding(torch.arange(T, device=device)) # (T,Embed)\n",
    "        \n",
    "        x = token_embed + pos_embed #+ bar_embed + beat_embed\n",
    "        x = self.blocks(x)\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        if targets is None:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "        else:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # Flatten all the batches\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens=1024, temperature=1.0):\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        self.train()\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11040392\n"
     ]
    }
   ],
   "source": [
    "model = DecoderTransformer(vocab_size=vocab.size)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "token_embedding.weight \t torch.Size([392, 384])\n",
      "positional_embedding.weight \t torch.Size([256, 384])\n",
      "blocks.0.self_attn.heads.0.tril \t torch.Size([256, 256])\n",
      "blocks.0.self_attn.heads.0.key.weight \t torch.Size([64, 384])\n",
      "blocks.0.self_attn.heads.0.query.weight \t torch.Size([64, 384])\n",
      "blocks.0.self_attn.heads.0.value.weight \t torch.Size([64, 384])\n",
      "blocks.0.self_attn.heads.1.tril \t torch.Size([256, 256])\n",
      "blocks.0.self_attn.heads.1.key.weight \t torch.Size([64, 384])\n",
      "blocks.0.self_attn.heads.1.query.weight \t torch.Size([64, 384])\n",
      "blocks.0.self_attn.heads.1.value.weight \t torch.Size([64, 384])\n",
      "blocks.0.self_attn.heads.2.tril \t torch.Size([256, 256])\n",
      "blocks.0.self_attn.heads.2.key.weight \t torch.Size([64, 384])\n",
      "blocks.0.self_attn.heads.2.query.weight \t torch.Size([64, 384])\n",
      "blocks.0.self_attn.heads.2.value.weight \t torch.Size([64, 384])\n",
      "blocks.0.self_attn.heads.3.tril \t torch.Size([256, 256])\n",
      "blocks.0.self_attn.heads.3.key.weight \t torch.Size([64, 384])\n",
      "blocks.0.self_attn.heads.3.query.weight \t torch.Size([64, 384])\n",
      "blocks.0.self_attn.heads.3.value.weight \t torch.Size([64, 384])\n",
      "blocks.0.self_attn.heads.4.tril \t torch.Size([256, 256])\n",
      "blocks.0.self_attn.heads.4.key.weight \t torch.Size([64, 384])\n",
      "blocks.0.self_attn.heads.4.query.weight \t torch.Size([64, 384])\n",
      "blocks.0.self_attn.heads.4.value.weight \t torch.Size([64, 384])\n",
      "blocks.0.self_attn.heads.5.tril \t torch.Size([256, 256])\n",
      "blocks.0.self_attn.heads.5.key.weight \t torch.Size([64, 384])\n",
      "blocks.0.self_attn.heads.5.query.weight \t torch.Size([64, 384])\n",
      "blocks.0.self_attn.heads.5.value.weight \t torch.Size([64, 384])\n",
      "blocks.0.self_attn.proj.weight \t torch.Size([384, 384])\n",
      "blocks.0.self_attn.proj.bias \t torch.Size([384])\n",
      "blocks.0.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.0.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.0.ff.net.2.weight \t torch.Size([384, 1536])\n",
      "blocks.0.ff.net.2.bias \t torch.Size([384])\n",
      "blocks.0.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.0.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.0.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.0.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.1.self_attn.heads.0.tril \t torch.Size([256, 256])\n",
      "blocks.1.self_attn.heads.0.key.weight \t torch.Size([64, 384])\n",
      "blocks.1.self_attn.heads.0.query.weight \t torch.Size([64, 384])\n",
      "blocks.1.self_attn.heads.0.value.weight \t torch.Size([64, 384])\n",
      "blocks.1.self_attn.heads.1.tril \t torch.Size([256, 256])\n",
      "blocks.1.self_attn.heads.1.key.weight \t torch.Size([64, 384])\n",
      "blocks.1.self_attn.heads.1.query.weight \t torch.Size([64, 384])\n",
      "blocks.1.self_attn.heads.1.value.weight \t torch.Size([64, 384])\n",
      "blocks.1.self_attn.heads.2.tril \t torch.Size([256, 256])\n",
      "blocks.1.self_attn.heads.2.key.weight \t torch.Size([64, 384])\n",
      "blocks.1.self_attn.heads.2.query.weight \t torch.Size([64, 384])\n",
      "blocks.1.self_attn.heads.2.value.weight \t torch.Size([64, 384])\n",
      "blocks.1.self_attn.heads.3.tril \t torch.Size([256, 256])\n",
      "blocks.1.self_attn.heads.3.key.weight \t torch.Size([64, 384])\n",
      "blocks.1.self_attn.heads.3.query.weight \t torch.Size([64, 384])\n",
      "blocks.1.self_attn.heads.3.value.weight \t torch.Size([64, 384])\n",
      "blocks.1.self_attn.heads.4.tril \t torch.Size([256, 256])\n",
      "blocks.1.self_attn.heads.4.key.weight \t torch.Size([64, 384])\n",
      "blocks.1.self_attn.heads.4.query.weight \t torch.Size([64, 384])\n",
      "blocks.1.self_attn.heads.4.value.weight \t torch.Size([64, 384])\n",
      "blocks.1.self_attn.heads.5.tril \t torch.Size([256, 256])\n",
      "blocks.1.self_attn.heads.5.key.weight \t torch.Size([64, 384])\n",
      "blocks.1.self_attn.heads.5.query.weight \t torch.Size([64, 384])\n",
      "blocks.1.self_attn.heads.5.value.weight \t torch.Size([64, 384])\n",
      "blocks.1.self_attn.proj.weight \t torch.Size([384, 384])\n",
      "blocks.1.self_attn.proj.bias \t torch.Size([384])\n",
      "blocks.1.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.1.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.1.ff.net.2.weight \t torch.Size([384, 1536])\n",
      "blocks.1.ff.net.2.bias \t torch.Size([384])\n",
      "blocks.1.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.1.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.1.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.1.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.2.self_attn.heads.0.tril \t torch.Size([256, 256])\n",
      "blocks.2.self_attn.heads.0.key.weight \t torch.Size([64, 384])\n",
      "blocks.2.self_attn.heads.0.query.weight \t torch.Size([64, 384])\n",
      "blocks.2.self_attn.heads.0.value.weight \t torch.Size([64, 384])\n",
      "blocks.2.self_attn.heads.1.tril \t torch.Size([256, 256])\n",
      "blocks.2.self_attn.heads.1.key.weight \t torch.Size([64, 384])\n",
      "blocks.2.self_attn.heads.1.query.weight \t torch.Size([64, 384])\n",
      "blocks.2.self_attn.heads.1.value.weight \t torch.Size([64, 384])\n",
      "blocks.2.self_attn.heads.2.tril \t torch.Size([256, 256])\n",
      "blocks.2.self_attn.heads.2.key.weight \t torch.Size([64, 384])\n",
      "blocks.2.self_attn.heads.2.query.weight \t torch.Size([64, 384])\n",
      "blocks.2.self_attn.heads.2.value.weight \t torch.Size([64, 384])\n",
      "blocks.2.self_attn.heads.3.tril \t torch.Size([256, 256])\n",
      "blocks.2.self_attn.heads.3.key.weight \t torch.Size([64, 384])\n",
      "blocks.2.self_attn.heads.3.query.weight \t torch.Size([64, 384])\n",
      "blocks.2.self_attn.heads.3.value.weight \t torch.Size([64, 384])\n",
      "blocks.2.self_attn.heads.4.tril \t torch.Size([256, 256])\n",
      "blocks.2.self_attn.heads.4.key.weight \t torch.Size([64, 384])\n",
      "blocks.2.self_attn.heads.4.query.weight \t torch.Size([64, 384])\n",
      "blocks.2.self_attn.heads.4.value.weight \t torch.Size([64, 384])\n",
      "blocks.2.self_attn.heads.5.tril \t torch.Size([256, 256])\n",
      "blocks.2.self_attn.heads.5.key.weight \t torch.Size([64, 384])\n",
      "blocks.2.self_attn.heads.5.query.weight \t torch.Size([64, 384])\n",
      "blocks.2.self_attn.heads.5.value.weight \t torch.Size([64, 384])\n",
      "blocks.2.self_attn.proj.weight \t torch.Size([384, 384])\n",
      "blocks.2.self_attn.proj.bias \t torch.Size([384])\n",
      "blocks.2.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.2.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.2.ff.net.2.weight \t torch.Size([384, 1536])\n",
      "blocks.2.ff.net.2.bias \t torch.Size([384])\n",
      "blocks.2.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.2.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.2.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.2.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.3.self_attn.heads.0.tril \t torch.Size([256, 256])\n",
      "blocks.3.self_attn.heads.0.key.weight \t torch.Size([64, 384])\n",
      "blocks.3.self_attn.heads.0.query.weight \t torch.Size([64, 384])\n",
      "blocks.3.self_attn.heads.0.value.weight \t torch.Size([64, 384])\n",
      "blocks.3.self_attn.heads.1.tril \t torch.Size([256, 256])\n",
      "blocks.3.self_attn.heads.1.key.weight \t torch.Size([64, 384])\n",
      "blocks.3.self_attn.heads.1.query.weight \t torch.Size([64, 384])\n",
      "blocks.3.self_attn.heads.1.value.weight \t torch.Size([64, 384])\n",
      "blocks.3.self_attn.heads.2.tril \t torch.Size([256, 256])\n",
      "blocks.3.self_attn.heads.2.key.weight \t torch.Size([64, 384])\n",
      "blocks.3.self_attn.heads.2.query.weight \t torch.Size([64, 384])\n",
      "blocks.3.self_attn.heads.2.value.weight \t torch.Size([64, 384])\n",
      "blocks.3.self_attn.heads.3.tril \t torch.Size([256, 256])\n",
      "blocks.3.self_attn.heads.3.key.weight \t torch.Size([64, 384])\n",
      "blocks.3.self_attn.heads.3.query.weight \t torch.Size([64, 384])\n",
      "blocks.3.self_attn.heads.3.value.weight \t torch.Size([64, 384])\n",
      "blocks.3.self_attn.heads.4.tril \t torch.Size([256, 256])\n",
      "blocks.3.self_attn.heads.4.key.weight \t torch.Size([64, 384])\n",
      "blocks.3.self_attn.heads.4.query.weight \t torch.Size([64, 384])\n",
      "blocks.3.self_attn.heads.4.value.weight \t torch.Size([64, 384])\n",
      "blocks.3.self_attn.heads.5.tril \t torch.Size([256, 256])\n",
      "blocks.3.self_attn.heads.5.key.weight \t torch.Size([64, 384])\n",
      "blocks.3.self_attn.heads.5.query.weight \t torch.Size([64, 384])\n",
      "blocks.3.self_attn.heads.5.value.weight \t torch.Size([64, 384])\n",
      "blocks.3.self_attn.proj.weight \t torch.Size([384, 384])\n",
      "blocks.3.self_attn.proj.bias \t torch.Size([384])\n",
      "blocks.3.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.3.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.3.ff.net.2.weight \t torch.Size([384, 1536])\n",
      "blocks.3.ff.net.2.bias \t torch.Size([384])\n",
      "blocks.3.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.3.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.3.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.3.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.4.self_attn.heads.0.tril \t torch.Size([256, 256])\n",
      "blocks.4.self_attn.heads.0.key.weight \t torch.Size([64, 384])\n",
      "blocks.4.self_attn.heads.0.query.weight \t torch.Size([64, 384])\n",
      "blocks.4.self_attn.heads.0.value.weight \t torch.Size([64, 384])\n",
      "blocks.4.self_attn.heads.1.tril \t torch.Size([256, 256])\n",
      "blocks.4.self_attn.heads.1.key.weight \t torch.Size([64, 384])\n",
      "blocks.4.self_attn.heads.1.query.weight \t torch.Size([64, 384])\n",
      "blocks.4.self_attn.heads.1.value.weight \t torch.Size([64, 384])\n",
      "blocks.4.self_attn.heads.2.tril \t torch.Size([256, 256])\n",
      "blocks.4.self_attn.heads.2.key.weight \t torch.Size([64, 384])\n",
      "blocks.4.self_attn.heads.2.query.weight \t torch.Size([64, 384])\n",
      "blocks.4.self_attn.heads.2.value.weight \t torch.Size([64, 384])\n",
      "blocks.4.self_attn.heads.3.tril \t torch.Size([256, 256])\n",
      "blocks.4.self_attn.heads.3.key.weight \t torch.Size([64, 384])\n",
      "blocks.4.self_attn.heads.3.query.weight \t torch.Size([64, 384])\n",
      "blocks.4.self_attn.heads.3.value.weight \t torch.Size([64, 384])\n",
      "blocks.4.self_attn.heads.4.tril \t torch.Size([256, 256])\n",
      "blocks.4.self_attn.heads.4.key.weight \t torch.Size([64, 384])\n",
      "blocks.4.self_attn.heads.4.query.weight \t torch.Size([64, 384])\n",
      "blocks.4.self_attn.heads.4.value.weight \t torch.Size([64, 384])\n",
      "blocks.4.self_attn.heads.5.tril \t torch.Size([256, 256])\n",
      "blocks.4.self_attn.heads.5.key.weight \t torch.Size([64, 384])\n",
      "blocks.4.self_attn.heads.5.query.weight \t torch.Size([64, 384])\n",
      "blocks.4.self_attn.heads.5.value.weight \t torch.Size([64, 384])\n",
      "blocks.4.self_attn.proj.weight \t torch.Size([384, 384])\n",
      "blocks.4.self_attn.proj.bias \t torch.Size([384])\n",
      "blocks.4.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.4.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.4.ff.net.2.weight \t torch.Size([384, 1536])\n",
      "blocks.4.ff.net.2.bias \t torch.Size([384])\n",
      "blocks.4.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.4.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.4.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.4.layer_norm2.bias \t torch.Size([384])\n",
      "blocks.5.self_attn.heads.0.tril \t torch.Size([256, 256])\n",
      "blocks.5.self_attn.heads.0.key.weight \t torch.Size([64, 384])\n",
      "blocks.5.self_attn.heads.0.query.weight \t torch.Size([64, 384])\n",
      "blocks.5.self_attn.heads.0.value.weight \t torch.Size([64, 384])\n",
      "blocks.5.self_attn.heads.1.tril \t torch.Size([256, 256])\n",
      "blocks.5.self_attn.heads.1.key.weight \t torch.Size([64, 384])\n",
      "blocks.5.self_attn.heads.1.query.weight \t torch.Size([64, 384])\n",
      "blocks.5.self_attn.heads.1.value.weight \t torch.Size([64, 384])\n",
      "blocks.5.self_attn.heads.2.tril \t torch.Size([256, 256])\n",
      "blocks.5.self_attn.heads.2.key.weight \t torch.Size([64, 384])\n",
      "blocks.5.self_attn.heads.2.query.weight \t torch.Size([64, 384])\n",
      "blocks.5.self_attn.heads.2.value.weight \t torch.Size([64, 384])\n",
      "blocks.5.self_attn.heads.3.tril \t torch.Size([256, 256])\n",
      "blocks.5.self_attn.heads.3.key.weight \t torch.Size([64, 384])\n",
      "blocks.5.self_attn.heads.3.query.weight \t torch.Size([64, 384])\n",
      "blocks.5.self_attn.heads.3.value.weight \t torch.Size([64, 384])\n",
      "blocks.5.self_attn.heads.4.tril \t torch.Size([256, 256])\n",
      "blocks.5.self_attn.heads.4.key.weight \t torch.Size([64, 384])\n",
      "blocks.5.self_attn.heads.4.query.weight \t torch.Size([64, 384])\n",
      "blocks.5.self_attn.heads.4.value.weight \t torch.Size([64, 384])\n",
      "blocks.5.self_attn.heads.5.tril \t torch.Size([256, 256])\n",
      "blocks.5.self_attn.heads.5.key.weight \t torch.Size([64, 384])\n",
      "blocks.5.self_attn.heads.5.query.weight \t torch.Size([64, 384])\n",
      "blocks.5.self_attn.heads.5.value.weight \t torch.Size([64, 384])\n",
      "blocks.5.self_attn.proj.weight \t torch.Size([384, 384])\n",
      "blocks.5.self_attn.proj.bias \t torch.Size([384])\n",
      "blocks.5.ff.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.5.ff.net.0.bias \t torch.Size([1536])\n",
      "blocks.5.ff.net.2.weight \t torch.Size([384, 1536])\n",
      "blocks.5.ff.net.2.bias \t torch.Size([384])\n",
      "blocks.5.layer_norm1.weight \t torch.Size([384])\n",
      "blocks.5.layer_norm1.bias \t torch.Size([384])\n",
      "blocks.5.layer_norm2.weight \t torch.Size([384])\n",
      "blocks.5.layer_norm2.bias \t torch.Size([384])\n",
      "layer_norm.weight \t torch.Size([384])\n",
      "layer_norm.bias \t torch.Size([384])\n",
      "lm_head.weight \t torch.Size([392, 384])\n",
      "lm_head.bias \t torch.Size([392])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "eval_iters = 100\n",
    "learning_rate = 3e-4\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_split = int(len(data) * 0.9)\n",
    "train_data, val_data = data[:n_split], data[n_split:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path('../data/model/midi_transformer_1.tar')\n",
    "\n",
    "average_log_losses = {\n",
    "    \"train\" : [],\n",
    "    \"val\" : []\n",
    "}\n",
    "\n",
    "def save_checkpoint(iter):\n",
    "    losses = estimate_loss()\n",
    "    train_loss = losses['train']\n",
    "    val_loss = losses['val']\n",
    "    average_log_losses['train'].append(train_loss.log10().item())\n",
    "    average_log_losses['val'].append(val_loss.log10().item())\n",
    "    print(f'Iteration {iter}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    torch.save({\n",
    "        'iter': iter,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'losses': average_log_losses,\n",
    "    }, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Train Loss: 6.1397, Val Loss: 6.1286\n",
      "Iteration 500, Train Loss: 3.9969, Val Loss: 4.1338\n",
      "Iteration 1000, Train Loss: 3.6346, Val Loss: 3.7656\n"
     ]
    }
   ],
   "source": [
    "eval_interval = 500\n",
    "iterations = 50000\n",
    "start_iter = 0\n",
    "\n",
    "if model_path.exists():\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    average_log_losses = checkpoint['losses']\n",
    "    iter = checkpoint['iter']\n",
    "    start_iter = iter + 1\n",
    "    print(f\"Loaded model from iteration {iter}\")\n",
    "\n",
    "model.train()\n",
    "\n",
    "remaining_iters = iterations - start_iter\n",
    "if remaining_iters != -1:\n",
    "    for iter in range(remaining_iters):\n",
    "\n",
    "        offset_iter = iter + start_iter\n",
    "        if offset_iter % eval_interval == 0:\n",
    "            save_checkpoint(offset_iter)\n",
    "\n",
    "        # Configure minibatch\n",
    "        Xb, Yb = get_batch('train')\n",
    "\n",
    "        # Forward pass\n",
    "        logits, loss = model(Xb, Yb)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    save_checkpoint(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final training loss:', 10 ** average_log_losses['train'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final validation loss:', 10 ** average_log_losses['val'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(average_log_losses['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(average_log_losses['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_tokens = model.generate(init_idx, max_new_tokens=12).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.to_tokens(generated_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_stream = idx_to_stream_enc(np.array(generated_tokens), vocab)\n",
    "generated_stream.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_stream.show('midi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
