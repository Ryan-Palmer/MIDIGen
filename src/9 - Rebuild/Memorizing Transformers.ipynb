{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'9 - Rebuild')\n",
    "import torch\n",
    "from miditok import REMI, TokenizerConfig  # here we choose to use REMI\n",
    "from pathlib import Path\n",
    "import random\n",
    "from miditok.utils import split_files_for_training\n",
    "from miditok.data_augmentation import augment_dataset\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from torch.utils.data import DataLoader\n",
    "from miditok import TokSequence\n",
    "from multiprocessing import Pool\n",
    "from memorizing_transformers_pytorch import MemorizingTransformer\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pretty_midi\n",
    "import IPython.display\n",
    "import music21 as m21\n",
    "musescore_path = '/usr/bin/mscore'\n",
    "m21.environment.set('musicxmlPath', musescore_path)\n",
    "m21.environment.set('musescoreDirectPNGPath', musescore_path)\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4090.\n"
     ]
    }
   ],
   "source": [
    "if device == \"cuda\":\n",
    "    print(f\"Device: {torch.cuda.get_device_name()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memorizing Transformers\n",
    "\n",
    "From our MIDITok research, we know that we we want to \n",
    "\n",
    "- Train a BPE tokenizer on the entire dataset.\n",
    "- Save it / Load it (BPE is deterministic so data doesn't need to be decoded with a the same tokenizer it was encoded with, providing whatever is used was trained on the same data with the same config. Unigram is *not* deterministic however so would require the same exact tokenizer for encode / decode).\n",
    "- Shuffle file names, so songs aren't biased to a set.\n",
    "- Split into test / train / validation sets.\n",
    "- Split the files into chunks for each set.\n",
    "- Optionally augment the dataset with pitch / velocity / duration shifted versions\n",
    "- Shuffle the chunks when loading, so that parts of a single song aren't biased to a batch.\n",
    "- Load the chunks with `max_seq_len` equal to that used when splitting files, to minimise padding / truncated data.\n",
    "- Split the chunks into context-length sequences and feed them through contiguously.\n",
    "- Manually reset memories between chunks rather than auto-reset on BOS / EOS tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_LENGTH = 2048\n",
    "SEGMENTS = 4\n",
    "TIMESTEPS = CHUNK_LENGTH // SEGMENTS # Context length\n",
    "BATCH_SIZE = 16\n",
    "VOCAB_SIZE = 716 # REMI basic untrained token count = 411, +chords = 425, +tempos = 457, +time sig = 530, +rests = 562, +chord root note = 716\n",
    "N_EMBED = 512\n",
    "N_LAYER = 8\n",
    "N_HEAD = 8\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-3\n",
    "NUM_BATCHES = int(1e5)\n",
    "MAX_GRAD_CLIP_NORM = 0.5\n",
    "VALIDATE_EVERY  = 64\n",
    "DIM_HEAD = N_EMBED // N_HEAD\n",
    "VERSION_LABEL = \"All_Options\"\n",
    "TOKENIZER_CONFIG = 'All_Options'\n",
    "\n",
    "midi_path = Path(f'../data/midi')\n",
    "dataset_name = 'lakh_clean'\n",
    "midi_dataset_path = Path(f'{midi_path}/{dataset_name}')\n",
    "midi_file_paths = [p.resolve() for p in midi_dataset_path.glob(\"**/*.mid\")]\n",
    "\n",
    "tokenizer_save_path = Path(f'../data/vocab/MidiTok/{dataset_name}_{VOCAB_SIZE}_{TOKENIZER_CONFIG}.json')\n",
    "\n",
    "if not tokenizer_save_path.exists():\n",
    "    TOKENIZER_PARAMS = {\n",
    "        \"pitch_range\": (21, 109),\n",
    "        \"beat_res\": {(0, 4): 8, (4, 12): 4},\n",
    "        \"num_velocities\": 32,\n",
    "        \"use_programs\": True,\n",
    "        \"program_changes \": True, # Only insert program changes when the instrument changes rather than before every note\n",
    "        \"use_chords\": True,\n",
    "        \"chord_tokens_with_root_note\": True, # Include the root note in the chord token\n",
    "        # \"use_pitch_bends\": True,\n",
    "        \"use_time_signatures\": True,\n",
    "        \"delete_equal_successive_time_sig_changes\": True, # Only insert time signatures when the time signature changes\n",
    "        \"use_tempos\": True,\n",
    "        \"delete_equal_successive_tempo_changes \": True, # Only insert tempos when the tempo changes after downsampling\n",
    "        \"use_rests\": True,\n",
    "    }\n",
    "    tokenizer_confg = TokenizerConfig(**TOKENIZER_PARAMS)\n",
    "    tokenizer = REMI(tokenizer_confg)\n",
    "    print(f\"Untrained token count: {tokenizer.len}\")\n",
    "    tokenizer.train(vocab_size=VOCAB_SIZE, files_paths=midi_file_paths)\n",
    "    tokenizer.save(tokenizer_save_path)\n",
    "else:\n",
    "    tokenizer = REMI(params=tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split files into train / test / validation folder when we chunk them, so no requirement to set seed here\n",
    "# random.seed(42)\n",
    "\n",
    "random.shuffle(midi_file_paths)\n",
    "len(midi_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = int(0.8 * len(midi_file_paths))\n",
    "n2 = int(0.9 * len(midi_file_paths))\n",
    "train_filepaths = midi_file_paths[:n1]\n",
    "valid_filepaths = midi_file_paths[n1:n2]\n",
    "test_filepaths = midi_file_paths[n2:]\n",
    "\n",
    "print(f'Train files: {len(train_filepaths)}, Valid files: {len(valid_filepaths)}, Test files: {len(test_filepaths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_path = Path(f'{midi_path}/mtok_split/{dataset_name}/v-{VOCAB_SIZE}_t-{TOKENIZER_CONFIG}_c-{CHUNK_LENGTH}')\n",
    "train_chunk_path = Path(f'{chunk_path}/train')\n",
    "valid_chunk_path = Path(f'{chunk_path}/valid')\n",
    "test_chunk_path = Path(f'{chunk_path}/test')\n",
    "\n",
    "split_data = [\n",
    "    (train_filepaths, train_chunk_path),\n",
    "    (valid_filepaths, valid_chunk_path),\n",
    "    (test_filepaths, test_chunk_path)\n",
    "]\n",
    "\n",
    "def chunk_files(filepaths, tokenizer, chunks_dir, max_seq_len):\n",
    "    split_files_for_training(\n",
    "        files_paths=filepaths,\n",
    "        tokenizer=tokenizer,\n",
    "        save_dir=chunks_dir,\n",
    "        max_seq_len=max_seq_len,\n",
    "        num_overlap_bars=1\n",
    "    )\n",
    "    augment_dataset(\n",
    "        chunks_dir,\n",
    "        pitch_offsets=[-12, 12],\n",
    "        velocity_offsets=[-4, 4],\n",
    "        duration_offsets=[-0.5, 0.5],\n",
    "    )\n",
    "\n",
    "if not chunk_path.exists():\n",
    "    with Pool(processes=3) as pool:\n",
    "        pool.starmap(chunk_files, [(filepaths, tokenizer, chunks_dir, CHUNK_LENGTH) for filepaths, chunks_dir in split_data])\n",
    "\n",
    "# for filepaths, chunks_dir in split_data:\n",
    "#     chunk_files(filepaths, tokenizer, chunks_dir, CHUNK_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chunk_filepaths = list(train_chunk_path.glob(\"**/*.mid\"))\n",
    "valid_chunk_filepaths = list(valid_chunk_path.glob(\"**/*.mid\"))\n",
    "test_chunk_filepaths = list(test_chunk_path.glob(\"**/*.mid\"))\n",
    "\n",
    "print(f'Train chunks: {len(train_chunk_filepaths)}, Valid chunks: {len(valid_chunk_filepaths)}, Test chunks: {len(test_chunk_filepaths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_paths = [train_chunk_filepaths, valid_chunk_filepaths, test_chunk_filepaths]\n",
    "\n",
    "for chunk_path in chunk_paths:\n",
    "    random.shuffle(chunk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(chunks_path, tokenizer, max_seq_len, batch_size):\n",
    "    collator = DataCollator(tokenizer.pad_token_id) # copy_inputs_as_labels and shift_labels not needed as done by the transformer\n",
    "    dataset = DatasetMIDI(\n",
    "        pre_tokenize=False,\n",
    "        files_paths=chunks_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_len=max_seq_len,\n",
    "        bos_token_id=tokenizer[\"BOS_None\"],\n",
    "        eos_token_id=tokenizer[\"EOS_None\"])\n",
    "    return DataLoader(dataset=dataset, collate_fn=collator, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "train_loader, valid_loader, test_loader = map(\n",
    "    lambda chunk_filepaths: cycle(create_data_loader(chunk_filepaths, tokenizer, CHUNK_LENGTH, BATCH_SIZE)),\n",
    "    chunk_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'memorizing_miditok_{dataset_name}_t-{TIMESTEPS}_v-{VOCAB_SIZE}_{VERSION_LABEL}'\n",
    "model_load_path = Path(f'../data/checkpoints/{model_name}.dat')\n",
    "model_save_path = Path(f'../data/checkpoints/{model_name}.dat')\n",
    "log_dir = Path(f'../tensorboard/{model_name}')\n",
    "tensorboard_writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a transformer and set up our training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MemorizingTransformer(\n",
    "    num_tokens = VOCAB_SIZE,\n",
    "    dim = N_EMBED,\n",
    "    depth = N_LAYER,\n",
    "    heads = N_HEAD,\n",
    "    dim_head = DIM_HEAD,\n",
    "    attn_dropout = 0.2,\n",
    "    ff_dropout = 0.2,\n",
    "    memorizing_layers = (4, 5),\n",
    "    max_knn_memories = CHUNK_LENGTH, # No point in having more memories than the chunk length as we clear them at the end of each chunk\n",
    "    num_retrieved_memories = 32, # Top K\n",
    "    xl_memory_layers = (2, 3, 4, 5),\n",
    "    xl_max_memories = TIMESTEPS, # One context-length of XL memory\n",
    "    pad_id = tokenizer.pad_token_id,\n",
    "    shift_knn_memories_down = 1,\n",
    "    shift_xl_memories_down = 1\n",
    ").to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise embeddings\n",
    "\n",
    "The following code was suggested by Claude. It uses T-SNE to visualise embeddings, clustered in a 2D plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For visualizing embeddings from a trained model\n",
    "def visualize_model_embeddings(model, tokenizer):\n",
    "    \n",
    "    all_ids = torch.tensor(list(tokenizer.vocab.values())).to(device)\n",
    "    all_ids_cpu = torch.tensor(list(tokenizer.vocab.values()))\n",
    "    \n",
    "    # Get embeddings from model\n",
    "    with torch.no_grad():\n",
    "        # Assuming model.wte is the token embedding layer (adjust as needed)\n",
    "        embeddings = model.token_emb(all_ids).detach().cpu().numpy()\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, perplexity=min(30, len(all_ids_cpu)-1))\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # Create categorical color mapping based on token types\n",
    "    # You can customize this based on your tokenizer's vocabulary\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=all_ids_cpu, cmap='tab20', alpha=0.7)\n",
    "    plt.colorbar(label='Token ID')\n",
    "    plt.title('Model Embeddings t-SNE Projection')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "def save_checkpoint(optimizer, completed_iterations, train_loss, val_loss):\n",
    "    tensorboard_writer.add_scalar('Loss/train', train_loss, completed_iterations)\n",
    "    tensorboard_writer.add_scalar('Loss/val', val_loss, completed_iterations)\n",
    "    clear_output(wait=True)\n",
    "    print(f'Writing to Tensorboard: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    torch.save({\n",
    "        'iter': completed_iterations,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, model_save_path)\n",
    "    visualize_model_embeddings(model, tokenizer)\n",
    "\n",
    "completed_iterations = 0\n",
    "if model_load_path.exists():\n",
    "    checkpoint = torch.load(model_load_path)\n",
    "    completed_iterations = checkpoint['iter']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print(f\"Loaded model from iteration {completed_iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(range(NUM_BATCHES - completed_iterations), mininterval = 10., desc = 'training'):\n",
    "    model.train()\n",
    "\n",
    "    data = next(train_loader)[\"input_ids\"].to(device)\n",
    "    if data.shape[0] != BATCH_SIZE:\n",
    "        print(f'Skipping batch {i} as it is not of size {BATCH_SIZE}, but {data.shape[0]}')\n",
    "        data = next(train_loader)[\"input_ids\"].to(device)\n",
    "\n",
    "    train_loss = 0.\n",
    "    with model.knn_memories_context(batch_size = BATCH_SIZE) as knn_memories:\n",
    "        xl_memories = None    \n",
    "        seq, labels = data[:, :-1], data[:, 1:]\n",
    "\n",
    "        for seq_segment, labels_segment in zip(seq.chunk(SEGMENTS, dim = -1), labels.chunk(SEGMENTS, dim = -1)):\n",
    "            loss, xl_memories = model(\n",
    "                seq_segment,\n",
    "                labels = labels_segment,\n",
    "                knn_memories = knn_memories,\n",
    "                xl_memories = xl_memories\n",
    "            )\n",
    "\n",
    "            train_loss += loss.item() / SEGMENTS\n",
    "            (loss / SEGMENTS).backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_CLIP_NORM)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if not (i % VALIDATE_EVERY):\n",
    "        model.eval()\n",
    "\n",
    "        valid_data = next(valid_loader)[\"input_ids\"].to(device)\n",
    "        if valid_data.shape[0] != BATCH_SIZE:\n",
    "            print(f'Skipping validation batch {i} as it is not of size {BATCH_SIZE}, but {valid_data.shape[0]}')\n",
    "            valid_data = next(valid_loader)[\"input_ids\"].to(device)\n",
    "\n",
    "        valid_loss = 0.\n",
    "\n",
    "        with torch.no_grad(), model.knn_memories_context(batch_size = BATCH_SIZE) as knn_memories:\n",
    "            xl_memories = None    \n",
    "            seq, labels = data[:, :-1], data[:, 1:]\n",
    "\n",
    "            for seq_segment, labels_segment in zip(seq.chunk(SEGMENTS, dim = -1), labels.chunk(SEGMENTS, dim = -1)):\n",
    "                loss, xl_memories = model(\n",
    "                    seq_segment,\n",
    "                    labels = labels_segment,\n",
    "                    knn_memories = knn_memories,\n",
    "                    xl_memories = xl_memories\n",
    "                )\n",
    "\n",
    "                valid_loss += loss.item() / SEGMENTS\n",
    "\n",
    "        save_checkpoint(optimizer, i + completed_iterations, train_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Results\n",
    "\n",
    "With the base REMI vocab size of 411 tokens along with the same embed / head / layer count as our hand built model, the results are significantly worse. The training curve almost immediately flattened out at 1.6 loss, compared to under 1.0 previously.\n",
    "\n",
    "This could be due to e.g.\n",
    "\n",
    "- Different encoding. Not only are tokens different in MidiTok / Remi but we used bar / beat *embeddings* rather than tokens previously. There are so many more meanings to the tokens in addition to the count rather than just note / duration / sos / eos / pad that it would be much harder to guess the right answer. If we do see convergence it should mean we have learnt more useful information rather than just gamed the system so to speak.\n",
    "- Larger vocabulary = more choices to pick for the next token\n",
    "- The MIDITok data loader only provides max `CHUNK_LENGTH` tokens, whereas our custom 'contiguous' data loader always fed in entire tracks. Only a handful of tracks are between 1 and 2k tokens, most are 3 - 5K, so are being split into multiple pieces.\n",
    "- The LucidRains Memorizing Transformers architecture is very similar to our model but there are a couple of changes (memory on a subset of layer vs all, lack of absolute embeddings) and of course it is a different implementation. I would expect it to be less buggy if anything though.\n",
    "\n",
    "I did have to tweak the transformer's KNN memory to run exclusively on the GPU as index was on CPU and the data on disk, which was a major bottleneck (as found in our model). Luckily once again the code was nearly identical so it was easy to follow / edit.\n",
    "\n",
    "## Things to try\n",
    "\n",
    "- Train the tokenizer with 1K, 5K, 10K, 20K, 30K size vocab. This should result in less tokens per song. We might need to drop the max seq length to avoid over-padded songs.\n",
    "\n",
    "- Add in chord / tempo / time sig / rest tokens. This should result in more tokens per song. We might need to increase the max seq length to avoid over-split songs.\n",
    "\n",
    "> Note - the files will need re-encoding as they are split based on estimated token length, which depends on encoding and vocab size!\n",
    "\n",
    "> Another note! - When the vocab size was halved, the number of tokens per file seemed to double, as you might expect given the bpe was trained on the same data. That meant doubling the chunk size and segment count in order to split the files into the same number of pieces.\n",
    "\n",
    "- Augment the dataset with pitch and velocity shifted versions using the MIDITok tools\n",
    "\n",
    "- Full Lakh dataset, although if we can't do well on a curated, stylistic dataset then a more varied one is likely to decrease performance.\n",
    "\n",
    "\n",
    "## Experiment log\n",
    "\n",
    "- Was seeing increase in loss after initial drop which eventually came back down again. Decreasing learning rate made it happen later and worse. Increased learning rate (and added dropout) which seemed to negate it.\n",
    "\n",
    "- Some apparent cyclic loss patterns so tried shuffling between epochs. Requires either waiting for a pretokenise op between epochs (long even on vg_large, let alone Lakh) or tokenise on the fly which reduces GPU usage to around 70%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLHF\n",
    "\n",
    "For non-verifiable domains we can train a model to mimic the preferences of humans.\n",
    "\n",
    "First we generate n continuations and get a human to rank them. Secondly, we train a separate NN to generate scores for each continuation and compare them to the rankings, then nudge the weights appropriately. Eventually we will have an automated preference model, which we can use to fine tune our generator at scale.\n",
    "\n",
    "NOTE - with non-concrete answers (i.e. a preference model rather than known answers) you can't train indefinitely as you will see degrading performance. The model finds ways to game the system, getting high scores for obscure results by finding gaps in the reward model (which is by definition a very simplified, limited version of a human's opinion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More experiments\n",
    "\n",
    "- Increasing vocab with BPE decreases performance, likely due to sparsity of occurences of any new tokens given the small data set.\n",
    "\n",
    "- Adding Chord tokens significantly increased perf\n",
    "\n",
    "- Adding Tempo tokens raised the vocab size but only slightly decreased perf from Chord only\n",
    "\n",
    "- To try\n",
    "    - Time signature tokens = better handling of non-4/4\n",
    "    - Tempo / time sig optimisations (remove duplicates / only insert when change) = Less wasted tokens in context window\n",
    "    - Use bigger chunks = compensate for longer sequences caused by chord / time sig / tempo tokens, more memories maintained in KNN db\n",
    "    - Rest tokens - better handling of silence\n",
    "    - Memorizing Transformers tweaks (e.g. shift mems down a layer, increase / decrease top K)\n",
    "    - Lakh dataset = much more variation, and the improved tokenizer has a better chance of capturing it - no need to convert to piano, handles drums and all the other metrics discussed above etc.\n",
    "    - Try BPE with Lakh dataset. Much larger example size could actually find more genuinely common token merges.\n",
    "    - Increase model param count (embed size, num layers, heads per layer etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_basic_path = Path(f'../data/vocab/MidiTok/vg_large_411_Basic.json')\n",
    "basic_tokenizer = REMI(params=tokenizer_basic_path)\n",
    "\n",
    "basic_model_name = f'memorizing_miditok_vg_large_t-512_v-411_1'\n",
    "basic_model_load_path = Path(f'../data/checkpoints/{basic_model_name}.dat')\n",
    "basic_model = MemorizingTransformer(\n",
    "    num_tokens = 411,\n",
    "    dim = N_EMBED,\n",
    "    depth = N_LAYER,\n",
    "    heads = N_HEAD,\n",
    "    dim_head = DIM_HEAD,\n",
    "    attn_dropout = 0.2,\n",
    "    ff_dropout = 0.2,\n",
    "    memorizing_layers = (4, 5),\n",
    "    max_knn_memories = CHUNK_LENGTH, # No point in having more meories than the chunk length as we clear them at the end of each chunk\n",
    "    num_retrieved_memories = 32, # Top K\n",
    "    xl_memory_layers = (2, 3, 4, 5),\n",
    "    xl_max_memories = TIMESTEPS, # One context-length of XL memory\n",
    "    pad_id = tokenizer.pad_token_id,)\n",
    "\n",
    "basic_model.load_state_dict(torch.load(basic_model_load_path)['model_state_dict'])\n",
    "\n",
    "visualize_model_embeddings(basic_model, basic_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_chords_path = Path(f'../data/vocab/MidiTok/vg_large_425_Chords.json')\n",
    "chords_tokenizer = REMI(params=tokenizer_chords_path)\n",
    "\n",
    "chords_model_name = f'memorizing_miditok_vg_large_t-512_v-425_Chords'\n",
    "chords_model_load_path = Path(f'../data/checkpoints/{chords_model_name}.dat')\n",
    "chords_model = MemorizingTransformer(\n",
    "    num_tokens = 425,\n",
    "    dim = N_EMBED,\n",
    "    depth = N_LAYER,\n",
    "    heads = N_HEAD,\n",
    "    dim_head = DIM_HEAD,\n",
    "    attn_dropout = 0.2,\n",
    "    ff_dropout = 0.2,\n",
    "    memorizing_layers = (4, 5),\n",
    "    max_knn_memories = CHUNK_LENGTH, # No point in having more meories than the chunk length as we clear them at the end of each chunk\n",
    "    num_retrieved_memories = 32, # Top K\n",
    "    xl_memory_layers = (2, 3, 4, 5),\n",
    "    xl_max_memories = TIMESTEPS, # One context-length of XL memory\n",
    "    pad_id = tokenizer.pad_token_id,)\n",
    "\n",
    "chords_model.load_state_dict(torch.load(chords_model_load_path)['model_state_dict'])\n",
    "\n",
    "visualize_model_embeddings(chords_model, chords_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chords and Tempos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_chords_tempos_path = Path(f'../data/vocab/MidiTok/vg_large_457_Chords_Tempos.json')\n",
    "chords_tempos_tokenizer = REMI(params=tokenizer_chords_tempos_path)\n",
    "\n",
    "chords_tempos_model_name = f'memorizing_miditok_vg_large_t-512_v-457_Chords_Tempos'\n",
    "chords_tempos_model_load_path = Path(f'../data/checkpoints/{chords_tempos_model_name}.dat')\n",
    "chords_tempos_model = MemorizingTransformer(\n",
    "    num_tokens = 457,\n",
    "    dim = N_EMBED,\n",
    "    depth = N_LAYER,\n",
    "    heads = N_HEAD,\n",
    "    dim_head = DIM_HEAD,\n",
    "    attn_dropout = 0.2,\n",
    "    ff_dropout = 0.2,\n",
    "    memorizing_layers = (4, 5),\n",
    "    max_knn_memories = CHUNK_LENGTH, # No point in having more meories than the chunk length as we clear them at the end of each chunk\n",
    "    num_retrieved_memories = 32, # Top K\n",
    "    xl_memory_layers = (2, 3, 4, 5),\n",
    "    xl_max_memories = TIMESTEPS, # One context-length of XL memory\n",
    "    pad_id = tokenizer.pad_token_id,)\n",
    "\n",
    "chords_tempos_model.load_state_dict(torch.load(chords_tempos_model_load_path)['model_state_dict'])\n",
    "\n",
    "visualize_model_embeddings(chords_tempos_model, chords_tempos_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chords, Tempos and Time Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_chords_tempos_time_sig_path = Path(f'../data/vocab/MidiTok/vg_large_530_Chords_Tempos_TimeSig.json')\n",
    "chords_tempos_time_sig_tokenizer = REMI(params=tokenizer_chords_tempos_time_sig_path)\n",
    "\n",
    "chords_tempos_time_sig_model_name = f'memorizing_miditok_vg_large_t-512_v-530_Chords_Tempos_TimeSig'\n",
    "chords_tempos_time_sig_model_load_path = Path(f'../data/checkpoints/{chords_tempos_time_sig_model_name}.dat')\n",
    "chords_tempos_time_sig_model = MemorizingTransformer(\n",
    "    num_tokens = 530,\n",
    "    dim = N_EMBED,\n",
    "    depth = N_LAYER,\n",
    "    heads = N_HEAD,\n",
    "    dim_head = DIM_HEAD,\n",
    "    attn_dropout = 0.2,\n",
    "    ff_dropout = 0.2,\n",
    "    memorizing_layers = (4, 5),\n",
    "    max_knn_memories = CHUNK_LENGTH, # No point in having more meories than the chunk length as we clear them at the end of each chunk\n",
    "    num_retrieved_memories = 32, # Top K\n",
    "    xl_memory_layers = (2, 3, 4, 5),\n",
    "    xl_max_memories = TIMESTEPS, # One context-length of XL memory\n",
    "    pad_id = tokenizer.pad_token_id,)\n",
    "\n",
    "chords_tempos_time_sig_model.load_state_dict(torch.load(chords_tempos_time_sig_model_load_path)['model_state_dict'])\n",
    "\n",
    "visualize_model_embeddings(chords_tempos_time_sig_model, chords_tempos_time_sig_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chords, Tempos, Time Signatures and Rests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_chords_tempos_time_sig_rests_path = Path(f'../data/vocab/MidiTok/vg_large_562_Chords_Tempos_TimeSig_Rests.json')\n",
    "chords_tempos_time_sig_rests_tokenizer = REMI(params=tokenizer_chords_tempos_time_sig_rests_path)\n",
    "\n",
    "chords_tempos_time_sig_rests_model_name = f'memorizing_miditok_vg_large_t-512_v-562_Chords_Tempos_TimeSig_Rests'\n",
    "chords_tempos_time_sig_rests_model_load_path = Path(f'../data/checkpoints/{chords_tempos_time_sig_rests_model_name}.dat')\n",
    "chords_tempos_time_sig_rests_model = MemorizingTransformer(\n",
    "    num_tokens = 562,\n",
    "    dim = N_EMBED,\n",
    "    depth = N_LAYER,\n",
    "    heads = N_HEAD,\n",
    "    dim_head = DIM_HEAD,\n",
    "    attn_dropout = 0.2,\n",
    "    ff_dropout = 0.2,\n",
    "    memorizing_layers = (4, 5),\n",
    "    max_knn_memories = CHUNK_LENGTH, # No point in having more meories than the chunk length as we clear them at the end of each chunk\n",
    "    num_retrieved_memories = 32, # Top K\n",
    "    xl_memory_layers = (2, 3, 4, 5),\n",
    "    xl_max_memories = TIMESTEPS, # One context-length of XL memory\n",
    "    pad_id = tokenizer.pad_token_id,)\n",
    "\n",
    "chords_tempos_time_sig_rests_model.load_state_dict(torch.load(chords_tempos_time_sig_rests_model_load_path)['model_state_dict'])\n",
    "\n",
    "visualize_model_embeddings(chords_tempos_time_sig_rests_model, chords_tempos_time_sig_rests_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chords, Tempos, Time Signatures and Rests (Optimised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_chords_tempos_time_sig_rests_optimized_path = Path(f'../data/vocab/MidiTok/vg_large_562_Chords_Tempos_TimeSig_Rests_Optimized.json')\n",
    "chords_tempos_time_sig_rests_optimized_tokenizer = REMI(params=tokenizer_chords_tempos_time_sig_rests_optimized_path)\n",
    "\n",
    "chords_tempos_time_sig_rests_optimized_model_name = f'memorizing_miditok_vg_large_t-512_v-562_Chords_Tempos_TimeSig_Rests_Optimized'\n",
    "chords_tempos_time_sig_rests_optimized_model_load_path = Path(f'../data/checkpoints/{chords_tempos_time_sig_rests_optimized_model_name}.dat')\n",
    "chords_tempos_time_sig_rests_optimized_model = MemorizingTransformer(\n",
    "    num_tokens = 562,\n",
    "    dim = N_EMBED,\n",
    "    depth = N_LAYER,\n",
    "    heads = N_HEAD,\n",
    "    dim_head = DIM_HEAD,\n",
    "    attn_dropout = 0.2,\n",
    "    ff_dropout = 0.2,\n",
    "    memorizing_layers = (4, 5),\n",
    "    max_knn_memories = CHUNK_LENGTH, # No point in having more meories than the chunk length as we clear them at the end of each chunk\n",
    "    num_retrieved_memories = 32, # Top K\n",
    "    xl_memory_layers = (2, 3, 4, 5),\n",
    "    xl_max_memories = TIMESTEPS, # One context-length of XL memory\n",
    "    pad_id = tokenizer.pad_token_id,)\n",
    "\n",
    "chords_tempos_time_sig_rests_optimized_model.load_state_dict(torch.load(chords_tempos_time_sig_rests_optimized_model_load_path)['model_state_dict'])\n",
    "\n",
    "visualize_model_embeddings(chords_tempos_time_sig_rests_optimized_model, chords_tempos_time_sig_rests_optimized_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chords with Roots, Tempos, Time Signatures and Rests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_chordroots_tempos_time_sig_rests_path = Path(f'../data/vocab/MidiTok/vg_large_716_ChordsRoots_Tempos_TimeSig_Rests_Optimized.json')\n",
    "chordroots_tempos_time_sig_rests_tokenizer = REMI(params=tokenizer_chordroots_tempos_time_sig_rests_path)\n",
    "chordroots_tempos_time_sig_rests_model_name = f'memorizing_miditok_vg_large_t-512_v-716_ChordsRoots_Tempos_TimeSig_Rests_Optimized'\n",
    "chordroots_tempos_time_sig_rests_model_load_path = Path(f'../data/checkpoints/{chordroots_tempos_time_sig_rests_model_name}.dat')\n",
    "chordroots_tempos_time_sig_rests_model = MemorizingTransformer(\n",
    "    num_tokens = 716,\n",
    "    dim = N_EMBED,\n",
    "    depth = N_LAYER,\n",
    "    heads = N_HEAD,\n",
    "    dim_head = DIM_HEAD,\n",
    "    attn_dropout = 0.2,\n",
    "    ff_dropout = 0.2,\n",
    "    memorizing_layers = (4, 5),\n",
    "    max_knn_memories = CHUNK_LENGTH, # No point in having more meories than the chunk length as we clear them at the end of each chunk\n",
    "    num_retrieved_memories = 32, # Top K\n",
    "    xl_memory_layers = (2, 3, 4, 5),\n",
    "    xl_max_memories = TIMESTEPS, # One context-length of XL memory\n",
    "    pad_id = tokenizer.pad_token_id,)\n",
    "\n",
    "chordroots_tempos_time_sig_rests_model.load_state_dict(torch.load(chordroots_tempos_time_sig_rests_model_load_path)['model_state_dict'])\n",
    "\n",
    "visualize_model_embeddings(chordroots_tempos_time_sig_rests_model, chordroots_tempos_time_sig_rests_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_path = random.choice(test_chunk_filepaths)\n",
    "random_train_data = tokenizer.encode(random_test_path)\n",
    "print(f'Random test file: {random_test_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symusic import Synthesizer\n",
    "synth = Synthesizer()\n",
    "\n",
    "random_start_tokens = random_train_data.tokens[:TIMESTEPS] # TODO: Think about checking / enforcing the correct length\n",
    "random_start_ids = random_train_data.ids[:TIMESTEPS]\n",
    "\n",
    "random_start_decoded = tokenizer.decode(random_start_tokens)\n",
    "audio = synth.render(random_start_decoded)\n",
    "IPython.display.Audio(audio, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "input_ids = torch.tensor([random_start_ids]).to(device)\n",
    "\n",
    "generated = model.generate(\n",
    "        input_ids = input_ids,\n",
    "        max_length = CHUNK_LENGTH,\n",
    "        eos_token_id = tokenizer[\"EOS_None\"]).detach().cpu()[0]\n",
    "    \n",
    "score = tokenizer.decode(generated)\n",
    "generated_decoded = tokenizer.decode(generated)\n",
    "audio = synth.render(generated_decoded)\n",
    "IPython.display.Audio(audio, rate=44100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
