{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'9 - Rebuild')\n",
    "import torch\n",
    "from miditok import REMI, TokenizerConfig  # here we choose to use REMI\n",
    "from pathlib import Path\n",
    "import random\n",
    "from miditok.utils import split_files_for_training\n",
    "from miditok.data_augmentation import augment_dataset\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from torch.utils.data import DataLoader\n",
    "from miditok import TokSequence\n",
    "from multiprocessing import Pool\n",
    "from memorizing_transformers_pytorch import MemorizingTransformer\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cuda\":\n",
    "    print(f\"Device: {torch.cuda.get_device_name()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memorizing Transformers\n",
    "\n",
    "From our MIDITok research, we know that we we want to \n",
    "\n",
    "- Train a BPE tokenizer on the entire dataset.\n",
    "- Save it / Load it (BPE is deterministic so data doesn't need to be decoded with a the same tokenizer it was encoded with, providing whatever is used was trained on the same data with the same config. Unigram is *not* deterministic however so would require the same exact tokenizer for encode / decode).\n",
    "- Shuffle file names.\n",
    "- Split into test / train / validation sets, so songs aren't biased to a set.\n",
    "- Split the files into chunks for each set.\n",
    "- Optionally augment the dataset with pitch / velocity / duration shifted versions\n",
    "- Shuffle the chunks when loading, so that parts of a single song aren't biased to a batch.\n",
    "- Load the chunks with `max_seq_len` equal to that used when splitting files, to minimise padding / truncated data.\n",
    "- Split the chunks into context-length sequences and feed them through contiguously.\n",
    "- Manually reset memories between chunks rather than auto-reset on BOS / EOS tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_LENGTH = 1024\n",
    "SEGMENTS = 4\n",
    "BATCH_SIZE = 16\n",
    "VOCAB_SIZE = 1000\n",
    "N_EMBED = 512\n",
    "N_LAYER = 8\n",
    "N_HEAD = 8\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-3\n",
    "NUM_BATCHES = int(1e5)\n",
    "MAX_GRAD_CLIP_NORM = 0.5\n",
    "VALIDATE_EVERY  = 40\n",
    "GENERATE_EVERY  = 500\n",
    "GENERATE_LENGTH = 512\n",
    "TIMESTEPS = CHUNK_LENGTH // SEGMENTS\n",
    "DIM_HEAD = N_EMBED // N_HEAD\n",
    "\n",
    "midi_path = Path(f'../data/midi')\n",
    "dataset_name = 'vg_large'\n",
    "midi_dataset_path = Path(f'{midi_path}/{dataset_name}')\n",
    "midi_file_paths = [p.resolve() for p in midi_dataset_path.glob(\"**/*.mid\")]\n",
    "\n",
    "tokenizer_save_path = Path(f'../data/vocab/MidiTok/{dataset_name}.json')\n",
    "\n",
    "if not tokenizer_save_path.exists():\n",
    "    TOKENIZER_PARAMS = {\n",
    "        \"pitch_range\": (21, 109),\n",
    "        \"beat_res\": {(0, 4): 8, (4, 12): 4},\n",
    "        \"num_velocities\": 32,\n",
    "        \"use_programs\": True\n",
    "        # \"use_chords\": True,\n",
    "        # \"use_time_signatures\": True,\n",
    "        # \"use_tempos\": True,\n",
    "        # \"num_tempos\": 32,  # number of tempo bins\n",
    "        # \"tempo_range\": (40, 250)\n",
    "    }\n",
    "    tokenizer_confg = TokenizerConfig(**TOKENIZER_PARAMS)\n",
    "    tokenizer = REMI(tokenizer_confg)\n",
    "    tokenizer.train(vocab_size=VOCAB_SIZE, files_paths=midi_file_paths)\n",
    "    tokenizer.save(tokenizer_save_path)\n",
    "else:\n",
    "    tokenizer = REMI(params=tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(midi_file_paths)\n",
    "len(midi_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = int(0.8 * len(midi_file_paths))\n",
    "n2 = int(0.9 * len(midi_file_paths))\n",
    "train_filepaths = midi_file_paths[:n1]\n",
    "valid_filepaths = midi_file_paths[n1:n2]\n",
    "test_filepaths = midi_file_paths[n2:]\n",
    "\n",
    "print(f'Train files: {len(train_filepaths)}, Valid files: {len(valid_filepaths)}, Test files: {len(test_filepaths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_path = Path(f'{midi_path}/{dataset_name}_miditok_split')\n",
    "train_chunk_path = Path(f'{chunk_path}/train')\n",
    "valid_chunk_path = Path(f'{chunk_path}/valid')\n",
    "test_chunk_path = Path(f'{chunk_path}/test')\n",
    "\n",
    "split_data = [\n",
    "    (train_filepaths, train_chunk_path),\n",
    "    (valid_filepaths, valid_chunk_path),\n",
    "    (test_filepaths, test_chunk_path)\n",
    "]\n",
    "\n",
    "def chunk_files(filepaths, tokenizer, chunks_dir, max_seq_len):\n",
    "    split_files_for_training(\n",
    "        files_paths=filepaths,\n",
    "        tokenizer=tokenizer,\n",
    "        save_dir=chunks_dir,\n",
    "        max_seq_len=max_seq_len,\n",
    "        num_overlap_bars=1\n",
    "    )\n",
    "\n",
    "if not chunk_path.exists():\n",
    "    with Pool(processes=3) as pool:\n",
    "        pool.starmap(chunk_files, [(filepaths, tokenizer, chunks_dir, CHUNK_LENGTH) for filepaths, chunks_dir in split_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chunk_filepaths = list(train_chunk_path.glob(\"**/*.mid\"))\n",
    "valid_chunk_filepaths = list(valid_chunk_path.glob(\"**/*.mid\"))\n",
    "test_chunk_filepaths = list(test_chunk_path.glob(\"**/*.mid\"))\n",
    "\n",
    "print(f'Train chunks: {len(train_chunk_filepaths)}, Valid chunks: {len(valid_chunk_filepaths)}, Test chunks: {len(test_chunk_filepaths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_paths = [train_chunk_filepaths, valid_chunk_filepaths, test_chunk_filepaths]\n",
    "\n",
    "for chunk_path in chunk_paths:\n",
    "    random.shuffle(chunk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(chunks_path, tokenizer, max_seq_len, batch_size):\n",
    "    collator = DataCollator(tokenizer.pad_token_id) # copy_inputs_as_labels and shift_labels not needed as done by the transformer\n",
    "    dataset = DatasetMIDI(\n",
    "        pre_tokenize=True,\n",
    "        files_paths=chunks_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_len=max_seq_len,\n",
    "        bos_token_id=tokenizer[\"BOS_None\"],\n",
    "        eos_token_id=tokenizer[\"EOS_None\"])\n",
    "    return DataLoader(dataset=dataset, collate_fn=collator, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "train_loader, valid_loader, test_loader = map(\n",
    "    lambda chunk_filepaths: cycle(create_data_loader(chunk_filepaths, tokenizer, CHUNK_LENGTH, BATCH_SIZE)),\n",
    "    chunk_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'memorizing_miditok_{dataset_name}'\n",
    "model_load_path = Path(f'../data/checkpoints/{model_name}.dat')\n",
    "model_save_path = Path(f'../data/checkpoints/{model_name}.dat')\n",
    "log_dir = Path(f'../tensorboard/{model_name}')\n",
    "tensorboard_writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a transformer and set up our training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MemorizingTransformer(\n",
    "    num_tokens = VOCAB_SIZE,\n",
    "    dim = N_EMBED,\n",
    "    depth = N_LAYER,\n",
    "    heads = N_HEAD,\n",
    "    dim_head = DIM_HEAD,\n",
    "    memorizing_layers = (4, 5),\n",
    "    max_knn_memories = CHUNK_LENGTH, # No point in having more meories than the chunk length as we clear them at the end of each chunk\n",
    "    num_retrieved_memories = 32, # Top K\n",
    "    xl_memory_layers = (2, 3, 4, 5),\n",
    "    xl_max_memories = TIMESTEPS, # One context-length of XL memory\n",
    "    clear_memories_on_sos_token_id = 0, # We know we will never have mixed songs in a sequence\n",
    "    # shift_knn_memories_down = 1,\n",
    "    # shift_xl_memories_down = 1\n",
    ").to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(optimizer, completed_iterations, train_loss, val_loss):\n",
    "    tensorboard_writer.add_scalar('Loss/train', train_loss, completed_iterations)\n",
    "    tensorboard_writer.add_scalar('Loss/val', val_loss, completed_iterations)\n",
    "    print(f'Writing to Tensorboard: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    torch.save({\n",
    "        'iter': completed_iterations,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, model_save_path)\n",
    "\n",
    "completed_iterations = 0\n",
    "if model_load_path.exists():\n",
    "    checkpoint = torch.load(model_load_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    completed_iterations = checkpoint['iter']\n",
    "    print(f\"Loaded model from iteration {completed_iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.7521767020225525\n",
      "training loss: 3.762714684009552\n",
      "training loss: 3.8888025879859924\n",
      "training loss: 3.6545620560646057\n",
      "training loss: 3.669543504714966\n",
      "training loss: 3.6860474348068237\n",
      "training loss: 3.755688786506653\n",
      "valid loss: 3.715519368648529\n",
      "Writing to Tensorboard: Train Loss: 3.7557, Val Loss: 3.7155\n",
      "training loss: 3.6279872059822083\n",
      "training loss: 3.7105730772018433\n",
      "training loss: 3.508239269256592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|          | 365/100000 [01:49<8:16:04,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 3.755535066127777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m seq, labels \u001b[38;5;241m=\u001b[39m data[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], data[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq_segment, labels_segment \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(seq\u001b[38;5;241m.\u001b[39mchunk(SEGMENTS, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), labels\u001b[38;5;241m.\u001b[39mchunk(SEGMENTS, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m---> 12\u001b[0m     loss, xl_memories \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_segment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels_segment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mknn_memories\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mknn_memories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxl_memories\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxl_memories\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m SEGMENTS\n\u001b[1;32m     20\u001b[0m     (loss \u001b[38;5;241m/\u001b[39m SEGMENTS)\u001b[38;5;241m.\u001b[39mbackward()    \n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/src/9 - Rebuild/memorizing_transformers_pytorch.py:461\u001b[0m, in \u001b[0;36mMemorizingTransformer.forward\u001b[0;34m(self, x, knn_memories, xl_memories, labels, add_knn_memory)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# if KNN memories are passed in, and researcher wants memories auto-cleared on <sos> token detection\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# do the appropriate logic\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_memories_on_sos_token_id):\n\u001b[0;32m--> 461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclear_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclear_memories_on_sos_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# handle XL memories\u001b[39;00m\n\u001b[1;32m    465\u001b[0m xl_memories \u001b[38;5;241m=\u001b[39m default(xl_memories, (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_xl_memory_layers)\n",
      "File \u001b[0;32m/src/9 - Rebuild/memorizing_transformers_pytorch.py:434\u001b[0m, in \u001b[0;36mMemorizingTransformer.clear_memory\u001b[0;34m(self, x, token_id)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" for auto-clearing KNN memories based on start and end of strings \"\"\"\u001b[39;00m\n\u001b[1;32m    433\u001b[0m clear_memory \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m==\u001b[39m token_id)\u001b[38;5;241m.\u001b[39many(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 434\u001b[0m batch_indices, _ \u001b[38;5;241m=\u001b[39m \u001b[43mclear_memory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mas_tuple\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m batch_indices_to_clear \u001b[38;5;241m=\u001b[39m batch_indices\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch_indices_to_clear) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(NUM_BATCHES - completed_iterations), mininterval = 10., desc = 'training'):\n",
    "    model.train()\n",
    "\n",
    "    data = next(train_loader)[\"input_ids\"].to(device)\n",
    "\n",
    "    train_loss = 0.\n",
    "    with model.knn_memories_context(batch_size = BATCH_SIZE) as knn_memories:\n",
    "        xl_memories = None    \n",
    "        seq, labels = data[:, :-1], data[:, 1:]\n",
    "\n",
    "        for seq_segment, labels_segment in zip(seq.chunk(SEGMENTS, dim = -1), labels.chunk(SEGMENTS, dim = -1)):\n",
    "            loss, xl_memories = model(\n",
    "                seq_segment,\n",
    "                labels = labels_segment,\n",
    "                knn_memories = knn_memories,\n",
    "                xl_memories = xl_memories\n",
    "            )\n",
    "\n",
    "            train_loss += loss.item() / SEGMENTS\n",
    "            (loss / SEGMENTS).backward()    \n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_CLIP_NORM)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if not (i % VALIDATE_EVERY):\n",
    "        model.eval()\n",
    "\n",
    "        valid_data = next(valid_loader)\n",
    "        valid_loss = 0.\n",
    "\n",
    "        with torch.no_grad(), model.knn_memories_context(batch_size = BATCH_SIZE) as knn_memories:\n",
    "            xl_memories = None    \n",
    "            seq, labels = data[:, :-1], data[:, 1:]\n",
    "\n",
    "            for seq_segment, labels_segment in zip(seq.chunk(SEGMENTS, dim = -1), labels.chunk(SEGMENTS, dim = -1)):\n",
    "                loss, xl_memories = model(\n",
    "                    seq_segment,\n",
    "                    labels = labels_segment,\n",
    "                    knn_memories = knn_memories,\n",
    "                    xl_memories = xl_memories\n",
    "                )\n",
    "\n",
    "                valid_loss += loss.item() / SEGMENTS\n",
    "\n",
    "        print(f'training loss: {train_loss}')\n",
    "        print(f'valid loss: {valid_loss}')\n",
    "        save_checkpoint(optimizer, i + completed_iterations, train_loss, valid_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
