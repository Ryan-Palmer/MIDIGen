{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'9 - Rebuild')\n",
    "import torch\n",
    "from miditok import REMI, TokenizerConfig  # here we choose to use REMI\n",
    "from pathlib import Path\n",
    "import random\n",
    "from miditok.utils import split_files_for_training\n",
    "from miditok.data_augmentation import augment_dataset\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from torch.utils.data import DataLoader\n",
    "from miditok import TokSequence\n",
    "from multiprocessing import Pool\n",
    "from memorizing_transformers_pytorch import MemorizingTransformer\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4090.\n"
     ]
    }
   ],
   "source": [
    "if device == \"cuda\":\n",
    "    print(f\"Device: {torch.cuda.get_device_name()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memorizing Transformers\n",
    "\n",
    "From our MIDITok research, we know that we we want to \n",
    "\n",
    "- Train a BPE tokenizer on the entire dataset.\n",
    "- Save it / Load it (BPE is deterministic so data doesn't need to be decoded with a the same tokenizer it was encoded with, providing whatever is used was trained on the same data with the same config. Unigram is *not* deterministic however so would require the same exact tokenizer for encode / decode).\n",
    "- Shuffle file names, so songs aren't biased to a set.\n",
    "- Split into test / train / validation sets.\n",
    "- Split the files into chunks for each set.\n",
    "- Optionally augment the dataset with pitch / velocity / duration shifted versions\n",
    "- Shuffle the chunks when loading, so that parts of a single song aren't biased to a batch.\n",
    "- Load the chunks with `max_seq_len` equal to that used when splitting files, to minimise padding / truncated data.\n",
    "- Split the chunks into context-length sequences and feed them through contiguously.\n",
    "- Manually reset memories between chunks rather than auto-reset on BOS / EOS tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/miditok/tokenizations/remi.py:88: UserWarning: Attribute controls are not compatible with 'config.one_token_stream_for_programs' and multi-vocabulary tokenizers. Disabling them from the config.\n",
      "  super().__init__(tokenizer_config, params)\n"
     ]
    }
   ],
   "source": [
    "CHUNK_LENGTH = 2048\n",
    "SEGMENTS = 8\n",
    "TIMESTEPS = CHUNK_LENGTH // SEGMENTS # Context length\n",
    "BATCH_SIZE = 16\n",
    "VOCAB_SIZE = 411 # REMI untrained token count is 411\n",
    "N_EMBED = 512\n",
    "N_LAYER = 8\n",
    "N_HEAD = 8\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-3\n",
    "NUM_BATCHES = int(1e5)\n",
    "MAX_GRAD_CLIP_NORM = 0.5\n",
    "VALIDATE_EVERY  = 64\n",
    "DIM_HEAD = N_EMBED // N_HEAD\n",
    "VERSION_LABEL = \"augmented\"\n",
    "TOKENIZER_CONFIG = 'Basic'\n",
    "\n",
    "midi_path = Path(f'../data/midi')\n",
    "dataset_name = 'vg_large'\n",
    "midi_dataset_path = Path(f'{midi_path}/{dataset_name}')\n",
    "midi_file_paths = [p.resolve() for p in midi_dataset_path.glob(\"**/*.mid\")]\n",
    "\n",
    "tokenizer_save_path = Path(f'../data/vocab/MidiTok/{dataset_name}_{VOCAB_SIZE}_{TOKENIZER_CONFIG}.json')\n",
    "\n",
    "if not tokenizer_save_path.exists():\n",
    "    TOKENIZER_PARAMS = {\n",
    "        \"pitch_range\": (21, 109),\n",
    "        \"beat_res\": {(0, 4): 8, (4, 12): 4},\n",
    "        \"num_velocities\": 32,\n",
    "        \"use_programs\": True\n",
    "        # \"use_chords\": True,\n",
    "        # \"use_time_signatures\": True,\n",
    "        # \"use_tempos\": True,\n",
    "        # \"num_tempos\": 32,  # number of tempo bins\n",
    "        # \"tempo_range\": (40, 250)\n",
    "    }\n",
    "    tokenizer_confg = TokenizerConfig(**TOKENIZER_PARAMS)\n",
    "    tokenizer = REMI(tokenizer_confg)\n",
    "    print(f\"Untrained token count: {tokenizer.len}\")\n",
    "    tokenizer.train(vocab_size=VOCAB_SIZE, files_paths=midi_file_paths)\n",
    "    tokenizer.save(tokenizer_save_path)\n",
    "else:\n",
    "    tokenizer = REMI(params=tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3839"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(midi_file_paths)\n",
    "len(midi_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 3071, Valid files: 384, Test files: 384\n"
     ]
    }
   ],
   "source": [
    "n1 = int(0.8 * len(midi_file_paths))\n",
    "n2 = int(0.9 * len(midi_file_paths))\n",
    "train_filepaths = midi_file_paths[:n1]\n",
    "valid_filepaths = midi_file_paths[n1:n2]\n",
    "test_filepaths = midi_file_paths[n2:]\n",
    "\n",
    "print(f'Train files: {len(train_filepaths)}, Valid files: {len(valid_filepaths)}, Test files: {len(test_filepaths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting music files (../data/midi/mtok_split/vg_large/v-411_t-Basic_c-2048/test): 100%|██████████| 384/384 [00:08<00:00, 46.93it/s]]\n",
      "Splitting music files (../data/midi/mtok_split/vg_large/v-411_t-Basic_c-2048/valid): 100%|██████████| 384/384 [00:08<00:00, 43.51it/s]\n",
      "Performing data augmentation: 100%|██████████| 943/943 [00:39<00:00, 23.99it/s]]in):  65%|██████▍   | 1989/3071 [00:34<00:17, 60.24it/s]\n",
      "Performing data augmentation: 100%|██████████| 1063/1063 [00:44<00:00, 23.98it/s]n):  75%|███████▍  | 2295/3071 [00:40<00:13, 59.16it/s]\n",
      "Splitting music files (../data/midi/mtok_split/vg_large/v-411_t-Basic_c-2048/train): 100%|██████████| 3071/3071 [01:06<00:00, 46.17it/s]\n",
      "Performing data augmentation: 100%|██████████| 7973/7973 [18:03<00:00,  7.36it/s]\n"
     ]
    }
   ],
   "source": [
    "chunk_path = Path(f'{midi_path}/mtok_split/{dataset_name}/v-{VOCAB_SIZE}_t-{TOKENIZER_CONFIG}_c-{CHUNK_LENGTH}')\n",
    "train_chunk_path = Path(f'{chunk_path}/train')\n",
    "valid_chunk_path = Path(f'{chunk_path}/valid')\n",
    "test_chunk_path = Path(f'{chunk_path}/test')\n",
    "\n",
    "split_data = [\n",
    "    (train_filepaths, train_chunk_path),\n",
    "    (valid_filepaths, valid_chunk_path),\n",
    "    (test_filepaths, test_chunk_path)\n",
    "]\n",
    "\n",
    "def chunk_files(filepaths, tokenizer, chunks_dir, max_seq_len):\n",
    "    split_files_for_training(\n",
    "        files_paths=filepaths,\n",
    "        tokenizer=tokenizer,\n",
    "        save_dir=chunks_dir,\n",
    "        max_seq_len=max_seq_len,\n",
    "        num_overlap_bars=1\n",
    "    )\n",
    "    augment_dataset(\n",
    "        chunks_dir,\n",
    "        pitch_offsets=[-12, 12],\n",
    "        velocity_offsets=[-4, 4],\n",
    "        duration_offsets=[-0.5, 0.5],\n",
    "    )\n",
    "\n",
    "if not chunk_path.exists():\n",
    "    with Pool(processes=3) as pool:\n",
    "        pool.starmap(chunk_files, [(filepaths, tokenizer, chunks_dir, CHUNK_LENGTH) for filepaths, chunks_dir in split_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train chunks: 52558, Valid chunks: 6993, Test chunks: 6267\n"
     ]
    }
   ],
   "source": [
    "train_chunk_filepaths = list(train_chunk_path.glob(\"**/*.mid\"))\n",
    "valid_chunk_filepaths = list(valid_chunk_path.glob(\"**/*.mid\"))\n",
    "test_chunk_filepaths = list(test_chunk_path.glob(\"**/*.mid\"))\n",
    "\n",
    "print(f'Train chunks: {len(train_chunk_filepaths)}, Valid chunks: {len(valid_chunk_filepaths)}, Test chunks: {len(test_chunk_filepaths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_paths = [train_chunk_filepaths, valid_chunk_filepaths, test_chunk_filepaths]\n",
    "\n",
    "for chunk_path in chunk_paths:\n",
    "    random.shuffle(chunk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(chunks_path, tokenizer, max_seq_len, batch_size):\n",
    "    collator = DataCollator(tokenizer.pad_token_id) # copy_inputs_as_labels and shift_labels not needed as done by the transformer\n",
    "    dataset = DatasetMIDI(\n",
    "        pre_tokenize=True,\n",
    "        files_paths=chunks_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_len=max_seq_len,\n",
    "        bos_token_id=tokenizer[\"BOS_None\"],\n",
    "        eos_token_id=tokenizer[\"EOS_None\"])\n",
    "    return DataLoader(dataset=dataset, collate_fn=collator, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-tokenizing: 100%|██████████| 52558/52558 [15:43<00:00, 55.68it/s]\n",
      "Pre-tokenizing: 100%|██████████| 6993/6993 [02:01<00:00, 57.54it/s]\n",
      "Pre-tokenizing: 100%|██████████| 6267/6267 [01:52<00:00, 55.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "train_loader, valid_loader, test_loader = map(\n",
    "    lambda chunk_filepaths: cycle(create_data_loader(chunk_filepaths, tokenizer, CHUNK_LENGTH, BATCH_SIZE)),\n",
    "    chunk_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'memorizing_miditok_{dataset_name}_t-{TIMESTEPS}_v-{VOCAB_SIZE}_{VERSION_LABEL}'\n",
    "model_load_path = Path(f'../data/checkpoints/{model_name}.dat')\n",
    "model_save_path = Path(f'../data/checkpoints/{model_name}.dat')\n",
    "log_dir = Path(f'../tensorboard/{model_name}')\n",
    "tensorboard_writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a transformer and set up our training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21958571\n"
     ]
    }
   ],
   "source": [
    "model = MemorizingTransformer(\n",
    "    num_tokens = VOCAB_SIZE,\n",
    "    dim = N_EMBED,\n",
    "    depth = N_LAYER,\n",
    "    heads = N_HEAD,\n",
    "    dim_head = DIM_HEAD,\n",
    "    attn_dropout = 0.2,\n",
    "    ff_dropout = 0.2,\n",
    "    memorizing_layers = (4, 5),\n",
    "    max_knn_memories = CHUNK_LENGTH, # No point in having more meories than the chunk length as we clear them at the end of each chunk\n",
    "    num_retrieved_memories = 32, # Top K\n",
    "    xl_memory_layers = (2, 3, 4, 5),\n",
    "    xl_max_memories = TIMESTEPS, # One context-length of XL memory\n",
    "    pad_id = tokenizer.pad_token_id,\n",
    "    # shift_knn_memories_down = 1,\n",
    "    # shift_xl_memories_down = 1\n",
    ").to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "token_emb.weight \t torch.Size([411, 512])\n",
      "rel_pos_bias.relative_attention_bias.weight \t torch.Size([32, 8])\n",
      "knn_rel_pos_bias.relative_attention_bias.weight \t torch.Size([32, 8])\n",
      "layers.0.0.fn.to_q.weight \t torch.Size([512, 512])\n",
      "layers.0.0.fn.to_kv.weight \t torch.Size([128, 512])\n",
      "layers.0.0.fn.to_out.weight \t torch.Size([512, 512])\n",
      "layers.0.0.fn.to_out.bias \t torch.Size([512])\n",
      "layers.0.0.norm.weight \t torch.Size([512])\n",
      "layers.0.0.norm.bias \t torch.Size([512])\n",
      "layers.0.1.fn.net.0.weight \t torch.Size([2048, 512])\n",
      "layers.0.1.fn.net.0.bias \t torch.Size([2048])\n",
      "layers.0.1.fn.net.3.weight \t torch.Size([512, 2048])\n",
      "layers.0.1.fn.net.3.bias \t torch.Size([512])\n",
      "layers.0.1.norm.weight \t torch.Size([512])\n",
      "layers.0.1.norm.bias \t torch.Size([512])\n",
      "layers.1.0.fn.to_q.weight \t torch.Size([512, 512])\n",
      "layers.1.0.fn.to_kv.weight \t torch.Size([128, 512])\n",
      "layers.1.0.fn.to_out.weight \t torch.Size([512, 512])\n",
      "layers.1.0.fn.to_out.bias \t torch.Size([512])\n",
      "layers.1.0.norm.weight \t torch.Size([512])\n",
      "layers.1.0.norm.bias \t torch.Size([512])\n",
      "layers.1.1.fn.net.0.weight \t torch.Size([2048, 512])\n",
      "layers.1.1.fn.net.0.bias \t torch.Size([2048])\n",
      "layers.1.1.fn.net.3.weight \t torch.Size([512, 2048])\n",
      "layers.1.1.fn.net.3.bias \t torch.Size([512])\n",
      "layers.1.1.norm.weight \t torch.Size([512])\n",
      "layers.1.1.norm.bias \t torch.Size([512])\n",
      "layers.2.0.fn.to_q.weight \t torch.Size([512, 512])\n",
      "layers.2.0.fn.to_kv.weight \t torch.Size([128, 512])\n",
      "layers.2.0.fn.to_out.weight \t torch.Size([512, 512])\n",
      "layers.2.0.fn.to_out.bias \t torch.Size([512])\n",
      "layers.2.0.norm.weight \t torch.Size([512])\n",
      "layers.2.0.norm.bias \t torch.Size([512])\n",
      "layers.2.1.fn.net.0.weight \t torch.Size([2048, 512])\n",
      "layers.2.1.fn.net.0.bias \t torch.Size([2048])\n",
      "layers.2.1.fn.net.3.weight \t torch.Size([512, 2048])\n",
      "layers.2.1.fn.net.3.bias \t torch.Size([512])\n",
      "layers.2.1.norm.weight \t torch.Size([512])\n",
      "layers.2.1.norm.bias \t torch.Size([512])\n",
      "layers.3.0.fn.scale \t torch.Size([8, 1, 1])\n",
      "layers.3.0.fn.to_q.weight \t torch.Size([512, 512])\n",
      "layers.3.0.fn.to_kv.weight \t torch.Size([128, 512])\n",
      "layers.3.0.fn.to_out.weight \t torch.Size([512, 512])\n",
      "layers.3.0.norm.weight \t torch.Size([512])\n",
      "layers.3.0.norm.bias \t torch.Size([512])\n",
      "layers.3.1.fn.net.0.weight \t torch.Size([2048, 512])\n",
      "layers.3.1.fn.net.0.bias \t torch.Size([2048])\n",
      "layers.3.1.fn.net.3.weight \t torch.Size([512, 2048])\n",
      "layers.3.1.fn.net.3.bias \t torch.Size([512])\n",
      "layers.3.1.norm.weight \t torch.Size([512])\n",
      "layers.3.1.norm.bias \t torch.Size([512])\n",
      "layers.4.0.fn.scale \t torch.Size([8, 1, 1])\n",
      "layers.4.0.fn.to_q.weight \t torch.Size([512, 512])\n",
      "layers.4.0.fn.to_kv.weight \t torch.Size([128, 512])\n",
      "layers.4.0.fn.to_out.weight \t torch.Size([512, 512])\n",
      "layers.4.0.norm.weight \t torch.Size([512])\n",
      "layers.4.0.norm.bias \t torch.Size([512])\n",
      "layers.4.1.fn.net.0.weight \t torch.Size([2048, 512])\n",
      "layers.4.1.fn.net.0.bias \t torch.Size([2048])\n",
      "layers.4.1.fn.net.3.weight \t torch.Size([512, 2048])\n",
      "layers.4.1.fn.net.3.bias \t torch.Size([512])\n",
      "layers.4.1.norm.weight \t torch.Size([512])\n",
      "layers.4.1.norm.bias \t torch.Size([512])\n",
      "layers.5.0.fn.to_q.weight \t torch.Size([512, 512])\n",
      "layers.5.0.fn.to_kv.weight \t torch.Size([128, 512])\n",
      "layers.5.0.fn.to_out.weight \t torch.Size([512, 512])\n",
      "layers.5.0.fn.to_out.bias \t torch.Size([512])\n",
      "layers.5.0.norm.weight \t torch.Size([512])\n",
      "layers.5.0.norm.bias \t torch.Size([512])\n",
      "layers.5.1.fn.net.0.weight \t torch.Size([2048, 512])\n",
      "layers.5.1.fn.net.0.bias \t torch.Size([2048])\n",
      "layers.5.1.fn.net.3.weight \t torch.Size([512, 2048])\n",
      "layers.5.1.fn.net.3.bias \t torch.Size([512])\n",
      "layers.5.1.norm.weight \t torch.Size([512])\n",
      "layers.5.1.norm.bias \t torch.Size([512])\n",
      "layers.6.0.fn.to_q.weight \t torch.Size([512, 512])\n",
      "layers.6.0.fn.to_kv.weight \t torch.Size([128, 512])\n",
      "layers.6.0.fn.to_out.weight \t torch.Size([512, 512])\n",
      "layers.6.0.fn.to_out.bias \t torch.Size([512])\n",
      "layers.6.0.norm.weight \t torch.Size([512])\n",
      "layers.6.0.norm.bias \t torch.Size([512])\n",
      "layers.6.1.fn.net.0.weight \t torch.Size([2048, 512])\n",
      "layers.6.1.fn.net.0.bias \t torch.Size([2048])\n",
      "layers.6.1.fn.net.3.weight \t torch.Size([512, 2048])\n",
      "layers.6.1.fn.net.3.bias \t torch.Size([512])\n",
      "layers.6.1.norm.weight \t torch.Size([512])\n",
      "layers.6.1.norm.bias \t torch.Size([512])\n",
      "layers.7.0.fn.to_q.weight \t torch.Size([512, 512])\n",
      "layers.7.0.fn.to_kv.weight \t torch.Size([128, 512])\n",
      "layers.7.0.fn.to_out.weight \t torch.Size([512, 512])\n",
      "layers.7.0.fn.to_out.bias \t torch.Size([512])\n",
      "layers.7.0.norm.weight \t torch.Size([512])\n",
      "layers.7.0.norm.bias \t torch.Size([512])\n",
      "layers.7.1.fn.net.0.weight \t torch.Size([2048, 512])\n",
      "layers.7.1.fn.net.0.bias \t torch.Size([2048])\n",
      "layers.7.1.fn.net.3.weight \t torch.Size([512, 2048])\n",
      "layers.7.1.fn.net.3.bias \t torch.Size([512])\n",
      "layers.7.1.norm.weight \t torch.Size([512])\n",
      "layers.7.1.norm.bias \t torch.Size([512])\n",
      "to_logits.0.weight \t torch.Size([512])\n",
      "to_logits.0.bias \t torch.Size([512])\n",
      "to_logits.1.weight \t torch.Size([411, 512])\n",
      "to_logits.1.bias \t torch.Size([411])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(optimizer, completed_iterations, train_loss, val_loss):\n",
    "    tensorboard_writer.add_scalar('Loss/train', train_loss, completed_iterations)\n",
    "    tensorboard_writer.add_scalar('Loss/val', val_loss, completed_iterations)\n",
    "    print(f'Writing to Tensorboard: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    torch.save({\n",
    "        'iter': completed_iterations,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, model_save_path)\n",
    "\n",
    "completed_iterations = 0\n",
    "if model_load_path.exists():\n",
    "    checkpoint = torch.load(model_load_path)\n",
    "    completed_iterations = checkpoint['iter']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print(f\"Loaded model from iteration {completed_iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|          | 0/100000 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  x.storage().data_ptr() + x.storage_offset() * 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to Tensorboard: Train Loss: 6.1722, Val Loss: 4.2459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|          | 50/100000 [00:30<16:27:01,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to Tensorboard: Train Loss: 2.3053, Val Loss: 2.2537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|          | 123/100000 [01:12<15:37:30,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to Tensorboard: Train Loss: 2.0493, Val Loss: 2.0265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|          | 179/100000 [01:44<15:45:41,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to Tensorboard: Train Loss: 2.0483, Val Loss: 1.9822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|          | 255/100000 [02:27<15:27:20,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to Tensorboard: Train Loss: 1.9109, Val Loss: 1.8700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|          | 312/100000 [02:59<15:22:29,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to Tensorboard: Train Loss: 1.8810, Val Loss: 1.8357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|          | 367/100000 [03:42<15:49:15,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to Tensorboard: Train Loss: 2.0241, Val Loss: 1.9513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|          | 439/100000 [04:13<15:53:25,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to Tensorboard: Train Loss: 1.7619, Val Loss: 1.7221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|          | 457/100000 [04:25<16:26:29,  1.68it/s]"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(NUM_BATCHES - completed_iterations), mininterval = 10., desc = 'training'):\n",
    "    model.train()\n",
    "\n",
    "    data = next(train_loader)[\"input_ids\"].to(device)\n",
    "    if data.shape[0] != BATCH_SIZE:\n",
    "        print(f'Skipping batch {i} as it is not of size {BATCH_SIZE}, but {data.shape[0]}')\n",
    "        data = next(train_loader)[\"input_ids\"].to(device)\n",
    "\n",
    "    train_loss = 0.\n",
    "    with model.knn_memories_context(batch_size = BATCH_SIZE) as knn_memories:\n",
    "        xl_memories = None    \n",
    "        seq, labels = data[:, :-1], data[:, 1:]\n",
    "\n",
    "        for seq_segment, labels_segment in zip(seq.chunk(SEGMENTS, dim = -1), labels.chunk(SEGMENTS, dim = -1)):\n",
    "            loss, xl_memories = model(\n",
    "                seq_segment,\n",
    "                labels = labels_segment,\n",
    "                knn_memories = knn_memories,\n",
    "                xl_memories = xl_memories\n",
    "            )\n",
    "\n",
    "            train_loss += loss.item() / SEGMENTS\n",
    "            (loss / SEGMENTS).backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_CLIP_NORM)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if not (i % VALIDATE_EVERY):\n",
    "        model.eval()\n",
    "\n",
    "        valid_data = next(valid_loader)[\"input_ids\"].to(device)\n",
    "        if valid_data.shape[0] != BATCH_SIZE:\n",
    "            print(f'Skipping validation batch {i} as it is not of size {BATCH_SIZE}, but {valid_data.shape[0]}')\n",
    "            valid_data = next(valid_loader)[\"input_ids\"].to(device)\n",
    "\n",
    "        valid_loss = 0.\n",
    "\n",
    "        with torch.no_grad(), model.knn_memories_context(batch_size = BATCH_SIZE) as knn_memories:\n",
    "            xl_memories = None    \n",
    "            seq, labels = data[:, :-1], data[:, 1:]\n",
    "\n",
    "            for seq_segment, labels_segment in zip(seq.chunk(SEGMENTS, dim = -1), labels.chunk(SEGMENTS, dim = -1)):\n",
    "                loss, xl_memories = model(\n",
    "                    seq_segment,\n",
    "                    labels = labels_segment,\n",
    "                    knn_memories = knn_memories,\n",
    "                    xl_memories = xl_memories\n",
    "                )\n",
    "\n",
    "                valid_loss += loss.item() / SEGMENTS\n",
    "\n",
    "        save_checkpoint(optimizer, i + completed_iterations, train_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Results\n",
    "\n",
    "With the base REMI vocab size of 411 tokens along with the same embed / head / layer count as our hand built model, the results are significantly worse. The training curve almost immediately flattened out at 1.6 loss, compared to under 1.0 previously.\n",
    "\n",
    "This could be due to e.g.\n",
    "\n",
    "- Different encoding. Not only are tokens different in MidiTok / Remi but we used bar / beat *embeddings* rather than tokens previously. There are so many more meanings to the tokens in addition to the count rather than just note / duration / sos / eos / pad that it would be much harder to guess the right answer. If we do see convergence it should mean we have learnt more useful information rather than just gamed the system so to speak.\n",
    "- Larger vocabulary = more choices to pick for the next token\n",
    "- The MIDITok data loader only provides max `CHUNK_LENGTH` tokens, whereas our custom 'contiguous' data loader always fed in entire tracks. Only a handful of tracks are between 1 and 2k tokens, most are 3 - 5K, so are being split into multiple pieces.\n",
    "- The LucidRains Memorizing Transformers architecture is very similar to our model but there are a couple of changes (memory on a subset of layer vs all, lack of absolute embeddings) and of course it is a different implementation. I would expect it to be less buggy if anything though.\n",
    "\n",
    "I did have to tweak the transformer's KNN memory to run exclusively on the GPU as index was on CPU and the data on disk, which was a major bottleneck (as found in our model). Luckily once again the code was nearly identical so it was easy to follow / edit.\n",
    "\n",
    "## Things to try\n",
    "\n",
    "- Train the tokenizer with 1K, 5K, 10K, 20K, 30K size vocab. This should result in less tokens per song. We might need to drop the max seq length to avoid over-padded songs.\n",
    "\n",
    "- Add in chord / tempo / time sig / rest tokens. This should result in more tokens per song. We might need to increase the max seq length to avoid over-split songs.\n",
    "\n",
    "> Note - the files will need re-encoding as they are split based on estimated token length, which depends on encoding and vocab size!\n",
    "\n",
    "> Another note! - When the vocab size was halved, the number of tokens per file seemed to double, as you might expect given the bpe was trained on the same data. That meant doubling the chunk size and segment count in order to split the files into the same number of pieces.\n",
    "\n",
    "- Augment the dataset with pitch and velocity shifted versions using the MIDITok tools\n",
    "\n",
    "- Full Lakh dataset, although if we can't do well on a curated, stylistic dataset then a more varied one is likely to decrease performance.\n",
    "\n",
    "\n",
    "## Experiment log\n",
    "\n",
    "- Was seeing increase in loss after initial drop which eventually came back down again. Decreasing learning rate made it happen later and worse. Increased learning rate (and added dropout) which seemed to negate it.\n",
    "\n",
    "- Some apparent cyclic loss patterns so tried shuffling between epochs. Requires either waiting for a pretokenise op between epochs (long even on vg_large, let alone Lakh) or tokenise on the fly which reduces GPU usage to around 70%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel DataLoaders\n",
    "\n",
    "Shuffling between epochs should avoid cyclic learning behaviour, but\n",
    "- If we pretokenise there is a big delay between epochs\n",
    "- If we don't, the GPU isn't used as effectively\n",
    "\n",
    "We could have two dataloaders, and pretokenise one whilst the other is in use.\n",
    "\n",
    "This doesn't seem to be an issue right now but we could implement it if needed.\n",
    "\n",
    "Claude suggested the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ParallelDataLoaderManager:\n",
    "    def __init__(self, chunk_filepaths: list, tokenizer, chunk_length: int, batch_size: int):\n",
    "        self.chunk_filepaths = chunk_filepaths\n",
    "        self.tokenizer = tokenizer\n",
    "        self.chunk_length = chunk_length\n",
    "        self.batch_size = batch_size\n",
    "        self.executor = ThreadPoolExecutor(max_workers=1)\n",
    "        self.next_loader_future = None\n",
    "        self.current_loader = None\n",
    "\n",
    "    def create_dataloader(self, filepaths) -> DataLoader:\n",
    "        # Using the create_data_loader function from your workspace\n",
    "        return create_data_loader(\n",
    "            filepaths,\n",
    "            self.tokenizer,\n",
    "            self.chunk_length, \n",
    "            self.batch_size\n",
    "        )\n",
    "    \n",
    "    def prepare_next_loader(self):\n",
    "        # Shuffle filepaths for next epoch\n",
    "        shuffled_paths = self.chunk_filepaths.copy()\n",
    "        random.shuffle(shuffled_paths)\n",
    "        \n",
    "        # Start creating next loader in background\n",
    "        self.next_loader_future = self.executor.submit(\n",
    "            self.create_dataloader,\n",
    "            shuffled_paths\n",
    "        )\n",
    "\n",
    "    def get_loader(self) -> DataLoader:\n",
    "        if self.current_loader is None:\n",
    "            # First call - create initial loader\n",
    "            self.current_loader = self.create_dataloader(self.chunk_filepaths)\n",
    "            # Start preparing next loader \n",
    "            self.prepare_next_loader()\n",
    "            return self.current_loader\n",
    "\n",
    "        # Wait for next loader to be ready and swap\n",
    "        self.current_loader = self.next_loader_future.result()\n",
    "        # Start preparing next loader\n",
    "        self.prepare_next_loader()\n",
    "        return self.current_loader\n",
    "\n",
    "    def __del__(self):\n",
    "        self.executor.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
