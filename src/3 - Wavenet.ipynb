{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from itertools import chain\n",
    "from itertools import groupby\n",
    "from functools import reduce\n",
    "from typing import Collection, List\n",
    "from pathlib import Path\n",
    "import music21 as m21\n",
    "musescore_path = '/usr/bin/mscore'\n",
    "m21.environment.set('musicxmlPath', musescore_path)\n",
    "m21.environment.set('musescoreDirectPNGPath', musescore_path)\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4090.\n"
     ]
    }
   ],
   "source": [
    "if device == \"cuda\":\n",
    "    print(f\"Device: {torch.cuda.get_device_name()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with lets bring in our import / encode / decode functions from last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEATS_PER_BAR = 4 # beats per bar\n",
    "DIVISIONS_PER_QUARTER = 4 # i.e. 4 beats per bar and 4 divisions per beat gives 16 divisions per bar\n",
    "MIDI_NOTE_COUNT = 128\n",
    "MAX_NOTE_DUR = (8*BEATS_PER_BAR*DIVISIONS_PER_QUARTER)\n",
    "SEPARATOR_IDX = -1 # separator value for numpy encoding\n",
    "PIANO_RANGE = (21, 108)\n",
    "SOS = '<|sos|>' # Start of sequence\n",
    "EOS = '<|eos|>' # End of sequence\n",
    "SEP = '<|sep|>' # End of timestep (required for polyphony). Note index -1\n",
    "PAD = '<|pad|>' # Padding to ensure blocks are the same size\n",
    " # SEP token must be last, i.e. one place before note tokens, so that adding the note offset still works when encoding\n",
    "SPECIAL_TOKENS = [SOS, EOS, PAD, SEP]\n",
    "MIDI_NOTE_COUNT = 128\n",
    "NOTE_TOKENS = [f'n{i}' for i in range(MIDI_NOTE_COUNT)]\n",
    "DURATION_SIZE = 8 * BEATS_PER_BAR * DIVISIONS_PER_QUARTER + 1 # 8 bars of sixteenth (semiquaver) notes + 1 for 0 length\n",
    "DURATION_TOKENS = [f'd{i}' for i in range(DURATION_SIZE)]\n",
    "NOTE_START, NOTE_END = NOTE_TOKENS[0], NOTE_TOKENS[-1]\n",
    "DURATION_START, DURATION_END = DURATION_TOKENS[0], DURATION_TOKENS[-1]\n",
    "ALL_TOKENS = SPECIAL_TOKENS + NOTE_TOKENS + DURATION_TOKENS\n",
    "ALL_TOKENS[0:8]\n",
    "TIMESIG = f'{BEATS_PER_BAR}/4'\n",
    "block_size = 8 # Bigger block size for WaveNet\n",
    "class MusicVocab():\n",
    "    def __init__(self):\n",
    "        itos = SPECIAL_TOKENS + NOTE_TOKENS + DURATION_TOKENS\n",
    "        # Ensure that the vocab is a multiple of 8 for fp16 training\n",
    "        if len(itos)%8 != 0:\n",
    "            itos = itos + [f'dummy{i}' for i in range(len(itos)%8)]\n",
    "        self.itos = itos\n",
    "        self.stoi = {v:k for k,v in enumerate(self.itos)}\n",
    "\n",
    "    def to_indices(self, t:Collection[str]) -> List[int]:\n",
    "        \"Convert a list of tokens `t` to their indices.\"\n",
    "        return [self.stoi[w] for w in t]\n",
    "\n",
    "    def to_tokens(self, nums:Collection[int], sep=' ') -> List[str]:\n",
    "        \"Convert a list of indices to their tokens.\"\n",
    "        items = [self.itos[i] for i in nums]\n",
    "        return sep.join(items) if sep is not None else items\n",
    "\n",
    "    @property\n",
    "    def sos_idx(self): return self.stoi[SOS]\n",
    "    @property\n",
    "    def eos_idx(self): return self.stoi[EOS]\n",
    "    @property\n",
    "    def sep_idx(self): return self.stoi[SEP]\n",
    "    @property\n",
    "    def pad_idx(self): return self.stoi[PAD]\n",
    "    @property\n",
    "    def note_position_enc_range(self): return (self.stoi[SEP], self.stoi[DURATION_END]+1)\n",
    "    @property\n",
    "    def note_range(self): return self.stoi[NOTE_START], self.stoi[NOTE_END]+1\n",
    "    @property\n",
    "    def duration_range(self): return self.stoi[DURATION_START], self.stoi[DURATION_END]+1\n",
    "    @property\n",
    "    def size(self): return len(self.itos)\n",
    "\n",
    "def stream_to_sparse_enc(stream_score, note_size=MIDI_NOTE_COUNT, sample_freq=DIVISIONS_PER_QUARTER, max_note_dur=MAX_NOTE_DUR):    \n",
    "    # Time is measured in quarter notes since the start of the piece\n",
    "    # Original states that we are assuming 4/4 time but I don't see why that would be the case. BPB isn't used here.\n",
    "\n",
    "    # (MusicAutobot author:) TODO: need to order by instruments most played and filter out percussion or include the channel\n",
    "    highest_time = max(\n",
    "        stream_score.flatten().getElementsByClass('Note').stream().highestTime,\n",
    "        stream_score.flatten().getElementsByClass('Chord').stream().highestTime)\n",
    "    \n",
    "    # Calculate the maximum number of time steps\n",
    "    max_timestep = round(highest_time * sample_freq) + 1\n",
    "    sparse_score = np.zeros((max_timestep, len(stream_score.parts), note_size), dtype=np.int32)\n",
    "\n",
    "    # Convert a note to a tuple of (pitch,offset,duration)\n",
    "    def note_data(pitch, note):\n",
    "        return (pitch.midi, int(round(note.offset*sample_freq)), int(round(note.duration.quarterLength*sample_freq)))\n",
    "\n",
    "    for idx, part in enumerate(stream_score.parts):\n",
    "        \n",
    "        notes = chain.from_iterable(\n",
    "            [note_data(elem.pitch, elem)] if isinstance(elem, m21.note.Note)\n",
    "            else [note_data(p, elem) for p in elem.pitches] if isinstance(elem, m21.chord.Chord) \n",
    "            else []\n",
    "            for elem in part.flatten()\n",
    "        )\n",
    "\n",
    "        # sort flattened note list by timestep (1), duration (2) so that hits are not overwritten and longer notes have priority\n",
    "        notes_sorted = sorted(notes, key=lambda x: (x[1], x[2])) \n",
    "\n",
    "        for note in notes_sorted:\n",
    "            if note is not None:\n",
    "                pitch, timestep, duration = note\n",
    "                clamped_duration = max_note_dur if max_note_dur is not None and duration > max_note_dur else duration\n",
    "                sparse_score[timestep, idx, pitch] = clamped_duration\n",
    "    \n",
    "    return sparse_score\n",
    "\n",
    "# Pass in the 'one-hot' encoded numpy score\n",
    "def sparse_to_position_enc(sparse_score, skip_last_rest=True):\n",
    "\n",
    "    def encode_timestep(acc, timestep):\n",
    "        encoded_timesteps, wait_count = acc\n",
    "        encoded_timestep = timestep_to_position_enc(timestep) # pass in all notes for both instruments, merged list returned\n",
    "        if len(encoded_timestep) == 0: # i.e. all zeroes at time step\n",
    "            wait_count += 1\n",
    "        else:\n",
    "            if wait_count > 0:\n",
    "                encoded_timesteps.append([SEPARATOR_IDX, wait_count])\n",
    "            encoded_timesteps.extend(encoded_timestep)\n",
    "            wait_count = 1\n",
    "        return encoded_timesteps, wait_count\n",
    "    \n",
    "    encoded_timesteps, final_wait_count = reduce(encode_timestep, sparse_score, ([], 0))\n",
    "\n",
    "    if final_wait_count > 0 and not skip_last_rest:\n",
    "        encoded_timesteps.append([SEPARATOR_IDX, final_wait_count]) # add trailing rests\n",
    "\n",
    "    return np.array(encoded_timesteps).reshape(-1, 2) # reshaping. Just in case result is empty\n",
    "    \n",
    "def timestep_to_position_enc(timestep, note_range=PIANO_RANGE):\n",
    "\n",
    "    note_min, note_max = note_range\n",
    "\n",
    "    def encode_note_data(note_data, active_note_idx):\n",
    "        instrument, pitch = active_note_idx\n",
    "        duration = timestep[instrument, pitch]\n",
    "        if pitch >= note_min and pitch < note_max:\n",
    "            note_data.append([pitch, duration, instrument])\n",
    "        return note_data\n",
    "    \n",
    "    active_note_idxs = zip(*timestep.nonzero())\n",
    "    encoded_notes = reduce(encode_note_data, active_note_idxs, [])\n",
    "    sorted_notes = sorted(encoded_notes, key=lambda x: x[0], reverse=True) # sort by note (highest to lowest)\n",
    "\n",
    "    # Dropping instrument information for simplicity.\n",
    "    # MusicAutobot allows different encoding schemes which include instrument number and split pitch into class / octave.\n",
    "    return [n[:2] for n in sorted_notes]\n",
    "\n",
    "def position_to_idx_enc(note_position_score, vocab):\n",
    "    note_idx_score = note_position_score.copy()\n",
    "    note_min_idx, _ = vocab.note_range\n",
    "    dur_min_idx, _ = vocab.duration_range\n",
    "    \n",
    "    # Replace note and duration tokens with their index in vocab. \n",
    "    # Tokens are the same order as notes and note_min_idx offset is constant so we can apply in one go.\n",
    "    # Using broadcasting to add the 1D [note_min_idx, dur_min_idx] to the 2D note_idx_score.\n",
    "    note_idx_score += np.array([note_min_idx, dur_min_idx])\n",
    "    \n",
    "    prefix =  np.array([vocab.sos_idx])\n",
    "    padding = np.full((block_size - 1), vocab.pad_idx)\n",
    "    suffix = np.array([vocab.eos_idx])\n",
    "\n",
    "    return np.concatenate([padding, prefix, note_idx_score.reshape(-1), suffix])\n",
    "\n",
    "def import_midi_file(file_path):\n",
    "    midifile = m21.midi.MidiFile()\n",
    "    if isinstance(file_path, bytes):\n",
    "        midifile.readstr(file_path)\n",
    "    else:\n",
    "        midifile.open(file_path)\n",
    "        midifile.read()\n",
    "        midifile.close()\n",
    "    return midifile\n",
    "\n",
    "def midifile_to_stream(midifile): \n",
    "    return m21.midi.translate.midiFileToStream(midifile)\n",
    "\n",
    "def midifile_to_idx_score(file_path, vocab):\n",
    "    midifile = import_midi_file(file_path)\n",
    "    stream = midifile_to_stream(midifile)\n",
    "    sparse_score = stream_to_sparse_enc(stream)\n",
    "    note_pos_score = sparse_to_position_enc(sparse_score)\n",
    "    return position_to_idx_enc(note_pos_score, vocab)\n",
    "\n",
    "# Combining notes with different durations into a single chord may overwrite conflicting durations.\n",
    "def group_notes_by_duration(notes):\n",
    "    get_note_quarter_length = lambda note: note.duration.quarterLength\n",
    "    sorted_notes = sorted(notes, key=get_note_quarter_length)\n",
    "    return [list(g) for k,g in groupby(sorted_notes, get_note_quarter_length)]\n",
    "\n",
    "def sparse_instrument_to_stream_part(sparse_instrument_score, step_duration):\n",
    "    part = m21.stream.Part()\n",
    "    part.append(m21.instrument.Piano())\n",
    "    \n",
    "    for t_idx, pitch_values in enumerate(sparse_instrument_score):\n",
    "\n",
    "        def decode_sparse_note(notes, pitch_index):\n",
    "            note = m21.note.Note(pitch_index)\n",
    "            quarters = sparse_instrument_score[t_idx, pitch_index]\n",
    "            note.duration = m21.duration.Duration(float(quarters) * step_duration.quarterLength)\n",
    "            notes.append(note)\n",
    "            return notes\n",
    "        \n",
    "        pitch_idxs = np.nonzero(pitch_values)[0]\n",
    "\n",
    "        if len(pitch_idxs) != 0: \n",
    "            notes = reduce(decode_sparse_note, pitch_idxs, [])\n",
    "            for note_group in group_notes_by_duration(notes):\n",
    "                note_position = t_idx*step_duration.quarterLength\n",
    "                if len(note_group) == 1:\n",
    "                    part.insert(note_position, note_group[0])\n",
    "                else:\n",
    "                    chord = m21.chord.Chord(note_group)\n",
    "                    part.insert(note_position, chord)\n",
    "\n",
    "    return part\n",
    "\n",
    "def sparse_to_stream_enc(sparse_score, bpm=120):\n",
    "    step_duration = m21.duration.Duration(1. / DIVISIONS_PER_QUARTER)\n",
    "    stream = m21.stream.Score()\n",
    "    stream.append(m21.meter.TimeSignature(TIMESIG))\n",
    "    stream.append(m21.tempo.MetronomeMark(number=bpm))\n",
    "\n",
    "    # Not required here but left as example of options available\n",
    "    stream.append(m21.key.KeySignature(0))\n",
    "    \n",
    "    for inst in range(sparse_score.shape[1]):\n",
    "        part = sparse_instrument_to_stream_part(sparse_score[:,inst,:], step_duration)\n",
    "        stream.append(part)\n",
    "    \n",
    "    # Again, not required yet but left as example\n",
    "    stream = stream.transpose(0)\n",
    "    \n",
    "    return stream\n",
    "\n",
    "def position_to_sparse_enc(note_position_score):\n",
    "\n",
    "    # Add all the separator durations as they denote the elapsed time\n",
    "    score_length = sum(timestep[1] for timestep in note_position_score if timestep[0] == SEPARATOR_IDX) + 1\n",
    "    \n",
    "    # Single instrument as we discarded the instrument information when encoding\n",
    "    # We will adapt to handle multiple instruments later\n",
    "    instrument = 0\n",
    "\n",
    "    def decode_note_position_step(acc, note_pos_step):\n",
    "        timestep, sparse_score = acc\n",
    "        note, duration = note_pos_step.tolist()\n",
    "        if note < SEPARATOR_IDX:  # Skip special token\n",
    "            return acc\n",
    "        elif note == SEPARATOR_IDX:  # Time elapsed\n",
    "            return (timestep + duration, sparse_score)\n",
    "        else:\n",
    "            sparse_score[timestep, instrument, note] = duration\n",
    "            return (timestep, sparse_score)\n",
    "\n",
    "     # (timesteps, instruments, pitches)\n",
    "    initial_sparse_score = np.zeros((score_length, 1, MIDI_NOTE_COUNT))\n",
    "    _, final_sparse_score = reduce(decode_note_position_step, note_position_score, (0, initial_sparse_score))\n",
    "\n",
    "    return final_sparse_score\n",
    "\n",
    "# No validation of note position encoding included to keep it simple for now\n",
    "def idx_to_position_enc(idx_score, vocab):\n",
    "    \n",
    "    # Filter out special tokens\n",
    "    notes_durs_start, notes_durs_end = vocab.note_position_enc_range # range of non-special token values\n",
    "    notes_durations_idx_score = idx_score[np.where((idx_score >= notes_durs_start) & (idx_score < notes_durs_end))]\n",
    "\n",
    "    # Reshape into pairs of (note, duration). If odd number of tokens, discard the last token.\n",
    "    if notes_durations_idx_score.shape[0] % 2 != 0:\n",
    "        notes_durations_idx_score = notes_durations_idx_score[:-1]\n",
    "\n",
    "    position_score = notes_durations_idx_score.copy().reshape(-1, 2)\n",
    "    \n",
    "    # Shift token index values to note and duration values\n",
    "    if position_score.shape[0] == 0: \n",
    "        return position_score\n",
    "    else:\n",
    "        note_min_idx, _ = vocab.note_range\n",
    "        dur_min_idx, _ = vocab.duration_range\n",
    "        position_score -= np.array([note_min_idx, dur_min_idx])\n",
    "        return position_score\n",
    "\n",
    "def idx_to_stream_enc(idx_score, vocab):\n",
    "    position_score = idx_to_position_enc(idx_score, vocab)\n",
    "    sparse_score = position_to_sparse_enc(position_score)\n",
    "    return sparse_to_stream_enc(sparse_score)\n",
    "\n",
    "vocab = MusicVocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wavenet\n",
    "\n",
    "In the previous notebook we only used a single, fully connected, hidden layer in our model.\n",
    "\n",
    "Because all of the input token embeddings are connected to all of the hidden neurons, we are crushing all of our information together in a single step.\n",
    "\n",
    "The Wavenet architectecture, on the other hand, progressively fuses sets of tokens in subsequent layers. This allows small features to be gradually merged into larger and broader ones.\n",
    "\n",
    "We can, for example, take 2 character bigrams of tokens at the input layer, then fuse those into 4-token chunks, then those into 8 token chunks in the final layer.\n",
    "\n",
    "For that we will need to encode / load out data with a block size of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3839"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vg_large_path = Path('data/midi/vg_large')\n",
    "vg_large_file_names = [f for f in os.listdir(vg_large_path) if os.path.isfile(os.path.join(vg_large_path, f))]\n",
    "len(vg_large_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_encode(file_names, vocab, block_size):\n",
    "    xs, ys = [], []\n",
    "    for file_name in file_names:\n",
    "        file_path = Path(vg_large_path, file_name)\n",
    "        idx_score = midifile_to_idx_score(file_path, vocab)\n",
    "        for i in range(0, len(idx_score) - block_size, 1):\n",
    "            xs.append(idx_score[i:i+block_size])\n",
    "            ys.append(idx_score[i+block_size])\n",
    "    return np.stack(xs), ys # stack xs to create 2D tensor\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(vg_large_file_names)\n",
    "n1 = int(0.8 * len(vg_large_file_names))\n",
    "n2 = int(0.9 * len(vg_large_file_names))\n",
    "train_filenames = vg_large_file_names[:n1]\n",
    "valid_filenames = vg_large_file_names[n1:n2]\n",
    "test_filenames = vg_large_file_names[n2:]\n",
    "\n",
    "vg_large_block_samples_train = Path(f'data/numpy/vg_large/block_{block_size}_samples_train.npy')\n",
    "vg_large_block_labels_train = Path(f'data/numpy/vg_large/block_{block_size}_labels_train.npy')\n",
    "vg_large_block_samples_valid = Path(f'data/numpy/vg_large/block_{block_size}_samples_valid.npy')\n",
    "vg_large_block_labels_valid = Path(f'data/numpy/vg_large/block_{block_size}_labels_valid.npy')\n",
    "vg_large_block_samples_test = Path(f'data/numpy/vg_large/block_{block_size}_samples_test.npy')\n",
    "vg_large_block_labels_test = Path(f'data/numpy/vg_large/block_{block_size}_labels_test.npy')\n",
    "\n",
    "def load_or_create(file_names, vocab, block_samples, block_labels):\n",
    "    if block_samples.exists() and block_labels.exists():\n",
    "        xs, ys = np.load(block_samples, allow_pickle=True), np.load(block_labels, allow_pickle=True)\n",
    "    else:\n",
    "        xs, ys = block_encode(file_names, vocab, block_size)\n",
    "        np.save(block_samples, xs)\n",
    "        np.save(block_labels, ys)\n",
    "    return torch.tensor(xs, device=device), torch.tensor(ys, device=device)\n",
    "\n",
    "Xtrain, Ytrain = load_or_create(train_filenames, vocab, vg_large_block_samples_train, vg_large_block_labels_train)\n",
    "Xvalid, Yvalid = load_or_create(valid_filenames, vocab, vg_large_block_samples_valid, vg_large_block_labels_valid)\n",
    "Xtest, Ytest = load_or_create(test_filenames, vocab, vg_large_block_samples_test, vg_large_block_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9281963, 8]), torch.int64, torch.Size([9281963]), torch.int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, Xtrain.dtype, Ytrain.shape, Ytrain.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1230454, 8]), torch.int64, torch.Size([1230454]), torch.int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xvalid.shape, Xvalid.dtype, Yvalid.shape, Yvalid.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1083728, 8]), torch.int64, torch.Size([1083728]), torch.int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest.shape, Xtest.dtype, Ytest.shape, Ytest.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to take our 8-token examples and reshape them into 'bigram' pairs for the first layer, e.g.\n",
    "\n",
    "(1,2) (3,4) (5,6) (7,8)\n",
    "\n",
    "We can simply specify the dimension of the last dimension, and the rest will be automatically worked out.\n",
    "\n",
    "To allow us to insert this step into our `Sequential` model, we need to make a custom class that inherits from `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenConsecutive(torch.nn.Module):\n",
    "  \n",
    "  def __init__(self, n):\n",
    "    super(FlattenConsecutive, self).__init__()\n",
    "    self.n = n\n",
    "    \n",
    "  def forward(self, x):\n",
    "    B, T, C = x.shape\n",
    "    x = x.view(B, T // self.n, C * self.n)\n",
    "    if x.shape[1] == 1:\n",
    "      x = x.squeeze(1) # Remove the time dimension if it is 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can insert our new layer which will cause the input to dilate with a factor of two, to neurons representing 2 tokens.\n",
    "\n",
    "We can repeat this to dilate to 4-token neurons, then again to 8-token neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221326\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 200\n",
    "n_embed = 10\n",
    "dilation = 2\n",
    "fan_in = n_embed * dilation # each neuron is now only connected to two inputs rather than the entire block\n",
    "fan_through = n_hidden * 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Embedding(vocab.size, n_embed),\n",
    "\n",
    "    FlattenConsecutive(dilation),\n",
    "    torch.nn.Linear(fan_in, n_hidden, bias=False),\n",
    "    torch.nn.BatchNorm1d(n_hidden),\n",
    "    torch.nn.Tanh(),\n",
    "    \n",
    "    FlattenConsecutive(dilation),\n",
    "    torch.nn.Linear(fan_through, n_hidden, bias=False),\n",
    "    torch.nn.BatchNorm1d(n_hidden),\n",
    "    torch.nn.Tanh(),\n",
    "    \n",
    "    FlattenConsecutive(dilation),\n",
    "    torch.nn.Linear(fan_through, n_hidden, bias=False),\n",
    "    torch.nn.BatchNorm1d(n_hidden),\n",
    "    torch.nn.Tanh(),\n",
    "\n",
    "    torch.nn.Linear(n_hidden, vocab.size)\n",
    ")\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
